{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Train = pd.read_csv(\"Train_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19731 entries, 0 to 19730\n",
      "Data columns (total 51 columns):\n",
      "Unnamed: 0               19731 non-null int64\n",
      "Agreement_ID             19731 non-null int64\n",
      "Foreclosure              19731 non-null int64\n",
      "Customer_ID              19731 non-null int64\n",
      "MOB                      19731 non-null int64\n",
      "Loan_Amt                 19731 non-null object\n",
      "NET_DISBURSED_AMT        19731 non-null object\n",
      "Interest_Start_Date      19731 non-null object\n",
      "Current_ROI              19731 non-null float64\n",
      "Original_ROI             19731 non-null float64\n",
      "Current_Tenure           19731 non-null float64\n",
      "Original_Tenure          19731 non-null int64\n",
      "Due_Day                  19731 non-null int64\n",
      "Authorization_Date       19731 non-null object\n",
      "City                     19731 non-null object\n",
      "Pre_EMI_Due_Amt          19731 non-null float64\n",
      "Pre_EMI_Received_Amt     19731 non-null float64\n",
      "PRE_EMI_OS_AMOUNT        19731 non-null float64\n",
      "EMI_Due_Amt              19731 non-null float64\n",
      "EMI_Received_Amt         19731 non-null float64\n",
      "EMI_OS_AMOUNT            19731 non-null float64\n",
      "Excess_Available         19731 non-null float64\n",
      "Excess_Adjusted_Amt      19731 non-null float64\n",
      "Balance_Excess           19731 non-null float64\n",
      "Net_Receivable           19731 non-null float64\n",
      "Outstanding_Principal    19731 non-null float64\n",
      "Paid_Principal           19731 non-null float64\n",
      "Paid_Interest            19731 non-null float64\n",
      "Month_Opening            19731 non-null float64\n",
      "Last_Receipt_Date        19663 non-null object\n",
      "LAST_RECEIPT_AMOUNT      19514 non-null float64\n",
      "Net_LTV                  19731 non-null float64\n",
      "Completed_Tenure         19731 non-null int64\n",
      "Balance_Tenure           19731 non-null float64\n",
      "DPD                      19731 non-null float64\n",
      "FOIR                     19731 non-null float64\n",
      "Product                  19731 non-null object\n",
      "Scheme_ID                19731 non-null float64\n",
      "NPA_In_Last_Month        127 non-null object\n",
      "NPA_In_Current_Month     127 non-null object\n",
      "Cust_Const_Type_ID       5992 non-null float64\n",
      "Cust_Category_ID         5992 non-null float64\n",
      "Age                      5746 non-null float64\n",
      "Gender                   5747 non-null object\n",
      "Marital_Status           5746 non-null object\n",
      "Qualification            5742 non-null object\n",
      "No_Of_Dependent          5949 non-null float64\n",
      "Gross_Income             5992 non-null float64\n",
      "Pre_Job_Years            1407 non-null float64\n",
      "Net_Take_Home_Income     5992 non-null float64\n",
      "Branch_Pincode           5949 non-null float64\n",
      "dtypes: float64(31), int64(8), object(12)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "Train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_Amt</th>\n",
       "      <th>NET_DISBURSED_AMT</th>\n",
       "      <th>Interest_Start_Date</th>\n",
       "      <th>Authorization_Date</th>\n",
       "      <th>City</th>\n",
       "      <th>Last_Receipt_Date</th>\n",
       "      <th>Product</th>\n",
       "      <th>NPA_In_Last_Month</th>\n",
       "      <th>NPA_In_Current_Month</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Qualification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,17,10,107.24</td>\n",
       "      <td>1,17,10,107.24</td>\n",
       "      <td>30-Aug-10</td>\n",
       "      <td>29-Aug-10</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>05-May-14</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>POSTGRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1,92,90,253.32</td>\n",
       "      <td>1,92,90,253.32</td>\n",
       "      <td>15-Sep-10</td>\n",
       "      <td>15-Sep-10</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>01-Nov-13</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>POSTGRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39,33,395.00</td>\n",
       "      <td>39,33,395.00</td>\n",
       "      <td>01-Nov-10</td>\n",
       "      <td>02-Nov-10</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>05-Aug-17</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>GRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,00,22,587.71</td>\n",
       "      <td>1,00,22,587.71</td>\n",
       "      <td>06-Oct-10</td>\n",
       "      <td>06-Oct-10</td>\n",
       "      <td>THANE</td>\n",
       "      <td>02-May-18</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>POSTGRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77,55,937.31</td>\n",
       "      <td>77,55,937.31</td>\n",
       "      <td>26-Oct-10</td>\n",
       "      <td>26-Oct-10</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>05-Apr-18</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>UG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Loan_Amt NET_DISBURSED_AMT Interest_Start_Date Authorization_Date  \\\n",
       "0   1,17,10,107.24    1,17,10,107.24            30-Aug-10          29-Aug-10   \n",
       "1   1,92,90,253.32    1,92,90,253.32            15-Sep-10          15-Sep-10   \n",
       "2     39,33,395.00      39,33,395.00            01-Nov-10          02-Nov-10   \n",
       "3   1,00,22,587.71    1,00,22,587.71            06-Oct-10          06-Oct-10   \n",
       "4     77,55,937.31      77,55,937.31            26-Oct-10          26-Oct-10   \n",
       "\n",
       "     City Last_Receipt_Date Product NPA_In_Last_Month NPA_In_Current_Month  \\\n",
       "0  MUMBAI         05-May-14      HL               NaN                  NaN   \n",
       "1  MUMBAI         01-Nov-13      HL               NaN                  NaN   \n",
       "2  MUMBAI         05-Aug-17      HL               NaN                  NaN   \n",
       "3   THANE         02-May-18      HL               NaN                  NaN   \n",
       "4  MUMBAI         05-Apr-18      HL               NaN                  NaN   \n",
       "\n",
       "  Gender Marital_Status Qualification  \n",
       "0      M              M      POSTGRAD  \n",
       "1      M              M      POSTGRAD  \n",
       "2      M              M          GRAD  \n",
       "3      M              M      POSTGRAD  \n",
       "4      M              M            UG  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# encode categorical variables using Label Encoder\n",
    "\n",
    "# select all categorical variables\n",
    "df_categorical = Train.select_dtypes(include=['object'])\n",
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TrainX_merged = pd.read_csv(\"TrainX_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_categorical = df_categorical[['City','Product','Gender','Marital_Status','Qualification']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_categorical.Gender = df_categorical.Gender.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_categorical.Marital_Status = df_categorical.Marital_Status.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_categorical.Qualification = df_categorical.Qualification.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Product</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Qualification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   City  Product  Gender  Marital_Status  Qualification\n",
       "0   145        0       1               0              5\n",
       "1   145        0       1               0              5\n",
       "2   145        0       1               0              2\n",
       "3   222        0       1               0              5\n",
       "4   145        0       1               0              7"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply Label encoder to df_categorical\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_categorical = df_categorical.apply(le.fit_transform)\n",
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_data = pd.concat([TrainX_merged, df_categorical],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_data = Train_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Amt_DF = Train.loc[:,['Pre_EMI_Due_Amt','Pre_EMI_Received_Amt','PRE_EMI_OS_AMOUNT','EMI_Due_Amt',\n",
    "                          'EMI_Received_Amt' ,'EMI_OS_AMOUNT','Excess_Available',\n",
    "                            'Excess_Adjusted_Amt','Balance_Excess','Net_Receivable','Outstanding_Principal'\n",
    "                            ,'Paid_Principal','Paid_Interest','Month_Opening']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_Amt_DF = round(Train_Amt_DF/100000,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_data = pd.concat([Train_data,Train_Amt_DF],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_Numeric = Train.loc[:,['DPD','FOIR','No_Of_Dependent'\n",
    "                            ,'Pre_Job_Years']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_data = pd.concat([Train_data,Train_Numeric],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19731 entries, 0 to 19730\n",
      "Data columns (total 29 columns):\n",
      "Net_Disbursed_Amt        19731 non-null float64\n",
      "ROI_Change_Ratio         19731 non-null float64\n",
      "Tenure_Ratio             19731 non-null float64\n",
      "Tenure_Change_Ratio      19731 non-null float64\n",
      "Foreclosure              19731 non-null int64\n",
      "Net_LTV                  19731 non-null float64\n",
      "City                     19731 non-null int32\n",
      "Product                  19731 non-null int32\n",
      "Gender                   19731 non-null int32\n",
      "Marital_Status           19731 non-null int32\n",
      "Qualification            19731 non-null int32\n",
      "Pre_EMI_Due_Amt          19731 non-null float64\n",
      "Pre_EMI_Received_Amt     19731 non-null float64\n",
      "PRE_EMI_OS_AMOUNT        19731 non-null float64\n",
      "EMI_Due_Amt              19731 non-null float64\n",
      "EMI_Received_Amt         19731 non-null float64\n",
      "EMI_OS_AMOUNT            19731 non-null float64\n",
      "Excess_Available         19731 non-null float64\n",
      "Excess_Adjusted_Amt      19731 non-null float64\n",
      "Balance_Excess           19731 non-null float64\n",
      "Net_Receivable           19731 non-null float64\n",
      "Outstanding_Principal    19731 non-null float64\n",
      "Paid_Principal           19731 non-null float64\n",
      "Paid_Interest            19731 non-null float64\n",
      "Month_Opening            19731 non-null float64\n",
      "DPD                      19731 non-null float64\n",
      "FOIR                     19731 non-null float64\n",
      "No_Of_Dependent          5949 non-null float64\n",
      "Pre_Job_Years            1407 non-null float64\n",
      "dtypes: float64(23), int32(5), int64(1)\n",
      "memory usage: 4.0 MB\n"
     ]
    }
   ],
   "source": [
    "Train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_Of_Dependent</th>\n",
       "      <th>Pre_Job_Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5949.000000</td>\n",
       "      <td>1407.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.522609</td>\n",
       "      <td>4.607676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.102681</td>\n",
       "      <td>6.443729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       No_Of_Dependent  Pre_Job_Years\n",
       "count      5949.000000    1407.000000\n",
       "mean          0.522609       4.607676\n",
       "std           1.102681       6.443729\n",
       "min           0.000000       0.000000\n",
       "25%           0.000000       0.000000\n",
       "50%           0.000000       2.000000\n",
       "75%           0.000000       7.000000\n",
       "max          10.000000      37.000000"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data[['No_Of_Dependent','Pre_Job_Years']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data['No_Of_Dependent'] = Train_data['No_Of_Dependent'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_data['Pre_Job_Years'] = Train_data['Pre_Job_Years'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing train-test-split \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting feature variable to X\n",
    "X = Train_data.drop(['Foreclosure'],axis=1)\n",
    "\n",
    "# Putting response variable to y\n",
    "y = Train_data['Foreclosure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Net_Disbursed_Amt</th>\n",
       "      <th>ROI_Change_Ratio</th>\n",
       "      <th>Tenure_Ratio</th>\n",
       "      <th>Tenure_Change_Ratio</th>\n",
       "      <th>Net_LTV</th>\n",
       "      <th>City</th>\n",
       "      <th>Product</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Qualification</th>\n",
       "      <th>...</th>\n",
       "      <th>Balance_Excess</th>\n",
       "      <th>Net_Receivable</th>\n",
       "      <th>Outstanding_Principal</th>\n",
       "      <th>Paid_Principal</th>\n",
       "      <th>Paid_Interest</th>\n",
       "      <th>Month_Opening</th>\n",
       "      <th>DPD</th>\n",
       "      <th>FOIR</th>\n",
       "      <th>No_Of_Dependent</th>\n",
       "      <th>Pre_Job_Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16257</th>\n",
       "      <td>13.51</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>37.83</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>30.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.92</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>23.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.12</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>27.58</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>73.06</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16897</th>\n",
       "      <td>15.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>85.15</td>\n",
       "      <td>247</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Net_Disbursed_Amt  ROI_Change_Ratio  Tenure_Ratio  Tenure_Change_Ratio  \\\n",
       "16257              13.51             -0.10          0.02                -0.17   \n",
       "7515               30.03              0.00          0.10                 0.00   \n",
       "1993               23.87              0.00          0.05                 0.00   \n",
       "2239               27.58             -0.04          0.22                -0.03   \n",
       "16897              15.51              0.00          0.03                 0.00   \n",
       "\n",
       "       Net_LTV  City  Product  Gender  Marital_Status  Qualification  \\\n",
       "16257    37.83   261        1       2               2              8   \n",
       "7515     70.92    90        2       1               1              7   \n",
       "1993     54.12    90        2       0               0              5   \n",
       "2239     73.06     2        0       1               0              7   \n",
       "16897    85.15   247        2       2               2              8   \n",
       "\n",
       "           ...        Balance_Excess  Net_Receivable  Outstanding_Principal  \\\n",
       "16257      ...                   0.0             0.0                   13.0   \n",
       "7515       ...                   0.0             0.0                   29.0   \n",
       "1993       ...                   0.0             0.0                   23.0   \n",
       "2239       ...                   0.0             0.0                   25.0   \n",
       "16897      ...                   0.0             0.0                   15.0   \n",
       "\n",
       "       Paid_Principal  Paid_Interest  Month_Opening  DPD  FOIR  \\\n",
       "16257             0.0            1.0           13.0  0.0  0.57   \n",
       "7515              1.0            7.0           29.0  0.0  0.30   \n",
       "1993              0.0            3.0           23.0  0.0  0.49   \n",
       "2239              3.0           13.0           25.0  0.0  0.49   \n",
       "16897             0.0            1.0           15.0  0.0  0.34   \n",
       "\n",
       "       No_Of_Dependent  Pre_Job_Years  \n",
       "16257             -1.0           -1.0  \n",
       "7515               0.0           -1.0  \n",
       "1993               0.0           -1.0  \n",
       "2239               2.0           -1.0  \n",
       "16897             -1.0           -1.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state = 99)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAYAAAC3Y/QeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkRak4C8i0oL+PxzF2FCQnECmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install imblearn\n",
    "import imblearn\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(ratio='minority')\n",
    "X_sm, y_sm = smote.fit_sample(X_train,\n",
    "                               y_train)\n",
    "\n",
    "plot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing decision tree classifier from sklearn library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Fitting the decision tree with default hyperparameters, apart from\n",
    "# max_depth which is 5 so that we can plot and read the tree.\n",
    "dt_default = DecisionTreeClassifier(max_depth=5)\n",
    "dt_default.fit(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93      5354\n",
      "           1       0.44      0.92      0.60       566\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      5920\n",
      "   macro avg       0.72      0.90      0.76      5920\n",
      "weighted avg       0.94      0.88      0.90      5920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check the evaluation metrics of our default model\n",
    "\n",
    "# Importing classification report and confusion matrix from sklearn metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Making predictions\n",
    "y_pred_default = dt_default.predict(X_test)\n",
    "\n",
    "# Printing classification report\n",
    "print(classification_report(y_test, y_pred_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4697  657]\n",
      " [  48  518]]\n",
      "0.8809121621621622\n"
     ]
    }
   ],
   "source": [
    "# Printing confusion matrix and accuracy\n",
    "print(confusion_matrix(y_test,y_pred_default))\n",
    "print(accuracy_score(y_test,y_pred_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=15, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': range(1, 40)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='accuracy',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV to find optimal max_depth\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 15\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'max_depth': range(1, 40)}\n",
    "\n",
    "# instantiate the model\n",
    "dtree = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                               random_state = 100)\n",
    "\n",
    "# fit tree on training data\n",
    "tree = GridSearchCV(dtree, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "tree.fit(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split10_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split11_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split12_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split13_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split14_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split10_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045995</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.843149</td>\n",
       "      <td>0.843189</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>39</td>\n",
       "      <td>0.774136</td>\n",
       "      <td>0.848122</td>\n",
       "      <td>0.855781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850417</td>\n",
       "      <td>0.842672</td>\n",
       "      <td>0.834327</td>\n",
       "      <td>0.843822</td>\n",
       "      <td>0.855781</td>\n",
       "      <td>0.842246</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.022325</td>\n",
       "      <td>0.001596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.077418</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.869496</td>\n",
       "      <td>0.869536</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 2}</td>\n",
       "      <td>38</td>\n",
       "      <td>0.796782</td>\n",
       "      <td>0.874734</td>\n",
       "      <td>0.881406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.878427</td>\n",
       "      <td>0.868901</td>\n",
       "      <td>0.862932</td>\n",
       "      <td>0.870008</td>\n",
       "      <td>0.880810</td>\n",
       "      <td>0.868688</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.022935</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.103023</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.881497</td>\n",
       "      <td>0.881577</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 3}</td>\n",
       "      <td>37</td>\n",
       "      <td>0.845054</td>\n",
       "      <td>0.884186</td>\n",
       "      <td>0.890942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885578</td>\n",
       "      <td>0.881291</td>\n",
       "      <td>0.876043</td>\n",
       "      <td>0.881972</td>\n",
       "      <td>0.890942</td>\n",
       "      <td>0.880865</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.013023</td>\n",
       "      <td>0.000933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.133233</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.926681</td>\n",
       "      <td>0.927734</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 4}</td>\n",
       "      <td>36</td>\n",
       "      <td>0.882002</td>\n",
       "      <td>0.930725</td>\n",
       "      <td>0.938617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934446</td>\n",
       "      <td>0.927148</td>\n",
       "      <td>0.927890</td>\n",
       "      <td>0.927702</td>\n",
       "      <td>0.933254</td>\n",
       "      <td>0.927276</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.015092</td>\n",
       "      <td>0.001064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.156223</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.929423</td>\n",
       "      <td>0.930544</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>35</td>\n",
       "      <td>0.886770</td>\n",
       "      <td>0.933705</td>\n",
       "      <td>0.942193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936830</td>\n",
       "      <td>0.930043</td>\n",
       "      <td>0.930274</td>\n",
       "      <td>0.930597</td>\n",
       "      <td>0.935042</td>\n",
       "      <td>0.930256</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.015179</td>\n",
       "      <td>0.000926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.180619</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.930695</td>\n",
       "      <td>0.934751</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 6}</td>\n",
       "      <td>34</td>\n",
       "      <td>0.880215</td>\n",
       "      <td>0.939794</td>\n",
       "      <td>0.945173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939213</td>\n",
       "      <td>0.933152</td>\n",
       "      <td>0.929082</td>\n",
       "      <td>0.934344</td>\n",
       "      <td>0.933850</td>\n",
       "      <td>0.933663</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.016542</td>\n",
       "      <td>0.002055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.204359</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.936934</td>\n",
       "      <td>0.941257</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 7}</td>\n",
       "      <td>33</td>\n",
       "      <td>0.880810</td>\n",
       "      <td>0.945031</td>\n",
       "      <td>0.949344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949940</td>\n",
       "      <td>0.940688</td>\n",
       "      <td>0.938021</td>\n",
       "      <td>0.942136</td>\n",
       "      <td>0.939213</td>\n",
       "      <td>0.940688</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.017926</td>\n",
       "      <td>0.001256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.227356</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.939636</td>\n",
       "      <td>0.945969</td>\n",
       "      <td>8</td>\n",
       "      <td>{'max_depth': 8}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.882598</td>\n",
       "      <td>0.949119</td>\n",
       "      <td>0.945769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953516</td>\n",
       "      <td>0.944180</td>\n",
       "      <td>0.944577</td>\n",
       "      <td>0.945840</td>\n",
       "      <td>0.942193</td>\n",
       "      <td>0.945968</td>\n",
       "      <td>0.008905</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.018094</td>\n",
       "      <td>0.001771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.243054</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.941822</td>\n",
       "      <td>0.951436</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_depth': 9}</td>\n",
       "      <td>31</td>\n",
       "      <td>0.879023</td>\n",
       "      <td>0.956357</td>\n",
       "      <td>0.947557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955304</td>\n",
       "      <td>0.950013</td>\n",
       "      <td>0.945769</td>\n",
       "      <td>0.952014</td>\n",
       "      <td>0.943385</td>\n",
       "      <td>0.950226</td>\n",
       "      <td>0.004156</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.020055</td>\n",
       "      <td>0.001733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.262441</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.944127</td>\n",
       "      <td>0.956400</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 10}</td>\n",
       "      <td>30</td>\n",
       "      <td>0.874851</td>\n",
       "      <td>0.960104</td>\n",
       "      <td>0.957092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960667</td>\n",
       "      <td>0.955591</td>\n",
       "      <td>0.952324</td>\n",
       "      <td>0.957336</td>\n",
       "      <td>0.945173</td>\n",
       "      <td>0.952695</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.021697</td>\n",
       "      <td>0.001813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.309764</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.946471</td>\n",
       "      <td>0.961407</td>\n",
       "      <td>11</td>\n",
       "      <td>{'max_depth': 11}</td>\n",
       "      <td>29</td>\n",
       "      <td>0.866508</td>\n",
       "      <td>0.965937</td>\n",
       "      <td>0.957688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960072</td>\n",
       "      <td>0.960572</td>\n",
       "      <td>0.960667</td>\n",
       "      <td>0.961807</td>\n",
       "      <td>0.950536</td>\n",
       "      <td>0.957677</td>\n",
       "      <td>0.017493</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.024477</td>\n",
       "      <td>0.001967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.328349</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.948259</td>\n",
       "      <td>0.966034</td>\n",
       "      <td>12</td>\n",
       "      <td>{'max_depth': 12}</td>\n",
       "      <td>28</td>\n",
       "      <td>0.871275</td>\n",
       "      <td>0.971515</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959476</td>\n",
       "      <td>0.966831</td>\n",
       "      <td>0.957688</td>\n",
       "      <td>0.965298</td>\n",
       "      <td>0.948153</td>\n",
       "      <td>0.962148</td>\n",
       "      <td>0.017882</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.024367</td>\n",
       "      <td>0.002389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.318391</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.949730</td>\n",
       "      <td>0.970982</td>\n",
       "      <td>13</td>\n",
       "      <td>{'max_depth': 13}</td>\n",
       "      <td>27</td>\n",
       "      <td>0.866508</td>\n",
       "      <td>0.975134</td>\n",
       "      <td>0.962455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965435</td>\n",
       "      <td>0.972707</td>\n",
       "      <td>0.959476</td>\n",
       "      <td>0.970961</td>\n",
       "      <td>0.951728</td>\n",
       "      <td>0.968492</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.002392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.335097</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.950962</td>\n",
       "      <td>0.975719</td>\n",
       "      <td>14</td>\n",
       "      <td>{'max_depth': 14}</td>\n",
       "      <td>26</td>\n",
       "      <td>0.860548</td>\n",
       "      <td>0.978966</td>\n",
       "      <td>0.965435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964243</td>\n",
       "      <td>0.977689</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>0.975688</td>\n",
       "      <td>0.952324</td>\n",
       "      <td>0.974027</td>\n",
       "      <td>0.017121</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>0.002256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.344056</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.951160</td>\n",
       "      <td>0.979835</td>\n",
       "      <td>15</td>\n",
       "      <td>{'max_depth': 15}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.854589</td>\n",
       "      <td>0.983735</td>\n",
       "      <td>0.970203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964839</td>\n",
       "      <td>0.980456</td>\n",
       "      <td>0.954708</td>\n",
       "      <td>0.979860</td>\n",
       "      <td>0.960072</td>\n",
       "      <td>0.979222</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.029373</td>\n",
       "      <td>0.002290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.345536</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.952790</td>\n",
       "      <td>0.983801</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 16}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.853993</td>\n",
       "      <td>0.987865</td>\n",
       "      <td>0.967819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>0.984544</td>\n",
       "      <td>0.961263</td>\n",
       "      <td>0.984544</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.984714</td>\n",
       "      <td>0.009056</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.029879</td>\n",
       "      <td>0.002058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.359967</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.953783</td>\n",
       "      <td>0.987573</td>\n",
       "      <td>17</td>\n",
       "      <td>{'max_depth': 17}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.846841</td>\n",
       "      <td>0.990760</td>\n",
       "      <td>0.969607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967819</td>\n",
       "      <td>0.987439</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>0.987737</td>\n",
       "      <td>0.960667</td>\n",
       "      <td>0.989015</td>\n",
       "      <td>0.022907</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.031544</td>\n",
       "      <td>0.001743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.360508</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.954022</td>\n",
       "      <td>0.990531</td>\n",
       "      <td>18</td>\n",
       "      <td>{'max_depth': 18}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.849225</td>\n",
       "      <td>0.992975</td>\n",
       "      <td>0.971990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966031</td>\n",
       "      <td>0.990803</td>\n",
       "      <td>0.961263</td>\n",
       "      <td>0.990718</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.991655</td>\n",
       "      <td>0.015222</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.032207</td>\n",
       "      <td>0.001458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.376209</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.954061</td>\n",
       "      <td>0.992958</td>\n",
       "      <td>19</td>\n",
       "      <td>{'max_depth': 19}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.853397</td>\n",
       "      <td>0.994891</td>\n",
       "      <td>0.973778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>0.993145</td>\n",
       "      <td>0.962455</td>\n",
       "      <td>0.992932</td>\n",
       "      <td>0.964243</td>\n",
       "      <td>0.993826</td>\n",
       "      <td>0.020392</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.031026</td>\n",
       "      <td>0.001176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.370590</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.953743</td>\n",
       "      <td>0.994737</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 20}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.851609</td>\n",
       "      <td>0.996083</td>\n",
       "      <td>0.969607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.965435</td>\n",
       "      <td>0.994720</td>\n",
       "      <td>0.964839</td>\n",
       "      <td>0.995359</td>\n",
       "      <td>0.009799</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.031138</td>\n",
       "      <td>0.000951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.369122</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.952988</td>\n",
       "      <td>0.996086</td>\n",
       "      <td>21</td>\n",
       "      <td>{'max_depth': 21}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.840882</td>\n",
       "      <td>0.997105</td>\n",
       "      <td>0.967819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963051</td>\n",
       "      <td>0.996551</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.995657</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>0.996764</td>\n",
       "      <td>0.017744</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.033782</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.396882</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.953108</td>\n",
       "      <td>0.997071</td>\n",
       "      <td>22</td>\n",
       "      <td>{'max_depth': 22}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>0.970799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962455</td>\n",
       "      <td>0.997488</td>\n",
       "      <td>0.963051</td>\n",
       "      <td>0.996721</td>\n",
       "      <td>0.966031</td>\n",
       "      <td>0.997658</td>\n",
       "      <td>0.033125</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>0.000658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.393252</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.953584</td>\n",
       "      <td>0.997826</td>\n",
       "      <td>23</td>\n",
       "      <td>{'max_depth': 23}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.840286</td>\n",
       "      <td>0.999106</td>\n",
       "      <td>0.972586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962455</td>\n",
       "      <td>0.998084</td>\n",
       "      <td>0.961263</td>\n",
       "      <td>0.997573</td>\n",
       "      <td>0.967223</td>\n",
       "      <td>0.998510</td>\n",
       "      <td>0.016703</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.034212</td>\n",
       "      <td>0.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.421669</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.952670</td>\n",
       "      <td>0.998436</td>\n",
       "      <td>24</td>\n",
       "      <td>{'max_depth': 24}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.835518</td>\n",
       "      <td>0.999489</td>\n",
       "      <td>0.968415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960667</td>\n",
       "      <td>0.998510</td>\n",
       "      <td>0.964243</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.000558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.397198</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.953267</td>\n",
       "      <td>0.998865</td>\n",
       "      <td>25</td>\n",
       "      <td>{'max_depth': 25}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.840286</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.970799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.998850</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>0.998765</td>\n",
       "      <td>0.966031</td>\n",
       "      <td>0.999446</td>\n",
       "      <td>0.025034</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.033835</td>\n",
       "      <td>0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.383170</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.952472</td>\n",
       "      <td>0.999228</td>\n",
       "      <td>26</td>\n",
       "      <td>{'max_depth': 26}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.840286</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>0.970203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.999021</td>\n",
       "      <td>0.961263</td>\n",
       "      <td>0.998936</td>\n",
       "      <td>0.966031</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.014717</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.033724</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.417718</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.952233</td>\n",
       "      <td>0.999495</td>\n",
       "      <td>27</td>\n",
       "      <td>{'max_depth': 27}</td>\n",
       "      <td>19</td>\n",
       "      <td>0.837902</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>0.967223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960667</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.960667</td>\n",
       "      <td>0.999148</td>\n",
       "      <td>0.964243</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.035212</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.033875</td>\n",
       "      <td>0.000431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.434894</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.952710</td>\n",
       "      <td>0.999708</td>\n",
       "      <td>28</td>\n",
       "      <td>{'max_depth': 28}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.839094</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.969011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959476</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.999446</td>\n",
       "      <td>0.964243</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.062510</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.033924</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.420795</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.952829</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>29</td>\n",
       "      <td>{'max_depth': 29}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.840286</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960667</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062273</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.033376</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.395192</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.952472</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 30}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036915</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.033480</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.413103</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.952591</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>31</td>\n",
       "      <td>{'max_depth': 31}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963051</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035057</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.033545</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.422332</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.952591</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 32}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960072</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081766</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.386703</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.952313</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>33</td>\n",
       "      <td>{'max_depth': 33}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041394</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.033391</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.411363</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.952392</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>34</td>\n",
       "      <td>{'max_depth': 34}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022394</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.033472</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.416079</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.952035</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>35</td>\n",
       "      <td>{'max_depth': 35}</td>\n",
       "      <td>24</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045271</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.033353</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.401146</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.952154</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>36</td>\n",
       "      <td>{'max_depth': 36}</td>\n",
       "      <td>20</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026228</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.033413</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.427409</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.952114</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>37</td>\n",
       "      <td>{'max_depth': 37}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036895</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.033392</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.382027</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.952074</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>38</td>\n",
       "      <td>{'max_depth': 38}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.380886</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.952074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>{'max_depth': 39}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        0.045995         0.000796         0.843149          0.843189   \n",
       "1        0.077418         0.000527         0.869496          0.869536   \n",
       "2        0.103023         0.000466         0.881497          0.881577   \n",
       "3        0.133233         0.000726         0.926681          0.927734   \n",
       "4        0.156223         0.000529         0.929423          0.930544   \n",
       "5        0.180619         0.000600         0.930695          0.934751   \n",
       "6        0.204359         0.000927         0.936934          0.941257   \n",
       "7        0.227356         0.000726         0.939636          0.945969   \n",
       "8        0.243054         0.000593         0.941822          0.951436   \n",
       "9        0.262441         0.000333         0.944127          0.956400   \n",
       "10       0.309764         0.000665         0.946471          0.961407   \n",
       "11       0.328349         0.000866         0.948259          0.966034   \n",
       "12       0.318391         0.000795         0.949730          0.970982   \n",
       "13       0.335097         0.000657         0.950962          0.975719   \n",
       "14       0.344056         0.000868         0.951160          0.979835   \n",
       "15       0.345536         0.000595         0.952790          0.983801   \n",
       "16       0.359967         0.000857         0.953783          0.987573   \n",
       "17       0.360508         0.000995         0.954022          0.990531   \n",
       "18       0.376209         0.000729         0.954061          0.992958   \n",
       "19       0.370590         0.000600         0.953743          0.994737   \n",
       "20       0.369122         0.000794         0.952988          0.996086   \n",
       "21       0.396882         0.000893         0.953108          0.997071   \n",
       "22       0.393252         0.000866         0.953584          0.997826   \n",
       "23       0.421669         0.000923         0.952670          0.998436   \n",
       "24       0.397198         0.000927         0.953267          0.998865   \n",
       "25       0.383170         0.000862         0.952472          0.999228   \n",
       "26       0.417718         0.000997         0.952233          0.999495   \n",
       "27       0.434894         0.000798         0.952710          0.999708   \n",
       "28       0.420795         0.000931         0.952829          0.999827   \n",
       "29       0.395192         0.000791         0.952472          0.999889   \n",
       "30       0.413103         0.001002         0.952591          0.999923   \n",
       "31       0.422332         0.000793         0.952591          0.999952   \n",
       "32       0.386703         0.000794         0.952313          0.999974   \n",
       "33       0.411363         0.001466         0.952392          0.999983   \n",
       "34       0.416079         0.000994         0.952035          0.999989   \n",
       "35       0.401146         0.000799         0.952154          0.999994   \n",
       "36       0.427409         0.001072         0.952114          0.999997   \n",
       "37       0.382027         0.000799         0.952074          0.999997   \n",
       "38       0.380886         0.000795         0.952074          1.000000   \n",
       "\n",
       "   param_max_depth             params  rank_test_score  split0_test_score  \\\n",
       "0                1   {'max_depth': 1}               39           0.774136   \n",
       "1                2   {'max_depth': 2}               38           0.796782   \n",
       "2                3   {'max_depth': 3}               37           0.845054   \n",
       "3                4   {'max_depth': 4}               36           0.882002   \n",
       "4                5   {'max_depth': 5}               35           0.886770   \n",
       "5                6   {'max_depth': 6}               34           0.880215   \n",
       "6                7   {'max_depth': 7}               33           0.880810   \n",
       "7                8   {'max_depth': 8}               32           0.882598   \n",
       "8                9   {'max_depth': 9}               31           0.879023   \n",
       "9               10  {'max_depth': 10}               30           0.874851   \n",
       "10              11  {'max_depth': 11}               29           0.866508   \n",
       "11              12  {'max_depth': 12}               28           0.871275   \n",
       "12              13  {'max_depth': 13}               27           0.866508   \n",
       "13              14  {'max_depth': 14}               26           0.860548   \n",
       "14              15  {'max_depth': 15}               25           0.854589   \n",
       "15              16  {'max_depth': 16}               10           0.853993   \n",
       "16              17  {'max_depth': 17}                3           0.846841   \n",
       "17              18  {'max_depth': 18}                2           0.849225   \n",
       "18              19  {'max_depth': 19}                1           0.853397   \n",
       "19              20  {'max_depth': 20}                4           0.851609   \n",
       "20              21  {'max_depth': 21}                8           0.840882   \n",
       "21              22  {'max_depth': 22}                7           0.839690   \n",
       "22              23  {'max_depth': 23}                5           0.840286   \n",
       "23              24  {'max_depth': 24}               12           0.835518   \n",
       "24              25  {'max_depth': 25}                6           0.840286   \n",
       "25              26  {'max_depth': 26}               15           0.840286   \n",
       "26              27  {'max_depth': 27}               19           0.837902   \n",
       "27              28  {'max_depth': 28}               11           0.839094   \n",
       "28              29  {'max_depth': 29}                9           0.840286   \n",
       "29              30  {'max_depth': 30}               15           0.839690   \n",
       "30              31  {'max_depth': 31}               13           0.839690   \n",
       "31              32  {'max_depth': 32}               13           0.839690   \n",
       "32              33  {'max_depth': 33}               18           0.839690   \n",
       "33              34  {'max_depth': 34}               17           0.839690   \n",
       "34              35  {'max_depth': 35}               24           0.839690   \n",
       "35              36  {'max_depth': 36}               20           0.839690   \n",
       "36              37  {'max_depth': 37}               21           0.839690   \n",
       "37              38  {'max_depth': 38}               22           0.839690   \n",
       "38              39  {'max_depth': 39}               22           0.839690   \n",
       "\n",
       "    split0_train_score  split10_test_score       ...         \\\n",
       "0             0.848122            0.855781       ...          \n",
       "1             0.874734            0.881406       ...          \n",
       "2             0.884186            0.890942       ...          \n",
       "3             0.930725            0.938617       ...          \n",
       "4             0.933705            0.942193       ...          \n",
       "5             0.939794            0.945173       ...          \n",
       "6             0.945031            0.949344       ...          \n",
       "7             0.949119            0.945769       ...          \n",
       "8             0.956357            0.947557       ...          \n",
       "9             0.960104            0.957092       ...          \n",
       "10            0.965937            0.957688       ...          \n",
       "11            0.971515            0.966627       ...          \n",
       "12            0.975134            0.962455       ...          \n",
       "13            0.978966            0.965435       ...          \n",
       "14            0.983735            0.970203       ...          \n",
       "15            0.987865            0.967819       ...          \n",
       "16            0.990760            0.969607       ...          \n",
       "17            0.992975            0.971990       ...          \n",
       "18            0.994891            0.973778       ...          \n",
       "19            0.996083            0.969607       ...          \n",
       "20            0.997105            0.967819       ...          \n",
       "21            0.998254            0.970799       ...          \n",
       "22            0.999106            0.972586       ...          \n",
       "23            0.999489            0.968415       ...          \n",
       "24            0.999659            0.970799       ...          \n",
       "25            0.999787            0.970203       ...          \n",
       "26            0.999787            0.967223       ...          \n",
       "27            0.999915            0.969011       ...          \n",
       "28            0.999957            0.966627       ...          \n",
       "29            1.000000            0.969607       ...          \n",
       "30            1.000000            0.969607       ...          \n",
       "31            1.000000            0.970203       ...          \n",
       "32            1.000000            0.967819       ...          \n",
       "33            1.000000            0.970799       ...          \n",
       "34            1.000000            0.967819       ...          \n",
       "35            1.000000            0.969607       ...          \n",
       "36            1.000000            0.969011       ...          \n",
       "37            1.000000            0.968415       ...          \n",
       "38            1.000000            0.968415       ...          \n",
       "\n",
       "    split7_test_score  split7_train_score  split8_test_score  \\\n",
       "0            0.850417            0.842672           0.834327   \n",
       "1            0.878427            0.868901           0.862932   \n",
       "2            0.885578            0.881291           0.876043   \n",
       "3            0.934446            0.927148           0.927890   \n",
       "4            0.936830            0.930043           0.930274   \n",
       "5            0.939213            0.933152           0.929082   \n",
       "6            0.949940            0.940688           0.938021   \n",
       "7            0.953516            0.944180           0.944577   \n",
       "8            0.955304            0.950013           0.945769   \n",
       "9            0.960667            0.955591           0.952324   \n",
       "10           0.960072            0.960572           0.960667   \n",
       "11           0.959476            0.966831           0.957688   \n",
       "12           0.965435            0.972707           0.959476   \n",
       "13           0.964243            0.977689           0.963647   \n",
       "14           0.964839            0.980456           0.954708   \n",
       "15           0.966627            0.984544           0.961263   \n",
       "16           0.967819            0.987439           0.963647   \n",
       "17           0.966031            0.990803           0.961263   \n",
       "18           0.963647            0.993145           0.962455   \n",
       "19           0.963647            0.995103           0.965435   \n",
       "20           0.963051            0.996551           0.961859   \n",
       "21           0.962455            0.997488           0.963051   \n",
       "22           0.962455            0.998084           0.961263   \n",
       "23           0.960667            0.998510           0.964243   \n",
       "24           0.961859            0.998850           0.963647   \n",
       "25           0.961859            0.999021           0.961263   \n",
       "26           0.960667            0.999659           0.960667   \n",
       "27           0.959476            0.999872           0.961859   \n",
       "28           0.960667            0.999915           0.961859   \n",
       "29           0.958880            1.000000           0.963647   \n",
       "30           0.958880            1.000000           0.963051   \n",
       "31           0.958880            1.000000           0.960072   \n",
       "32           0.958880            1.000000           0.961859   \n",
       "33           0.958880            1.000000           0.961859   \n",
       "34           0.958880            1.000000           0.961859   \n",
       "35           0.958880            1.000000           0.961859   \n",
       "36           0.958880            1.000000           0.961859   \n",
       "37           0.958880            1.000000           0.961859   \n",
       "38           0.958880            1.000000           0.961859   \n",
       "\n",
       "    split8_train_score  split9_test_score  split9_train_score  std_fit_time  \\\n",
       "0             0.843822           0.855781            0.842246      0.005609   \n",
       "1             0.870008           0.880810            0.868688      0.006048   \n",
       "2             0.881972           0.890942            0.880865      0.005121   \n",
       "3             0.927702           0.933254            0.927276      0.011192   \n",
       "4             0.930597           0.935042            0.930256      0.003416   \n",
       "5             0.934344           0.933850            0.933663      0.007841   \n",
       "6             0.942136           0.939213            0.940688      0.008789   \n",
       "7             0.945840           0.942193            0.945968      0.008905   \n",
       "8             0.952014           0.943385            0.950226      0.004156   \n",
       "9             0.957336           0.945173            0.952695      0.003268   \n",
       "10            0.961807           0.950536            0.957677      0.017493   \n",
       "11            0.965298           0.948153            0.962148      0.017882   \n",
       "12            0.970961           0.951728            0.968492      0.013557   \n",
       "13            0.975688           0.952324            0.974027      0.017121   \n",
       "14            0.979860           0.960072            0.979222      0.020202   \n",
       "15            0.984544           0.961859            0.984714      0.009056   \n",
       "16            0.987737           0.960667            0.989015      0.022907   \n",
       "17            0.990718           0.961859            0.991655      0.015222   \n",
       "18            0.992932           0.964243            0.993826      0.020392   \n",
       "19            0.994720           0.964839            0.995359      0.009799   \n",
       "20            0.995657           0.966627            0.996764      0.017744   \n",
       "21            0.996721           0.966031            0.997658      0.033125   \n",
       "22            0.997573           0.967223            0.998510      0.016703   \n",
       "23            0.998254           0.966627            0.999063      0.045070   \n",
       "24            0.998765           0.966031            0.999446      0.025034   \n",
       "25            0.998936           0.966031            0.999702      0.014717   \n",
       "26            0.999148           0.964243            0.999915      0.035212   \n",
       "27            0.999446           0.964243            0.999957      0.062510   \n",
       "28            0.999702           0.963647            1.000000      0.062273   \n",
       "29            0.999872           0.963647            1.000000      0.036915   \n",
       "30            0.999872           0.963647            1.000000      0.035057   \n",
       "31            0.999957           0.963647            1.000000      0.081766   \n",
       "32            1.000000           0.963647            1.000000      0.041394   \n",
       "33            1.000000           0.963647            1.000000      0.022394   \n",
       "34            1.000000           0.963647            1.000000      0.045271   \n",
       "35            1.000000           0.963647            1.000000      0.026228   \n",
       "36            1.000000           0.963647            1.000000      0.036895   \n",
       "37            1.000000           0.963647            1.000000      0.021985   \n",
       "38            1.000000           0.963647            1.000000      0.018229   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.000980        0.022325         0.001596  \n",
       "1         0.000493        0.022935         0.001640  \n",
       "2         0.000498        0.013023         0.000933  \n",
       "3         0.000438        0.015092         0.001064  \n",
       "4         0.000495        0.015179         0.000926  \n",
       "5         0.000490        0.016542         0.002055  \n",
       "6         0.000440        0.017926         0.001256  \n",
       "7         0.000438        0.018094         0.001771  \n",
       "8         0.000484        0.020055         0.001733  \n",
       "9         0.000470        0.021697         0.001813  \n",
       "10        0.000470        0.024477         0.001967  \n",
       "11        0.000340        0.024367         0.002389  \n",
       "12        0.000398        0.025509         0.002392  \n",
       "13        0.000464        0.027329         0.002256  \n",
       "14        0.000506        0.029373         0.002290  \n",
       "15        0.000486        0.029879         0.002058  \n",
       "16        0.000497        0.031544         0.001743  \n",
       "17        0.000014        0.032207         0.001458  \n",
       "18        0.000440        0.031026         0.001176  \n",
       "19        0.000490        0.031138         0.000951  \n",
       "20        0.000537        0.033782         0.000719  \n",
       "21        0.000269        0.033993         0.000658  \n",
       "22        0.000340        0.034212         0.000608  \n",
       "23        0.000442        0.034868         0.000558  \n",
       "24        0.000442        0.033835         0.000559  \n",
       "25        0.000338        0.033724         0.000511  \n",
       "26        0.000367        0.033875         0.000431  \n",
       "27        0.000399        0.033924         0.000343  \n",
       "28        0.000443        0.033376         0.000244  \n",
       "29        0.000396        0.033480         0.000181  \n",
       "30        0.000366        0.033545         0.000138  \n",
       "31        0.000397        0.033526         0.000103  \n",
       "32        0.000397        0.033391         0.000076  \n",
       "33        0.001790        0.033472         0.000053  \n",
       "34        0.000368        0.033353         0.000042  \n",
       "35        0.000400        0.033413         0.000021  \n",
       "36        0.000583        0.033392         0.000011  \n",
       "37        0.000544        0.033372         0.000011  \n",
       "38        0.000398        0.033372         0.000000  \n",
       "\n",
       "[39 rows x 41 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = tree.cv_results_\n",
    "pd.DataFrame(scores).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAELCAYAAAAoUKpTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqZMlC2OMMXWyZGGMMaZO/x8Iqlteg1AVTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting accuracies with max_depth\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'min_samples_leaf': range(5, 200, 20)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV to find optimal max_depth\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 15\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_leaf': range(5, 200, 20)}\n",
    "\n",
    "# instantiate the model\n",
    "dtree = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                               random_state = 100)\n",
    "\n",
    "# fit tree on training data\n",
    "tree = GridSearchCV(dtree, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "tree.fit(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301206</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.949650</td>\n",
       "      <td>0.978759</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 5}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.897894</td>\n",
       "      <td>0.985544</td>\n",
       "      <td>0.959277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962639</td>\n",
       "      <td>0.977548</td>\n",
       "      <td>0.966415</td>\n",
       "      <td>0.977399</td>\n",
       "      <td>0.962043</td>\n",
       "      <td>0.976704</td>\n",
       "      <td>0.019179</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.025982</td>\n",
       "      <td>0.003413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.263301</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.941861</td>\n",
       "      <td>0.955522</td>\n",
       "      <td>25</td>\n",
       "      <td>{'min_samples_leaf': 25}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.889948</td>\n",
       "      <td>0.962047</td>\n",
       "      <td>0.949344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957472</td>\n",
       "      <td>0.952712</td>\n",
       "      <td>0.956677</td>\n",
       "      <td>0.953109</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.955494</td>\n",
       "      <td>0.018796</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.026120</td>\n",
       "      <td>0.003403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.246451</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.935861</td>\n",
       "      <td>0.944673</td>\n",
       "      <td>45</td>\n",
       "      <td>{'min_samples_leaf': 45}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.895709</td>\n",
       "      <td>0.952707</td>\n",
       "      <td>0.941398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951312</td>\n",
       "      <td>0.941238</td>\n",
       "      <td>0.945350</td>\n",
       "      <td>0.942976</td>\n",
       "      <td>0.945548</td>\n",
       "      <td>0.944168</td>\n",
       "      <td>0.013504</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.020326</td>\n",
       "      <td>0.004129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.232430</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.934510</td>\n",
       "      <td>0.941017</td>\n",
       "      <td>65</td>\n",
       "      <td>{'min_samples_leaf': 65}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.899086</td>\n",
       "      <td>0.949478</td>\n",
       "      <td>0.937426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.944952</td>\n",
       "      <td>0.937512</td>\n",
       "      <td>0.946940</td>\n",
       "      <td>0.939499</td>\n",
       "      <td>0.944157</td>\n",
       "      <td>0.940344</td>\n",
       "      <td>0.016344</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>0.004342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.218582</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.932841</td>\n",
       "      <td>0.938662</td>\n",
       "      <td>85</td>\n",
       "      <td>{'min_samples_leaf': 85}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.898292</td>\n",
       "      <td>0.944660</td>\n",
       "      <td>0.940008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943959</td>\n",
       "      <td>0.936519</td>\n",
       "      <td>0.939984</td>\n",
       "      <td>0.936320</td>\n",
       "      <td>0.941971</td>\n",
       "      <td>0.937910</td>\n",
       "      <td>0.023409</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.017339</td>\n",
       "      <td>0.003072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.301206         0.001405         0.949650          0.978759   \n",
       "1       0.263301         0.001203         0.941861          0.955522   \n",
       "2       0.246451         0.001404         0.935861          0.944673   \n",
       "3       0.232430         0.001410         0.934510          0.941017   \n",
       "4       0.218582         0.001008         0.932841          0.938662   \n",
       "\n",
       "  param_min_samples_leaf                    params  rank_test_score  \\\n",
       "0                      5   {'min_samples_leaf': 5}                1   \n",
       "1                     25  {'min_samples_leaf': 25}                2   \n",
       "2                     45  {'min_samples_leaf': 45}                3   \n",
       "3                     65  {'min_samples_leaf': 65}                4   \n",
       "4                     85  {'min_samples_leaf': 85}                7   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score       ...         \\\n",
       "0           0.897894            0.985544           0.959277       ...          \n",
       "1           0.889948            0.962047           0.949344       ...          \n",
       "2           0.895709            0.952707           0.941398       ...          \n",
       "3           0.899086            0.949478           0.937426       ...          \n",
       "4           0.898292            0.944660           0.940008       ...          \n",
       "\n",
       "   split2_test_score  split2_train_score  split3_test_score  \\\n",
       "0           0.962639            0.977548           0.966415   \n",
       "1           0.957472            0.952712           0.956677   \n",
       "2           0.951312            0.941238           0.945350   \n",
       "3           0.944952            0.937512           0.946940   \n",
       "4           0.943959            0.936519           0.939984   \n",
       "\n",
       "   split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
       "0            0.977399           0.962043            0.976704      0.019179   \n",
       "1            0.953109           0.955882            0.955494      0.018796   \n",
       "2            0.942976           0.945548            0.944168      0.013504   \n",
       "3            0.939499           0.944157            0.940344      0.016344   \n",
       "4            0.936320           0.941971            0.937910      0.023409   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.000492        0.025982         0.003413  \n",
       "1        0.000401        0.026120         0.003403  \n",
       "2        0.000491        0.020326         0.004129  \n",
       "3        0.000487        0.018001         0.004342  \n",
       "4        0.000022        0.017339         0.003072  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = tree.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAELCAYAAAAoUKpTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j8kcur5Z5c67QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting accuracies with min_samples_leaf\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'min_samples_split': range(5, 200, 20)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV to find optimal min_samples_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_split': range(5, 200, 20)}\n",
    "\n",
    "# instantiate the model\n",
    "dtree = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                               random_state = 100)\n",
    "\n",
    "# fit tree on training data\n",
    "tree = GridSearchCV(dtree, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "tree.fit(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.339710</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.949611</td>\n",
       "      <td>0.994735</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_split': 5}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892133</td>\n",
       "      <td>0.997914</td>\n",
       "      <td>0.961263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964825</td>\n",
       "      <td>0.995082</td>\n",
       "      <td>0.967409</td>\n",
       "      <td>0.993195</td>\n",
       "      <td>0.962440</td>\n",
       "      <td>0.994039</td>\n",
       "      <td>0.030260</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.028820</td>\n",
       "      <td>0.001718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.313555</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.948379</td>\n",
       "      <td>0.975799</td>\n",
       "      <td>25</td>\n",
       "      <td>{'min_samples_split': 25}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.896106</td>\n",
       "      <td>0.982613</td>\n",
       "      <td>0.960469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962838</td>\n",
       "      <td>0.973823</td>\n",
       "      <td>0.962242</td>\n",
       "      <td>0.974270</td>\n",
       "      <td>0.960254</td>\n",
       "      <td>0.975512</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.026159</td>\n",
       "      <td>0.003518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.325875</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.947107</td>\n",
       "      <td>0.968119</td>\n",
       "      <td>45</td>\n",
       "      <td>{'min_samples_split': 45}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.896702</td>\n",
       "      <td>0.974367</td>\n",
       "      <td>0.958482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960652</td>\n",
       "      <td>0.966918</td>\n",
       "      <td>0.963235</td>\n",
       "      <td>0.966124</td>\n",
       "      <td>0.956479</td>\n",
       "      <td>0.966769</td>\n",
       "      <td>0.012503</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>0.003136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.319463</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.946829</td>\n",
       "      <td>0.963937</td>\n",
       "      <td>65</td>\n",
       "      <td>{'min_samples_split': 65}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.898888</td>\n",
       "      <td>0.971336</td>\n",
       "      <td>0.954311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960652</td>\n",
       "      <td>0.960610</td>\n",
       "      <td>0.961645</td>\n",
       "      <td>0.961951</td>\n",
       "      <td>0.958665</td>\n",
       "      <td>0.964087</td>\n",
       "      <td>0.020113</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.024106</td>\n",
       "      <td>0.003868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.305019</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.945239</td>\n",
       "      <td>0.961135</td>\n",
       "      <td>85</td>\n",
       "      <td>{'min_samples_split': 85}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.898490</td>\n",
       "      <td>0.969001</td>\n",
       "      <td>0.953317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959857</td>\n",
       "      <td>0.958375</td>\n",
       "      <td>0.956280</td>\n",
       "      <td>0.957481</td>\n",
       "      <td>0.958267</td>\n",
       "      <td>0.962249</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.004259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.333493</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.943371</td>\n",
       "      <td>0.959168</td>\n",
       "      <td>105</td>\n",
       "      <td>{'min_samples_split': 105}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.897100</td>\n",
       "      <td>0.967909</td>\n",
       "      <td>0.952721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.956479</td>\n",
       "      <td>0.954798</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.956984</td>\n",
       "      <td>0.954690</td>\n",
       "      <td>0.959468</td>\n",
       "      <td>0.026337</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.023175</td>\n",
       "      <td>0.004616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.340520</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.943650</td>\n",
       "      <td>0.957588</td>\n",
       "      <td>125</td>\n",
       "      <td>{'min_samples_split': 125}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.897696</td>\n",
       "      <td>0.967163</td>\n",
       "      <td>0.951728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.956876</td>\n",
       "      <td>0.954550</td>\n",
       "      <td>0.957472</td>\n",
       "      <td>0.955295</td>\n",
       "      <td>0.954491</td>\n",
       "      <td>0.956189</td>\n",
       "      <td>0.030960</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.023070</td>\n",
       "      <td>0.004821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.346927</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.944087</td>\n",
       "      <td>0.956496</td>\n",
       "      <td>145</td>\n",
       "      <td>{'min_samples_split': 145}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.901867</td>\n",
       "      <td>0.965872</td>\n",
       "      <td>0.951728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.952017</td>\n",
       "      <td>0.956479</td>\n",
       "      <td>0.954351</td>\n",
       "      <td>0.954491</td>\n",
       "      <td>0.956189</td>\n",
       "      <td>0.034165</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.021176</td>\n",
       "      <td>0.004871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.322070</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.942299</td>\n",
       "      <td>0.954369</td>\n",
       "      <td>165</td>\n",
       "      <td>{'min_samples_split': 165}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.900874</td>\n",
       "      <td>0.962842</td>\n",
       "      <td>0.949146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953696</td>\n",
       "      <td>0.950228</td>\n",
       "      <td>0.955286</td>\n",
       "      <td>0.953308</td>\n",
       "      <td>0.952504</td>\n",
       "      <td>0.954252</td>\n",
       "      <td>0.017321</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.020813</td>\n",
       "      <td>0.004472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.317050</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.940431</td>\n",
       "      <td>0.952581</td>\n",
       "      <td>185</td>\n",
       "      <td>{'min_samples_split': 185}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.896305</td>\n",
       "      <td>0.961749</td>\n",
       "      <td>0.949146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952504</td>\n",
       "      <td>0.948242</td>\n",
       "      <td>0.952901</td>\n",
       "      <td>0.951073</td>\n",
       "      <td>0.951312</td>\n",
       "      <td>0.952513</td>\n",
       "      <td>0.033348</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.004811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.339710         0.001403         0.949611          0.994735   \n",
       "1       0.313555         0.001608         0.948379          0.975799   \n",
       "2       0.325875         0.002004         0.947107          0.968119   \n",
       "3       0.319463         0.001594         0.946829          0.963937   \n",
       "4       0.305019         0.001213         0.945239          0.961135   \n",
       "5       0.333493         0.001805         0.943371          0.959168   \n",
       "6       0.340520         0.002013         0.943650          0.957588   \n",
       "7       0.346927         0.001601         0.944087          0.956496   \n",
       "8       0.322070         0.001807         0.942299          0.954369   \n",
       "9       0.317050         0.002203         0.940431          0.952581   \n",
       "\n",
       "  param_min_samples_split                      params  rank_test_score  \\\n",
       "0                       5    {'min_samples_split': 5}                1   \n",
       "1                      25   {'min_samples_split': 25}                2   \n",
       "2                      45   {'min_samples_split': 45}                3   \n",
       "3                      65   {'min_samples_split': 65}                4   \n",
       "4                      85   {'min_samples_split': 85}                5   \n",
       "5                     105  {'min_samples_split': 105}                8   \n",
       "6                     125  {'min_samples_split': 125}                7   \n",
       "7                     145  {'min_samples_split': 145}                6   \n",
       "8                     165  {'min_samples_split': 165}                9   \n",
       "9                     185  {'min_samples_split': 185}               10   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score       ...         \\\n",
       "0           0.892133            0.997914           0.961263       ...          \n",
       "1           0.896106            0.982613           0.960469       ...          \n",
       "2           0.896702            0.974367           0.958482       ...          \n",
       "3           0.898888            0.971336           0.954311       ...          \n",
       "4           0.898490            0.969001           0.953317       ...          \n",
       "5           0.897100            0.967909           0.952721       ...          \n",
       "6           0.897696            0.967163           0.951728       ...          \n",
       "7           0.901867            0.965872           0.951728       ...          \n",
       "8           0.900874            0.962842           0.949146       ...          \n",
       "9           0.896305            0.961749           0.949146       ...          \n",
       "\n",
       "   split2_test_score  split2_train_score  split3_test_score  \\\n",
       "0           0.964825            0.995082           0.967409   \n",
       "1           0.962838            0.973823           0.962242   \n",
       "2           0.960652            0.966918           0.963235   \n",
       "3           0.960652            0.960610           0.961645   \n",
       "4           0.959857            0.958375           0.956280   \n",
       "5           0.956479            0.954798           0.955882   \n",
       "6           0.956876            0.954550           0.957472   \n",
       "7           0.955882            0.952017           0.956479   \n",
       "8           0.953696            0.950228           0.955286   \n",
       "9           0.952504            0.948242           0.952901   \n",
       "\n",
       "   split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
       "0            0.993195           0.962440            0.994039      0.030260   \n",
       "1            0.974270           0.960254            0.975512      0.021007   \n",
       "2            0.966124           0.956479            0.966769      0.012503   \n",
       "3            0.961951           0.958665            0.964087      0.020113   \n",
       "4            0.957481           0.958267            0.962249      0.010617   \n",
       "5            0.956984           0.954690            0.959468      0.026337   \n",
       "6            0.955295           0.954491            0.956189      0.030960   \n",
       "7            0.954351           0.954491            0.956189      0.034165   \n",
       "8            0.953308           0.952504            0.954252      0.017321   \n",
       "9            0.951073           0.951312            0.952513      0.033348   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.000801        0.028820         0.001718  \n",
       "1        0.000488        0.026159         0.003518  \n",
       "2        0.000636        0.025306         0.003136  \n",
       "3        0.000494        0.024106         0.003868  \n",
       "4        0.000411        0.023480         0.004259  \n",
       "5        0.000401        0.023175         0.004616  \n",
       "6        0.001093        0.023070         0.004821  \n",
       "7        0.000488        0.021176         0.004871  \n",
       "8        0.000402        0.020813         0.004472  \n",
       "9        0.001947        0.022105         0.004811  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = tree.cv_results_\n",
    "pd.DataFrame(scores).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAELCAYAAAAoUKpTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VdW5+P/PkzkhcwIhkEDCIDNh\nCIoiZRSxrSii1rnaOg+9t/eHt3pt1dqfrbXe21artWhxqlWo1ql1QBBUEJUgg8xhCBACJAwJYQhk\neL5/7J3kJGQinMM5SZ7367VfOWfvdfZ5zg6cJ2utvdYSVcUYY4xpSpC/AzDGGBP4LFkYY4xpliUL\nY4wxzbJkYYwxplmWLIwxxjTLkoUxxphmWbIwxhjTLEsWxhhjmmXJwhhjTLNC/B2AtyQnJ2tGRoa/\nwzDGmDZl+fLl+1S1c3Pl2k2yyMjIICcnx99hGGNMmyIi21tSzpqhjDHGNMuShTHGmGZZsjDGGNOs\ndtNnYYxpnfLycvLz8ykrK/N3KMaHIiIiSEtLIzQ0tFWvt2RhTAeXn59PTEwMGRkZiIi/wzE+oKrs\n37+f/Px8MjMzW3UOa4YypoMrKysjKSnJEkU7JiIkJSWdVu3RkoUxxhJFB3C6v+MOnyzKyit5eWke\nu4qP+TsUY4wJWB0+Wew/coJf/Wsdzy7a4u9QjOmQiouLeeaZZ1r12u9+97sUFxc3WebBBx9k/vz5\nrTq/qdXhk0X3+EguH5nGnGU72VNid4MYc6Y1lSwqKyubfO37779PfHx8k2UeeeQRJk+e3Or4/KGi\nosLfIZykwycLgDvH96FSlWc/tdqFMWfafffdx5YtWxg2bBj33nsvixYtYsKECVxzzTUMGTIEgEsv\nvZSRI0cyaNAgZs2aVfPajIwM9u3bR15eHgMGDOCWW25h0KBBTJkyhWPHnKblG2+8kTfeeKOm/EMP\nPcSIESMYMmQIGzZsAKCoqIgLLriAESNGcNttt9GzZ0/27dt3Uqx33HEH2dnZDBo0iIceeqhm/7Jl\nyzjvvPPIysri7LPPprS0lMrKSmbOnMmQIUMYOnQoTz31VJ2YAXJychg/fjwADz/8MLfeeitTpkzh\nhhtuIC8vj7FjxzJixAhGjBjBF198UfN+jz/+OEOGDCErK6vm+o0YMaLmeG5uLiNHjjzt340nu3UW\nSE+MYvrw7rz29Q7unNCbLjER/g7JGL/45XtrWVdwyKvnHNgtlocuHtTo8ccee4w1a9awcuVKABYt\nWsTXX3/NmjVram7znD17NomJiRw7doxRo0YxY8YMkpKS6pwnNzeX1157jeeee44rr7ySN998k+uu\nu+6k90tOTuabb77hmWee4YknnuD555/nl7/8JRMnTuT+++/nww8/rJOQPD366KMkJiZSWVnJpEmT\nWL16Nf379+cHP/gBc+bMYdSoURw6dIjIyEhmzZrFtm3bWLFiBSEhIRw4cKDZa7V8+XIWL15MZGQk\nR48e5eOPPyYiIoLc3FyuvvpqcnJy+OCDD3j77bf56quviIqK4sCBAyQmJhIXF8fKlSsZNmwYL7zw\nAjfeeGOz73cqrGbhumtCH8orq3jus63+DsWYDu/ss8+uMx7gySefJCsri9GjR7Nz505yc3NPek1m\nZibDhg0DYOTIkeTl5TV47ssuu+ykMosXL+aqq64CYOrUqSQkJDT42rlz5zJixAiGDx/O2rVrWbdu\nHRs3biQ1NZVRo0YBEBsbS0hICPPnz+f2228nJMT5mzwxMbHZzz1t2jQiIyMBZ7DkLbfcwpAhQ7ji\niitYt24dAPPnz+emm24iKiqqznlvvvlmXnjhBSorK5kzZw7XXHNNs+93Kqxm4cpM7sQlw7rzty93\ncPu43iRFh/s7JGPOuKZqAGdSp06dah4vWrSI+fPns3TpUqKiohg/fnyD4wXCw2v/zwYHB9c0QzVW\nLjg4uKZvQFWbjWnbtm088cQTLFu2jISEBG688UbKyspQ1QZvS21sf0hICFVVVQAnfQ7Pz/373/+e\nlJQUVq1aRVVVFREREU2ed8aMGTU1pJEjR55U8zpdVrPwcNeEPpRVVPL84m3+DsWYDiMmJobS0tJG\nj5eUlJCQkEBUVBQbNmzgyy+/9HoM559/PnPnzgVg3rx5HDx48KQyhw4dolOnTsTFxbF3714++OAD\nAPr3709BQQHLli0DoLS0lIqKCqZMmcKzzz5bk5Cqm6EyMjJYvnw5AG+++WajMZWUlJCamkpQUBCv\nvPJKTWf/lClTmD17NkePHq1z3oiICC688ELuuOMObrrpptO+JvVZsvDQp0s03xuSystf5HHwyAl/\nh2NMh5CUlMSYMWMYPHgw995770nHp06dSkVFBUOHDuUXv/gFo0eP9noMDz30EPPmzWPEiBF88MEH\npKamEhMTU6dMVlYWw4cPZ9CgQfzoRz9izJgxAISFhTFnzhzuuecesrKyuOCCCygrK+Pmm2+mR48e\nDB06lKysLP7+97/XvNd//Md/MHbsWIKDgxuN6c477+Sll15i9OjRbNq0qabWMXXqVKZNm0Z2djbD\nhg3jiSeeqHnNtddei4gwZcoUb18ipCXVr7YgOztbvbH40YY9h5j6h8/5ycQ+/NeUfl6IzJjAtn79\negYMGODvMPzq+PHjBAcHExISwtKlS7njjjtqOtzbkieeeIKSkhJ+9atfNXi8od+1iCxX1ezmzm19\nFvX07xrL1EFdeWFJHj8e24u4yNbN0GiMaTt27NjBlVdeSVVVFWFhYTz33HP+DumUTZ8+nS1btvDJ\nJ5/45PyWLBpwz6Q+fLh2Dy99kcdPJvX1dzjGGB/r27cvK1as8HcYp+Wtt97y6fmtz6IBg7rFMXlA\nF/66eBulZeX+DscYY/zOkkUj7pnYl5Jj5by8tEVrmRtjTLtmyaIRWenxjDurM39dvI0jxwNvnhZj\njDmTfJosRGSqiGwUkc0icl8Dx3uKyAIRWS0ii0QkzePYb0Vkjbv9wJdxNuYnk/py4MgJXv3KahfG\nmI7NZ8lCRIKBp4GLgIHA1SIysF6xJ4CXVXUo8AjwG/e13wNGAMOAc4B7RSTWV7E2ZmTPBM7vk8ys\nz7Zx7ETTs18aY1rndKYoB/jDH/5QM0DN+I4vaxZnA5tVdauqngBeBy6pV2YgsMB9vNDj+EDgU1Wt\nUNUjwCpgqg9jbdQ9E/uw7/BxXvt6hz/e3ph2rz0ki0CcUtzbfJksugM7PZ7nu/s8rQJmuI+nAzEi\nkuTuv0hEokQkGZgApPsw1kad0yuJczITefbTLZSVW+3CGG+rP0U5wO9+9ztGjRrF0KFDa6YCP3Lk\nCN/73vfIyspi8ODBzJkzhyeffJKCggImTJjAhAkTTjr3I488wqhRoxg8eDC33nprzRxQmzdvZvLk\nyWRlZTFixAi2bHGWJ6g/9TfA+PHjqR7wu2/fPjIyMgB48cUXueKKK7j44ouZMmUKhw8fZtKkSTXT\nn7/zzjs1cbz88ss1I7mvv/56SktLyczMpLzcudvy0KFDZGRk1DwPRL4cZ9HQgq/1h4vPBP4kIjcC\nnwG7gApVnScio4AvgCJgKXBS6haRW4FbAXr06OG9yOv5yaS+XPv8V/wjZyfXn5vhs/cxxu8+uA/2\nfOvdc3YdAhc91ujh+lOUz5s3j9zcXL7++mtUlWnTpvHZZ59RVFREt27d+Pe//w04cyfFxcXxf//3\nfyxcuJDk5OSTzn333Xfz4IMPAnD99dfzr3/9i4svvphrr72W++67j+nTp1NWVkZVVVWDU383Z+nS\npaxevZrExEQqKip46623iI2NZd++fYwePZpp06axbt06Hn30UZYsWUJycjIHDhwgJiaG8ePH8+9/\n/5tLL72U119/nRkzZhAaGriDgH1Zs8inbm0gDSjwLKCqBap6maoOBx5w95W4Px9V1WGqegFO4jlp\nTmJVnaWq2aqa3blzZ199Ds7rncTIngn8edEWTlRU+ex9jDFOspg3bx7Dhw9nxIgRbNiwgdzcXIYM\nGcL8+fP52c9+xueff05cXFyz51q4cCHnnHMOQ4YM4ZNPPmHt2rWUlpaya9cupk+fDjgT8EVFRTU6\n9XdTLrjggppyqsr//M//MHToUCZPnsyuXbvYu3cvn3zyCZdffnlNMqs/pTjACy+84JPJ/7zJlzWL\nZUBfEcnEqTFcBdSZYN1tYjqgqlXA/cBsd38wEK+q+0VkKDAUmOfDWJskIvxkUl9+OPtr3vwmn6vP\n9l0txhi/aqIGcKaoKvfffz+33XbbSceWL1/O+++/z/3338+UKVNqag0NKSsr48477yQnJ4f09HQe\nfvjhminFG3vf05lS/NVXX6WoqIjly5cTGhpKRkZGk1OYjxkzhry8PD799FMqKysZPHhwo58lEPis\nZqGqFcDdwEfAemCuqq4VkUdEZJpbbDywUUQ2ASnAo+7+UOBzEVkHzAKuc8/nN9/pm0xWWhxPL9xM\neaXVLozxlvpTlF944YXMnj2bw4cPA7Br1y4KCwspKCggKiqK6667jpkzZ/LNN980+Ppq1V/sycnJ\nHD58uGZp1djYWNLS0nj77bcBZxLBo0ePNjr1t+eU4tXnaEhJSQldunQhNDSUhQsXsn27c8v9pEmT\nmDt3Lvv3769zXoAbbriBq6++OuBrFeDjuaFU9X3g/Xr7HvR4/AZw0tVX1TKcO6ICRnXt4scv5fDW\nil1cme2X/nZj2h3PKcovuugifve737F+/XrOPfdcAKKjo/nb3/7G5s2buffeewkKCiI0NJQ///nP\nANx6661cdNFFpKamsnDhwprzxsfH16w0l5GRUbOSHcArr7zCbbfdxoMPPkhoaCj/+Mc/mDp1KitX\nriQ7O5uwsDC++93v8utf/5qZM2dy5ZVX8sorrzBx4sRGP8e1117LxRdfXDN1eP/+/QEYNGgQDzzw\nAOPGjSM4OJjhw4fz4osv1rzm5z//OVdffbW3L6vX2RTlp0BV+f5TizlyvIL5/zWOkGAbAG/aPpui\n3H/eeOMN3nnnHV555ZUz8n6nM0W5fdudAhHhnol9ydt/lPdWFzT/AmOMacQ999zDfffdxy9+8Qt/\nh9IiNkX5KZoyMIX+XWP40yebmZbVneCghu4QNsaYpj311FP+DuGUWM3iFAUFCXdP7MOWoiO8/+1u\nf4djjFe0l+Zo07jT/R1bsmiFiwan0qdLNE99kktVlf0nM21bREQE+/fvt4TRjqkq+/fvJyIiotXn\nsGaoVggOEu6e0If/nLOSeev2MHVwqr9DMqbV0tLSyM/Pp6ioyN+hGB+KiIggLS2t+YKNsGTRSt8f\nmsofF+Ty5ILNXDioa4ODboxpC0JDQ8nMzPR3GCbAWTNUK4UEB3HXhD6s232IBesL/R2OMcb4lCWL\n03DJsG6kJ0by5Ce51t5rjGnXLFmchtDgIO4a34fV+SUs2mTtvcaY9suSxWm6bEQa3eMjeXKB1S6M\nMe2XJYvTFBYSxO3je7NiRzFLNu/3dzjGGOMTliy84MrsNLrGRvDkJyctuWGMMe2CJQsvCA8J5rZx\nvfh62wG+3Gq1C2NM+2PJwkuuPrsHydHhPLnAahfGmPbHkoWXRIQGc9t3evHFlv3k5DW/dq8xxrQl\nliy86NrRPUjsFMaTn2z2dyjGGONVliy8KCoshFvG9uKzTUWs3Fns73CMMcZrLFl42fXn9iQ+KpSn\nrO/CGNOOWLLwsujwEH48JpMFGwpZs6vE3+EYY4xXWLLwgR+OySAmIoSnbNyFMaadsGThA7ERodw0\nJpOP1u5l/e5D/g7HGGNOmyULH/nRmAyiw0P4k90ZZYxpByxZ+Eh8VBg3nNuT99fsJndvqb/DMcaY\n02LJwoduHtuLyNBg/rTQahfGmLbNkoUPJXYK47rRPXlvVQFbiw77OxxjjGk1SxY+dsvYXoQGB/H0\nwi3+DsUYY1rNp8lCRKaKyEYR2Swi9zVwvKeILBCR1SKySETSPI49LiJrRWS9iDwpIuLLWH2lc0w4\n157Tk7dX7mLH/qP+DscYY1rFZ8lCRIKBp4GLgIHA1SIysF6xJ4CXVXUo8AjwG/e15wFjgKHAYGAU\nMM5XsfrabeN6ERwkPLPI+i6MMW2TL2sWZwObVXWrqp4AXgcuqVdmILDAfbzQ47gCEUAYEA6EAnt9\nGKtPpcRGcNWodN5Ynk/+QatdGGPaHl8mi+7ATo/n+e4+T6uAGe7j6UCMiCSp6lKc5LHb3T5S1fU+\njNXnbh/XGxF49lPruzDGtD2+TBYN9TFoveczgXEisgKnmWkXUCEifYABQBpOgpkoIt856Q1EbhWR\nHBHJKSoq8m70XtYtPpLLR6Yzd1k+u0uO+TscY4w5Jb5MFvlAusfzNKDAs4CqFqjqZao6HHjA3VeC\nU8v4UlUPq+ph4ANgdP03UNVZqpqtqtmdO3f21efwmjvH96ZKlb98utXfoRhjzCnxZbJYBvQVkUwR\nCQOuAt71LCAiySJSHcP9wGz38Q6cGkeIiITi1DradDMUQHpiFNOHd+e1r3dQeKjM3+EYY0yL+SxZ\nqGoFcDfwEc4X/VxVXSsij4jINLfYeGCjiGwCUoBH3f1vAFuAb3H6NVap6nu+ivVMumtCH8orq5j1\nmdUujDFth6jW70Zom7KzszUnJ8ffYbTIT+es5MM1e/j8ZxNIjg73dzjGmA5MRJaranZz5WwEtx/c\nNaEPZRWVPP/5Nn+HYowxLWLJwg/6dInm+0O78fLSPA4eOeHvcIwxplmWLPzk7gl9OHqiktlLrHZh\njAl8liz8pF/XGC4a3JUXl+RRcqzc3+EYY0yTLFn40d0T+1B6vIIXl+T5OxRjjGmSJQs/GtQtjskD\nUvjr4q2UllntwhgTuCxZ+NlPJvXhUFkFLy/d7u9QjDGmUZYs/GxoWjzj+3Xm+c+3cuR4hb/DMcaY\nBlmyCAD3TOzLwaPl/O1Lq10YYwKTJYsAMLJnAuf3Sea5z63vwhgTmCxZBIifXuDULi7/81J2HrAF\nkowxgcWSRYAY2TORF28axe6SY1zy9BK+2rrf3yEZY0wNSxYBZGzfzrx91xjio0K59vmveO3rHf4O\nyRhjAEsWAadX52jeunMM5/VJ5v5/fsvD766lorLK32EZYzo4SxYBKC4ylNk/zOZHYzJ58Ys8bnpx\nGSVHrePbGOM/liwCVEhwEA9ePJDHZwzly637mf7MErYUHfZ3WMaYDsqSRYC7clQ6f79lNMXHyrn0\n6SV8tqnI3yEZYzogSxZtwKiMRN65awzd4yO58YWvmb14G+1lhUNjTNtgyaKNSE+M4s07zmPSgBQe\n+dc67v/nt5yosI5vY8yZ0WyyEJG7RSThTARjmtYpPIS/XDeSuyf04fVlO7nu+a/Yf/i4v8MyxnQA\nLalZdAWWichcEZkqIuLroEzjgoKEmRf2449XDWNVfjGXPL2EDXsO+TssY0w712yyUNWfA32BvwI3\nArki8msR6e3j2EwTLhnWnbm3ncuJiipmPPMF89bu8XdIxph2rEV9Fur0pu5xtwogAXhDRB73YWym\nGVnp8bx3z/n06RLNbX9bztMLN1vHtzHGJ1rSZ/ETEVkOPA4sAYao6h3ASGCGj+MzzUiJjWDObedy\n8dBu/O6jjfznnJWUlVf6OyxjTDsT0oIyycBlqlpnsQVVrRKR7/smLHMqIkKD+eNVw+jXNYbffbSR\nvH1HmHVDNimxEf4OzRjTTrSkGep94ED1ExGJEZFzAFR1va8CM6dGRLhrQh/+cv1IcgsPM+1Pi1md\nX+zvsIwx7URLksWfAc95Jo64+0wAunBQV9684zxCgoK44tmlvLeqwN8hGWPagZYkC1GPXlNVraJl\nzVfGTwakxvLu3WMYmhbHPa+t4H/nbaSqyjq+jTGt15JksdXt5A51t/8Atrbk5O64jI0isllE7mvg\neE8RWSAiq0VkkYikufsniMhKj61MRC49tY/WsSVFh/PqzaP5QXY6T32ymTteXc6R4xX+DssY00a1\nJFncDpwH7ALygXOAW5t7kYgEA08DFwEDgatFZGC9Yk8AL6vqUOAR4DcAqrpQVYep6jBgInAUmNei\nT2RqhIUE8diMITz4/YF8vG4vM/78BfkHbclWY8ypa8mgvEJVvUpVu6hqiqpeo6qFLTj32cBmVd2q\nqieA14FL6pUZCCxwHy9s4DjA5cAHqmrfcq0gIvzo/ExeuOlsdhUf45I/LSEn70DzLzTGGA8tGWcR\nISJ3icgzIjK7emvBubsDOz2e57v7PK2idqzGdCBGRJLqlbkKeK2R2G4VkRwRySkqsqm7mzLuLGfJ\n1tjIUK5+7kvm5uxs/kXGGONqSTPUKzjzQ10IfAqkAaUteF1Dc0jV72WdCYwTkRXAOJymrpqGdRFJ\nBYYAHzX0Bqo6S1WzVTW7c+fOLQipY+vdOZq37xzDOZlJ/Pcbq/n//7WOSuv4Nsa0QEuSRR9V/QVw\nRFVfAr6H8wXenHwg3eN5GlDnPk5VLVDVy1R1OPCAu6/Eo8iVwFuqamuKeklcVCgv3jSKG8/L4PnF\n2/jRi8s4VGaX1xjTtJYki+pvkmIRGQzEARkteN0yoK+IZIpIGE5z0rueBUQkWUSqY7gfqN+8dTWN\nNEGZ1gsJDuLhaYP49fQhLNm8j+lPL2HbviP+DssYE8BakixmuetZ/Bzny34d8NvmXqSqFcDdOE1I\n64G5qrpWRB4RkWlusfHARhHZBKQAj1a/XkQycGomn7b0w5hTc805Pfjbzedw4MgJLn16CUs27/N3\nSMaYACVNzVLq/tV/uarOPXMhtU52drbm5OT4O4w2aeeBo9z8Ug6biw7z0MUDueHcDH+HZIw5Q0Rk\nuapmN1euyZqFO1r7bq9FZQJSemIUb955HhP6debBd9Zy35urKSwt83dYxpgA0pJmqI9FZKaIpItI\nYvXm88jMGRUdHsKs67O5c3xv5uTs5PzHFvLfb6xi096W3PhmjGnvmmyGAhCRbQ3sVlXt5ZuQWsea\nobxn274jzF68jX8s30lZeRXjzurMLWN7MaZPEraqrjHtS0uboZpNFm2FJQvvO3jkBH/7cjsvLd3O\nvsPHGZAayy1jM/n+0G6EhbRokUVjTIDzWrIQkRsa2q+qL7cyNp+wZOE7ZeWVvLuygOc+30pu4WFS\nYsO58bxMrjmnB3GRof4OzxhzGryZLJ7yeBoBTAK+UdXLTy9E77Jk4XuqyqJNRTz/+VaWbN5PVFgw\nPxiVzo/GZJKeGOXv8IwxreCzZigRiQNeUdVpzRY+gyxZnFlrC0r46+fbeHdVAVWqXDQ4lZvHZjK8\nR4K/QzPGnAJfJotQYLWqDmhtcL5gycI/dpcc48Uv8vj7VzsoLatgVEYCN4/txeQBKQQHWWe4MYHO\nm81Q71E7AWAQzrTic1X1pMWM/MmShX8dPl7B3GU7+evibewqPkZGUhQ/Pj+Ty0emExkW7O/wjDGN\n8GayGOfxtALYrqr5pxmf11myCAwVlVV8uHYPz32+jVU7i0mICuW60T254dwMOseE+zs8Y0w93kwW\nmcBuVS1zn0cCKaqa541AvcWSRWBRVXK2H+S5z7by8fq9hAYFcenwbtw8thdnpcT4OzxjjKulySKk\nBef6B86yqtUq3X2jWhmb6QBEhFEZiYzKSGTbviP8dfFW3liez9ycfMb3cwb5ndfbBvkZ01a0pGax\n0l0L23PfKlXN8mlkp8hqFoHvgDvI7+Wleew7fIKBqbHc8h1nkF9osA3yM8YfvDKRoKvIY0pxROQS\nwOayNqcssVMYP5nUl8U/m8hvZwyhvLKKn85ZxdjfLuTZT7dQcswWYTImULWkZtEbeBXo5u7KB25Q\n1c0+ju2UWM2i7amqUj7NrR3k1yksmB+M6sFNYzJskJ8xZ4jXx1mISLRbPiCnIbVk0batLSjh+c+3\n8V71IL8hqfzw3AxG9kyw8RrG+JA374b6NfC4qha7zxOA/09Vf+6VSL3EkkX7sLvkGC8uyePvXzuD\n/JI6hTG+XxcuGNiFsX070ym8JfdkGGNaypvJYoWqDq+37xtVHXGaMXqVJYv25fDxCj7ZUMiC9XtZ\nuKGQQ2UVhAUHMbp3EpMHdGHSgBS6x0f6O0xj2jxvJovVwChVPe4+jwRyVHWQVyL1EksW7Vd5ZRU5\neQdZsH4vCzYUsm3fEQAGpMbWJI6h3eMIsuYqY06ZN5PFfwPTgBfcXTcB76rq46cdpRdZsug4thQd\nZsH6vcxfX0hO3gGqFDrHhDOxXxcmD0zh/D7JNsWIMS3k1Q5uEZkKTAYEOAikqupdpx2lF1my6JgO\nHjnBok2FzF9fyGcbiyg9XkF4SBBj+iQzaUAXJvVPoWtchL/DNCZgeXMEN8AeoAq4EtgGvHkasRnj\nNQmdwpg+PI3pw9M4UVHFsrwDfLxuLws27OWTDYU8wBqGdI9j0oAuTB6QwqBusTZq3JhWaLRmISJn\nAVcBVwP7gTnATFXteebCazmrWRhPqkpu4WHmr9/LgvWFfLPjIKrQNTaiJnGc2zuJiFBrrjId22k3\nQ4lIFfA58OPqAXgislVVe3k1Ui+xZGGasu/wcRZuKGTB+kI+yy3i6IlKIkODOb9vMpMHdGFi/xSb\nFdd0SN5ohpqBU7NYKCIfAq/j9FkY0+YkR4dzRXY6V2Snc7yiki+3HmD+ur0sWL+Xj9ftReRbstLi\na+6u6t81xpqrjPHQkruhOgGX4jRHTQReAt5S1Xm+D6/lrGZhWkNVWb+71Lm7akMhq3YWA9A9PrKm\nuerszERrrjLtlk+WVRWRROAK4AeqOvE04vM6SxbGGwoPlfHJBufuqsWbiygrryI0WBiQGktWWjxZ\n6fEMS4+jV3K0jesw7YLP1uA+xSCmAn8EgoHnVfWxesd7ArOBzsAB4LrqVfhEpAfwPJCOs6zrd5ta\ncMmShfG2svJKvtiyj2V5B1m1s5jV+SUcPl4BQEx4CEPT4zwSSDwpsXaLrml7/J4sRCQY2ARcgDNT\n7TLgalVd51HmH8C/VPUlEZkI3KSq17vHFgGPqurH7iSGVap6tLH3s2RhfK2yStladJiVO4tZlV/M\nyp3FbNhdSkWV838oNS6iJnlkpccxpHscMRGhfo7amKZ5e5xFa5wNbFbVrW5ArwOXAOs8ygwEfuo+\nXgi87ZYdCISo6scAqnrYh3F1wVCVAAAZWElEQVQa0yLBQULflBj6psRwRXY64NQ+1hYcYpVHAvlw\n7R4ARKBP52iGpdfWPvp1jbGFnkyb5Mtk0R3Y6fE8HzinXplVOHdd/RGYDsSISBJwFlAsIv8EMoH5\nwH2qWunDeI05ZRGhwYzsmcDIngk1+w4eOcGq/GJW7SxhVX4xCzYU8o/l+QCEhwQxqFssw9ITyEqP\nY1h6PD0So+zOKxPwfJksGvrXX7/NaybwJxG5EfgM2AVUuHGNBYYDO3AGBN4I/LXOG4jcCtwK0KNH\nD+9FbsxpSHCnVR/frwvg3HGVf/CY03zl1kD+/vV2Zi+pAiA+KpSsNKfmMSw9nqFpcSRF25gPE1h8\nmSzycTqnq6UBBZ4FVLUAuAxqFleaoaolIpIPrPBownobGE29ZKGqs4BZ4PRZ+OhzGHNaRIT0xCjS\nE6O4OMtZcLK8sopNe0ud2sdOp/nqydxcqrsQ0xMj6ySQQd3ibHJE41e+TBbLgL4ikolTY7gKuMaz\ngIgkAwdUtQq4H+fOqOrXJohIZ1UtwhnfYb3Xpt0IDQ5iULc4BnWL45pznFrx4eMVrNlVmzy+2X6Q\nf63eDTj9JZnJneiXEsNZKTGclRLNWV1j6JkYRYj1gZgzwGfJQlUrRORu4COcW2dnq+paEXkEZz2M\nd4HxwG9ERHGaoe5yX1spIjOBBeI05i4HnvNVrMYEgujwEEb3SmJ0r6SafYWHyliV7ySQDXtK+XZX\nCe+v2V1TAwkLDqJ3l2gnebiJpF9KDGkJkTYOxHiVT8dZnEl266zpKI6eqGBz4WE27T3Mpr2lzran\nlIKSspoykaHB9K1JILWJJDUuwjrTTR2BcOusMcYHosJCGJoWz9C0+Dr7D5WVk7v3MLl7S9m4t5Tc\nvYf5dFMRb7h3YoEzmLBvSjT9usbQt0uM8zMlms7R4ZZETJMsWRjTTsRGhJ50Gy84t/Ju2lvKpsLD\nbNrjJJIP1+zhtaO1d7YnRIXW1D7O6hrDWV2c2khCp7Az/TFMgLJkYUw7l9ApjHN6JXGOR1+IqlJ0\n+Di5ew+zcU8puYWlbNxTytsrdlHqTmkCznK1/VKc2kc/d0BiemIkyZ3CrU+kg7FkYUwHJCJ0iYmg\nS0wEY/ok1+xXVXaXlNX2hbj9Iq9/vZNj5bVjYsOCg0iNj6BbXCTdEyLpFh9J9/gIusU7j7vFRdqt\nvu2MJQtjTA0RqfnCrx5UCFBV5QwszC0sZVfxMXYVH6OguIyC4mMs2byPvYfKqKp3r0xipzC6eSSU\n7tWJJD6SbvERVjtpYyxZGGOaFRQk9EiKokdSVIPHyyur2HuojILiMnYVH3V/HqOg+Bh5+4+wZPM+\njpyoO1uP1U7aFksWxpjTFhocRFpCFGkJUUDiScdVlUNlFRQUH2PXwWMUlJxe7SQ9IYp+XWMIC7EB\niWeKJQtjjM+JCHGRocRFhjIgNbbBMqdaOwkPCSIrLZ4RPRPIdu8Cs7u3fMeShTEmIJxK7WRr0RG+\n2XGQnO0Hef7zrTz7qVMl6dW5U03iGNkzkd6dO9n4ES+xEdzGmDatrLySVTuLWb7jIMvzDrJ8x0GK\nj5YDzoy+I3skMDIjgeyeiQxNi7P11OuxEdzGmA4hIjS4zjgSVWVL0RGWbz/A8u1O7WPBhkIAQoOF\nQd3iGOnRdNXFlsNtEatZGGPavQNHTvDN9oM1tY9V+cUcr3DWE0lPjCS7Z2JN38dZKTEEd6Bbev2+\nBveZZsnCGNNSJyqqWFtQwvLtB2tqH0WlxwFn/qxhPeLJ7pnIyJ4JDOsRT3R4+22EsWRhjDEtVL2a\nYc72A+TkOQlk495SVCFIYEBqbM28WyN7JtA9PrLddJxbsjDGmNNwqKyclTuKydl+kG+2H2TFjoM1\nt+52jY1gZM8EhveIZ3D3OAZ2iyU2ItTPEbeOdXAbY8xpiI0I5TtndeY7Z3UGoKKyig17Sp1bdt3a\nx7+/3V1TvmdSFIO6xTKom5M8BneLo3NM+1lL3WoWxhjTSkWlx1lbUMLagkM1P7fvP1pzvEtMeE0C\nGdzd+ZmWEFhNWFazMMYYH+scE874fl3qTLp4qKyc9QWHWOMmkHUFh/gsdx+V7lwmsREh7vrrsQxy\nE0iv5E4Bv5a6JQtjjPGi2IjQk9YPKSuvZOOeUtYWHGKNWwN55cvtNbfvRoQG0b9rbJ1ayFkpMQE1\ngNCShTHG+FhEaDBZ6fFkpdcuhVtRWcXWfUdYs6u2GevdVQW8+tUOAEKChD5domtrId1iGdgtlhg/\ndaRbn4UxxgQIVWXngWOsLSipqYGsLThUMwYEICMpqqYTfVC3WAZ3jyM5uvUd6dZnYYwxbYxI7boh\nFw1JrdlfeKisTif66l3Fde7EOjsjkbm3n+vT2CxZGGNMgOsSG0GX2Agm9K/tSC85Vs46N4GciXU9\nLFkYY0wbFBcZyrm9kzi3d1Lzhb0gsO/VMsYYExAsWRhjjGmWJQtjjDHN8mmyEJGpIrJRRDaLyH0N\nHO8pIgtEZLWILBKRNI9jlSKy0t3e9WWcxhhjmuazDm4RCQaeBi4A8oFlIvKuqq7zKPYE8LKqviQi\nE4HfANe7x46p6jBfxWeMMablfFmzOBvYrKpbVfUE8DpwSb0yA4EF7uOFDRw3xhgTAHyZLLoDOz2e\n57v7PK0CZriPpwMxIlJ9H1iEiOSIyJcicqkP4zTGGNMMXyaLhubgrT+3yExgnIisAMYBu4AK91gP\ndwj6NcAfRKT3SW8gcqubUHKKiopaF6Uq7PkWyg617vXGGNMB+HJQXj6Q7vE8DSjwLKCqBcBlACIS\nDcxQ1RKPY6jqVhFZBAwHttR7/SxgFjhzQ7UqyqMH4NnzncdRSRDfExIyTt5iu0OwjWE0xnRMvvz2\nWwb0FZFMnBrDVTi1hBoikgwcUNUq4H5gtrs/ATiqqsfdMmOAx30SZWgEXPkyHMyr3XavhPXvQlVF\nbbmgEIhLbziRJGRAZPxJpzbGmPbCZ8lCVStE5G7gIyAYmK2qa0XkESBHVd8FxgO/EREFPgPucl8+\nAPiLiFThNJU9Vu8uKu8J6wQDG+hXr6yA0oK6SaR6W/8uHN1ft3xEvEfyqFc7iUuH4La5Pq8xxoBN\nUd56ZYegeHu9ROI+L94OlSdqy0oQxKU1UivJhMgECKBlFo0xHYdNUe5rEbHQdYiz1VdVBaW7G66V\nbPwQjhTWLR8eW682kgmJmc7PuHTrKzHG+J19C/lCUBDEdXe2jDEnHz9xpLYWUr0Vb4eiTbBpHlQe\n9ziX21dSnTwSe9U+TsiAsKgz85mMMR2aJQt/COsEKQOdrb6aWsk2OLCt7s9d30BZcd3y0V09Ekm9\nn1GJ1rxljPEKSxaBpk6t5PyTjx876JFEtsKBPOfx1kWw6u91y4bHOrWPhpJJbHcICpzF4I0xgc2S\nRVsTmQDdE6D7iJOPlR9zm7fq1Ur2roUN70NVeW3Z4DCI71GbPBJ71U0koZGWTIwxNSxZtCehkdCl\nv7PVV1UJh3ad3LR1YBvs/AqONzCCPSgUQsLdLcJJMCERHvvq74+AEI8ywR5lQjzKtPQ8oVGWsIwJ\nEJYsOoqgYKcmEd8DZ2YVD6rOSPbq5HFoF1Qch4oy5xbgijJ3q3583OmErzgOx0sb3l/92tMRHA6d\nz4IuA6HLAOdn5/5Oh3+QLcVizJlkycI4neCdkpwtrdnbrVuuqqo22VR6JJQKz4Ti8bh+0indA0Ub\nIG8xrJ5Te96waCdpVCeQLv2dn9Ep1qFvjI9YsjC+ExQEQRHOlCqn61ixkzgK10Gh+3Pj+7Dildoy\nkQketZAB0Nn9GZV4+u9vTAdnycK0DZHx0GO0s3k6XOQkjppEsh5Wz63bBxPd1aMWUp1I+kF4zJn9\nDMa0YZYsTNsW3Rmix0Evj34YVaffpdAjgRSug5zZUHGstlx8j9oEUl0LST7LOzUhY9oZSxam/RFx\n5uKKS4O+k2v3V1U6o+U9ayGF62HzgtrbiiUIEnvX1kC6DICUIc6txdapbjowSxam4wgKhqTeztb/\ne7X7K8th/5a6tZC9a2H9e9Ss1xUeC6lZztZtuLMlZFoCMR2GJQtjgkMbHp9SfgyKNsKe1VCw0lnn\n5OvnaufuCo+D1KFu8hhWm0DsjizTDlmyMKYxoZFuEhgGI25w9lWWO7WPghVO8ihYAV89WzumJCIO\nUofVJo/UYc6UK5ZATBtnycKYUxEc6tQmUocCP3T2VZxwmq52r3RqIAUrYOkztf0gEfF1k0e3Yc7y\nvZZAWqeq0lnFMiTc35F0KJYsjDldIWG1NZCR7r6K404CqU4eu1fCF3+qTSCRCW7i8GjCiktvfwmk\nshxOHHam5T/u/qx+3uLH9Z5X39GW2NtJ2l3d5N01y7k7zviErZRnzJlScdzpOK9uvipY6SSU6rXe\nIxPrJo/UYc4dXd5OIFWVJ0/N4jl6vqmR9RVlUH605V/upzLlS0iEM31/WCdnlH6Dj93nWuVcyz2r\noXhH7Tliurk1v6zaJNIek7AX2Up5xgSakHBntmDPGYPLy6BwbW3yKFgJS/5Ym0Cikp3kkTrM+aJs\n9Eu+qSlV6u33nH24tUI7nfxlHhHvJLeGvtybexzaqfUrQh49AHu+dRLH7tWwexXkznMSCji1OM/a\nR2qWc0ecTVJ5SqxmYUygKS9z/mou+Ka2H6RwPWilc7x6dt5mZ++tv7+hWYDrzw7c1HncfaFRgX/L\n8Imjbs1jlZM8dq92anHVNZ3QKEgZ7N4O7TZldRnQIftBWlqzsGRhTFtQccL5Szk4LPC/qANVZXnt\nrdDVCWTPt3Ci1Dke5N5C3TWrtikrZTCER/s3bh+zZGGMMc2pqnKm5t+9qm4z1tF9bgFxmqy6DvWo\nhWQ5MzS3E9ZnYYwxzQkKqh3VP/gyZ58qlO52ax5u8sjPgbX/rH1dbFpt7aP6duiYrv75DGeIJQtj\njPEkArHdnK3f1Nr9dTrS3WasTR/WdqTHpJ58O3R0F/98Bh+wZGGMMS0RlejMbuw5w/GJI04Cqbmb\nbYWTQKrnFIvpVps8qhNJGx0LYsnCGGNaK6zTyeusHC91E4jHgMyN71OTQGK7e4zmdxNJp2S/hH8q\nLFkYY4w3hcdAz/OcrdrxUqfZynNOsQ3/qj0el+4xo/EwSB0ecJ3oliyMMcbXwmMgY4yzVSsrce++\n8hjRXyeB9IBuWXVrIX5cItinyUJEpgJ/BIKB51X1sXrHewKzgc7AAeA6Vc33OB4LrAfeUtW7fRmr\nMcacURFxkDnW2aodK3anxPfoA1n/Xu3x+B51k0dq1hlLID5LFiISDDwNXADkA8tE5F1VXedR7Ang\nZVV9SUQmAr8Brvc4/ivgU1/FaIwxASUyHjK/42zVjh107r7y7ANZ907t8YQM6DMZvve/Pg3NlzWL\ns4HNqroVQEReBy4BPJPFQOCn7uOFwNvVB0RkJJACfAg0O2DEGGPapcgE6DXe2aodPeAmEDd5VM8l\n5kO+TBbdgZ0ez/OBc+qVWQXMwGmqmg7EiEgScBD4X5xaxiQfxmiMMW1PVCL0nuBsZ4gvJ5lpaE7g\n+nOLzATGicgKYBywC6gA7gTeV9WdNEFEbhWRHBHJKSoq8kbMxhhjGuDLmkU+kO7xPA0o8CygqgXA\nZQAiEg3MUNUSETkXGCsidwLRQJiIHFbV++q9fhYwC5y5oXz2SYwxpoPzZbJYBvQVkUycGsNVwDWe\nBUQkGTigqlXA/Th3RqGq13qUuRHIrp8ojDHGnDk+a4ZS1QrgbuAjnNtf56rqWhF5RESmucXGAxtF\nZBNOZ/ajvorHGGNM69kU5cYY04G1dIpyW0XFGGNMsyxZGGOMaZYlC2OMMc1qN30WIlIEbG/kcDKw\nr5FjgaStxAltJ1aL0/vaSqwWZ8v0VNVmF9loN8miKSKS05IOHH9rK3FC24nV4vS+thKrxeld1gxl\njDGmWZYsjDHGNKujJItZ/g6ghdpKnNB2YrU4va+txGpxelGH6LMwxhhzejpKzcIYY8xpaPfJQkSm\nishGEdksIgEzGaGIpIvIQhFZLyJrReQ/3P0Pi8guEVnpbt8NgFjzRORbN54cd1+iiHwsIrnuzwQ/\nx9jP45qtFJFDIvKfgXI9RWS2iBSKyBqPfQ1eQ3E86f6bXS0iI/wc5+9EZIMby1siEu/uzxCRYx7X\n9lk/x9no71pE7nev50YRufBMxdlErHM84swTkZXufr9d02aparvdcNb+3gL0AsJwFlsa6O+43NhS\ngRHu4xhgE87KgQ8DM/0dX71Y84DkevseB+5zH98H/Nbfcdb7ve8BegbK9QS+A4wA1jR3DYHvAh/g\nrAkzGvjKz3FOAULcx7/1iDPDs1wAXM8Gf9fu/6tVQDiQ6X4nBPsz1nrH/xd40N/XtLmtvdcsapZ2\nVdUTQPXSrn6nqrtV9Rv3cSnOzLzd/RvVKbkEeMl9/BJwqR9jqW8SsEVVGxukecap6mfAgXq7G7uG\nl+CsTa+q+iUQLyKp/opTVeepM4s0wJc4a9P4VSPXszGXAK+r6nFV3QZsxvluOCOailVEBLgSeO1M\nxdNa7T1ZNLS0a8B9IYtIBjAc+Mrddbdb5Z/t7+YdlwLzRGS5iNzq7ktR1d3gJD6gi9+iO9lV1P3P\nF2jXs1pj1zCQ/93+CKfWUy1TRFaIyKciMtZfQXlo6HcdyNdzLLBXVXM99gXaNQXaf7JoydKufiXO\nCoFvAv+pqoeAPwO9gWHAbpwqqr+NUdURwEXAXSLyHX8H1BgRCQOmAf9wdwXi9WxOQP67FZEHcJY9\nftXdtRvooarDgf8C/i4isf6Kj8Z/1wF5PV1XU/cPm0C7pjXae7JodmlXfxKRUJxE8aqq/hNAVfeq\naqU6qwc+xxmsLjdGneVvUdVC4C2cmPZWN424Pwv9F2EdFwHfqOpeCMzr6aGxaxhw/25F5IfA94Fr\n1W1cd5t19ruPl+P0BZzlrxib+F0H3PUEEJEQnGWl51TvC7Rr6qm9J4uapV3dvzivAt71c0xATVvl\nX4H1qvp/Hvs926anA2vqv/ZMEpFOIhJT/Rins3MNznX8oVvsh8A7/onwJHX+Ugu061lPY9fwXeAG\n966o0UBJdXOVP4jIVOBnwDRVPeqxv7OIBLuPewF9ga3+ibLJ3/W7wFUiEi7OMs99ga/PdHwNmAxs\nUNX86h2Bdk3r8HcPu683nDtLNuFk6Af8HY9HXOfjVIVXAyvd7bvAK8C37v53gVQ/x9kL506SVcDa\n6msIJAELgFz3Z2IAXNMoYD8Q57EvIK4nTgLbDZTj/KX748auIU6zydPuv9lvcdag92ecm3Ha/Kv/\nnT7rlp3h/ptYBXwDXOznOBv9XQMPuNdzI3CRv3/37v4XgdvrlfXbNW1usxHcxhhjmtXem6GMMcZ4\ngSULY4wxzbJkYYwxplmWLIwxxjTLkoUxxphmWbIwxhjTLEsWpl0SkWkSQFPSN8adnjrZD++bUT1l\ntohki8iT7uPxInLemY7HBL4QfwdgjC+o6rsEyGj9QKeqOUCO+3Q8cBj4wm8BmYBkNQvT5rh/FW8Q\nkedFZI2IvCoik0VkiTgLCZ0tIjeKyJ/c8i+Ks5jQFyKyVUQub+LcqSLymbvwzJrqWT9F5M8ikiPO\nQlW/9CifJyK/FpGl7vERIvKRiGwRkdvdMuPdc74lIutE5FkROen/nohcJyJfu+/9FxEJdrcX3Vi+\nFZGfNhH7T9zzrxaR1919D4vIKyLyiXttbmngdeNF5F/izH58O/BTN4aAmfHU+J/VLExb1Qe4ArgV\nZw6wa3CmUJkG/A/wdr3yqe7x/jg1jjcaOe81wEeq+qg7R0+Uu/8BVT3g7lsgIkNVdbV7bKeqnisi\nv8eZwmEMEIEzbUP1Smdn4yzCsx34EGcCuZoYRGQA8AOcGX7LReQZ4Fr3HN1VdbBbLr6Ja3IfkKmq\nx+uVG4qziFInYIWI/LuhF6tqnjgrsx1W1SeaeB/TAVnNwrRV21T1W3VmGF0LLFBn7ppvcVYbq+9t\nVa1S1XVAShPnXQbcJCIPA0PUWZgK4EoR+QZYAQzC+eKvVt3c9S3OqnalqloElHl8aX+tziJclThz\nBZ1f730nASOBZeIssTkJZ16urUAvEXnKndDvUBOxrwZeFZHrcKYSr/aOqh5T1X3AQgJr5l3TRliy\nMG3VcY/HVR7Pq2i4xuxZvqH1DYCaVc2+A+wCXhGRG9yZSmcCk1R1KPBvnJpD/XN7xlE/lvqTsNV/\nLsBLqjrM3fqp6sOqehDIAhYBdwHPNxY78D2cCQhHAsvdKbBb8t7GNMuShTEeRKQnUKiqz+FMIT8C\niAWOACUikoKzZsapOtudKj8Ip7lpcb3jC4DLRaSLG0eiiPR075QKUtU3gV+48TQUdxCQrqoLgf8G\n4oFo9/AlIhIhIkk4HdjLmoizFGdNeGPqsD4LY+oaD9wrIuU4dwXdoKrbRGQFTnPXVmBJK867FHgM\nGAJ8hrOIVA1VXSciP8dZvjYIZzrru4BjwAseHeL3N3L+YOBvIhKHU0v5vaoWiwg4azf8G+gB/EpV\nC9zO7Ia8B7whIpcA96jq5634rKYdsinKjfExERkPzFTV7/vhvR/GOqyNF1gzlDHGmGZZzcJ0SCIy\nBGdlNU/HVfUcf8RzKkTkaZzbcz39UVVf8Ec8pmOwZGGMMaZZ1gxljDGmWZYsjDHGNMuShTHGmGZZ\nsjDGGNMsSxbGGGOa9f8AKQSiSOuL5LkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting accuracies with min_samples_leaf\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': range(5, 15, 5), 'min_samples_leaf': range(50, 150, 50), 'min_samples_split': range(50, 150, 50), 'criterion': ['entropy', 'gini']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "    'criterion': [\"entropy\", \"gini\"]\n",
    "}\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, \n",
    "                          cv = n_folds, verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048931</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.935848</td>\n",
       "      <td>0.939813</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931209</td>\n",
       "      <td>0.940176</td>\n",
       "      <td>0.936640</td>\n",
       "      <td>0.942076</td>\n",
       "      <td>0.935168</td>\n",
       "      <td>0.937195</td>\n",
       "      <td>0.008999</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.042518</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.935848</td>\n",
       "      <td>0.939813</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931209</td>\n",
       "      <td>0.940176</td>\n",
       "      <td>0.936640</td>\n",
       "      <td>0.942076</td>\n",
       "      <td>0.935168</td>\n",
       "      <td>0.937195</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.049130</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.934835</td>\n",
       "      <td>0.938835</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931571</td>\n",
       "      <td>0.939995</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.932633</td>\n",
       "      <td>0.936923</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>0.001854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.044511</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.934835</td>\n",
       "      <td>0.938835</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931571</td>\n",
       "      <td>0.939995</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.932633</td>\n",
       "      <td>0.936923</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>0.001854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.053348</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.938962</td>\n",
       "      <td>0.946528</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.947416</td>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.946330</td>\n",
       "      <td>0.941326</td>\n",
       "      <td>0.945973</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.000573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.057763</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.938962</td>\n",
       "      <td>0.946528</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.947416</td>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.946330</td>\n",
       "      <td>0.941326</td>\n",
       "      <td>0.945973</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.000573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.045515</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.937948</td>\n",
       "      <td>0.941188</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936278</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.936255</td>\n",
       "      <td>0.940995</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.001492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.045921</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.937948</td>\n",
       "      <td>0.941188</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936278</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.936255</td>\n",
       "      <td>0.940995</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.001492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.039116</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.936355</td>\n",
       "      <td>0.939740</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931209</td>\n",
       "      <td>0.938546</td>\n",
       "      <td>0.938450</td>\n",
       "      <td>0.941714</td>\n",
       "      <td>0.935893</td>\n",
       "      <td>0.937557</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.002105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.037520</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.936862</td>\n",
       "      <td>0.939740</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.933744</td>\n",
       "      <td>0.938546</td>\n",
       "      <td>0.938450</td>\n",
       "      <td>0.941714</td>\n",
       "      <td>0.935893</td>\n",
       "      <td>0.937557</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.002105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.036708</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.936934</td>\n",
       "      <td>0.939125</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932657</td>\n",
       "      <td>0.937913</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.940719</td>\n",
       "      <td>0.933720</td>\n",
       "      <td>0.937285</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>0.001642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.035505</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.936934</td>\n",
       "      <td>0.939125</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932657</td>\n",
       "      <td>0.937913</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.940719</td>\n",
       "      <td>0.933720</td>\n",
       "      <td>0.937285</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>0.001642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.047732</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.939251</td>\n",
       "      <td>0.946365</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938088</td>\n",
       "      <td>0.945515</td>\n",
       "      <td>0.939537</td>\n",
       "      <td>0.947054</td>\n",
       "      <td>0.943861</td>\n",
       "      <td>0.945792</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.046526</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.939758</td>\n",
       "      <td>0.946365</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940623</td>\n",
       "      <td>0.945515</td>\n",
       "      <td>0.939537</td>\n",
       "      <td>0.947054</td>\n",
       "      <td>0.943861</td>\n",
       "      <td>0.945792</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.041128</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.939107</td>\n",
       "      <td>0.940935</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937002</td>\n",
       "      <td>0.941352</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.940719</td>\n",
       "      <td>0.936617</td>\n",
       "      <td>0.941086</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.000496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.040918</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.939107</td>\n",
       "      <td>0.940935</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937002</td>\n",
       "      <td>0.941352</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.940719</td>\n",
       "      <td>0.936617</td>\n",
       "      <td>0.941086</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.000496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        0.048931         0.001605         0.935848          0.939813   \n",
       "1        0.042518         0.001805         0.935848          0.939813   \n",
       "2        0.049130         0.002006         0.934835          0.938835   \n",
       "3        0.044511         0.001806         0.934835          0.938835   \n",
       "4        0.053348         0.001204         0.938962          0.946528   \n",
       "5        0.057763         0.001803         0.938962          0.946528   \n",
       "6        0.045515         0.001608         0.937948          0.941188   \n",
       "7        0.045921         0.001399         0.937948          0.941188   \n",
       "8        0.039116         0.001805         0.936355          0.939740   \n",
       "9        0.037520         0.001199         0.936862          0.939740   \n",
       "10       0.036708         0.001810         0.936934          0.939125   \n",
       "11       0.035505         0.001404         0.936934          0.939125   \n",
       "12       0.047732         0.001208         0.939251          0.946365   \n",
       "13       0.046526         0.001411         0.939758          0.946365   \n",
       "14       0.041128         0.001608         0.939107          0.940935   \n",
       "15       0.040918         0.001609         0.939107          0.940935   \n",
       "\n",
       "   param_criterion param_max_depth param_min_samples_leaf  \\\n",
       "0          entropy               5                     50   \n",
       "1          entropy               5                     50   \n",
       "2          entropy               5                    100   \n",
       "3          entropy               5                    100   \n",
       "4          entropy              10                     50   \n",
       "5          entropy              10                     50   \n",
       "6          entropy              10                    100   \n",
       "7          entropy              10                    100   \n",
       "8             gini               5                     50   \n",
       "9             gini               5                     50   \n",
       "10            gini               5                    100   \n",
       "11            gini               5                    100   \n",
       "12            gini              10                     50   \n",
       "13            gini              10                     50   \n",
       "14            gini              10                    100   \n",
       "15            gini              10                    100   \n",
       "\n",
       "   param_min_samples_split                                             params  \\\n",
       "0                       50  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "1                      100  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "2                       50  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "3                      100  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "4                       50  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "5                      100  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "6                       50  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "7                      100  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "8                       50  {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "9                      100  {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "10                      50  {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "11                     100  {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "12                      50  {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "13                     100  {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "14                      50  {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "15                     100  {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "\n",
       "    rank_test_score       ...         split2_test_score  split2_train_score  \\\n",
       "0                13       ...                  0.931209            0.940176   \n",
       "1                13       ...                  0.931209            0.940176   \n",
       "2                15       ...                  0.931571            0.939995   \n",
       "3                15       ...                  0.931571            0.939995   \n",
       "4                 5       ...                  0.940261            0.947416   \n",
       "5                 5       ...                  0.940261            0.947416   \n",
       "6                 7       ...                  0.936278            0.943796   \n",
       "7                 7       ...                  0.936278            0.943796   \n",
       "8                12       ...                  0.931209            0.938546   \n",
       "9                11       ...                  0.933744            0.938546   \n",
       "10                9       ...                  0.932657            0.937913   \n",
       "11                9       ...                  0.932657            0.937913   \n",
       "12                2       ...                  0.938088            0.945515   \n",
       "13                1       ...                  0.940623            0.945515   \n",
       "14                3       ...                  0.937002            0.941352   \n",
       "15                3       ...                  0.937002            0.941352   \n",
       "\n",
       "    split3_test_score  split3_train_score  split4_test_score  \\\n",
       "0            0.936640            0.942076           0.935168   \n",
       "1            0.936640            0.942076           0.935168   \n",
       "2            0.935554            0.939271           0.932633   \n",
       "3            0.935554            0.939271           0.932633   \n",
       "4            0.935192            0.946330           0.941326   \n",
       "5            0.935192            0.946330           0.941326   \n",
       "6            0.935554            0.939271           0.936255   \n",
       "7            0.935554            0.939271           0.936255   \n",
       "8            0.938450            0.941714           0.935893   \n",
       "9            0.938450            0.941714           0.935893   \n",
       "10           0.940261            0.940719           0.933720   \n",
       "11           0.940261            0.940719           0.933720   \n",
       "12           0.939537            0.947054           0.943861   \n",
       "13           0.939537            0.947054           0.943861   \n",
       "14           0.940261            0.940719           0.936617   \n",
       "15           0.940261            0.940719           0.936617   \n",
       "\n",
       "    split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0             0.937195      0.008999        0.000496        0.004853   \n",
       "1             0.937195      0.003389        0.000401        0.004853   \n",
       "2             0.936923      0.002286        0.000634        0.005448   \n",
       "3             0.936923      0.005688        0.001607        0.005448   \n",
       "4             0.945973      0.004718        0.000401        0.003174   \n",
       "5             0.945973      0.003011        0.000400        0.003174   \n",
       "6             0.940995      0.001865        0.000494        0.003538   \n",
       "7             0.940995      0.001482        0.000496        0.003538   \n",
       "8             0.937557      0.001562        0.000751        0.004943   \n",
       "9             0.937557      0.001202        0.000396        0.004499   \n",
       "10            0.937285      0.004324        0.000759        0.004226   \n",
       "11            0.937285      0.001359        0.000491        0.004226   \n",
       "12            0.945792      0.001201        0.000399        0.004025   \n",
       "13            0.945792      0.000489        0.000486        0.004006   \n",
       "14            0.941086      0.000626        0.000485        0.002554   \n",
       "15            0.941086      0.000731        0.000484        0.002554   \n",
       "\n",
       "    std_train_score  \n",
       "0          0.002492  \n",
       "1          0.002492  \n",
       "2          0.001854  \n",
       "3          0.001854  \n",
       "4          0.000573  \n",
       "5          0.000573  \n",
       "6          0.001492  \n",
       "7          0.001492  \n",
       "8          0.002105  \n",
       "9          0.002105  \n",
       "10         0.001642  \n",
       "11         0.001642  \n",
       "12         0.000772  \n",
       "13         0.000772  \n",
       "14         0.000496  \n",
       "15         0.000496  \n",
       "\n",
       "[16 rows x 24 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy 0.9397581637824922\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=50, min_samples_split=100,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print(\"best accuracy\", grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=50, min_samples_split=50,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model with optimal hyperparameters\n",
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                  random_state = 100,\n",
    "                                  max_depth=10, \n",
    "                                  min_samples_leaf=50,\n",
    "                                  min_samples_split=50)\n",
    "clf_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9402027027027027"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score\n",
    "clf_gini.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'edelweiss_decision_tree_model.sav'\n",
    "pickle.dump(clf_gini, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
