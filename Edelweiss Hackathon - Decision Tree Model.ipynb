{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Train = pd.read_csv(\"Train_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19731 entries, 0 to 19730\n",
      "Data columns (total 51 columns):\n",
      "Unnamed: 0               19731 non-null int64\n",
      "Agreement_ID             19731 non-null int64\n",
      "Foreclosure              19731 non-null int64\n",
      "Customer_ID              19731 non-null int64\n",
      "MOB                      19731 non-null int64\n",
      "Loan_Amt                 19731 non-null object\n",
      "NET_DISBURSED_AMT        19731 non-null object\n",
      "Interest_Start_Date      19731 non-null object\n",
      "Current_ROI              19731 non-null float64\n",
      "Original_ROI             19731 non-null float64\n",
      "Current_Tenure           19731 non-null float64\n",
      "Original_Tenure          19731 non-null int64\n",
      "Due_Day                  19731 non-null int64\n",
      "Authorization_Date       19731 non-null object\n",
      "City                     19731 non-null object\n",
      "Pre_EMI_Due_Amt          19731 non-null float64\n",
      "Pre_EMI_Received_Amt     19731 non-null float64\n",
      "PRE_EMI_OS_AMOUNT        19731 non-null float64\n",
      "EMI_Due_Amt              19731 non-null float64\n",
      "EMI_Received_Amt         19731 non-null float64\n",
      "EMI_OS_AMOUNT            19731 non-null float64\n",
      "Excess_Available         19731 non-null float64\n",
      "Excess_Adjusted_Amt      19731 non-null float64\n",
      "Balance_Excess           19731 non-null float64\n",
      "Net_Receivable           19731 non-null float64\n",
      "Outstanding_Principal    19731 non-null float64\n",
      "Paid_Principal           19731 non-null float64\n",
      "Paid_Interest            19731 non-null float64\n",
      "Month_Opening            19731 non-null float64\n",
      "Last_Receipt_Date        19663 non-null object\n",
      "LAST_RECEIPT_AMOUNT      19514 non-null float64\n",
      "Net_LTV                  19731 non-null float64\n",
      "Completed_Tenure         19731 non-null int64\n",
      "Balance_Tenure           19731 non-null float64\n",
      "DPD                      19731 non-null float64\n",
      "FOIR                     19731 non-null float64\n",
      "Product                  19731 non-null object\n",
      "Scheme_ID                19731 non-null float64\n",
      "NPA_In_Last_Month        127 non-null object\n",
      "NPA_In_Current_Month     127 non-null object\n",
      "Cust_Const_Type_ID       5992 non-null float64\n",
      "Cust_Category_ID         5992 non-null float64\n",
      "Age                      5746 non-null float64\n",
      "Gender                   5747 non-null object\n",
      "Marital_Status           5746 non-null object\n",
      "Qualification            5742 non-null object\n",
      "No_Of_Dependent          5949 non-null float64\n",
      "Gross_Income             5992 non-null float64\n",
      "Pre_Job_Years            1407 non-null float64\n",
      "Net_Take_Home_Income     5992 non-null float64\n",
      "Branch_Pincode           5949 non-null float64\n",
      "dtypes: float64(31), int64(8), object(12)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "Train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_Amt</th>\n",
       "      <th>NET_DISBURSED_AMT</th>\n",
       "      <th>Interest_Start_Date</th>\n",
       "      <th>Authorization_Date</th>\n",
       "      <th>City</th>\n",
       "      <th>Last_Receipt_Date</th>\n",
       "      <th>Product</th>\n",
       "      <th>NPA_In_Last_Month</th>\n",
       "      <th>NPA_In_Current_Month</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Qualification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,17,10,107.24</td>\n",
       "      <td>1,17,10,107.24</td>\n",
       "      <td>30-Aug-10</td>\n",
       "      <td>29-Aug-10</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>05-May-14</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>POSTGRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1,92,90,253.32</td>\n",
       "      <td>1,92,90,253.32</td>\n",
       "      <td>15-Sep-10</td>\n",
       "      <td>15-Sep-10</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>01-Nov-13</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>POSTGRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39,33,395.00</td>\n",
       "      <td>39,33,395.00</td>\n",
       "      <td>01-Nov-10</td>\n",
       "      <td>02-Nov-10</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>05-Aug-17</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>GRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,00,22,587.71</td>\n",
       "      <td>1,00,22,587.71</td>\n",
       "      <td>06-Oct-10</td>\n",
       "      <td>06-Oct-10</td>\n",
       "      <td>THANE</td>\n",
       "      <td>02-May-18</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>POSTGRAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77,55,937.31</td>\n",
       "      <td>77,55,937.31</td>\n",
       "      <td>26-Oct-10</td>\n",
       "      <td>26-Oct-10</td>\n",
       "      <td>MUMBAI</td>\n",
       "      <td>05-Apr-18</td>\n",
       "      <td>HL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>UG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Loan_Amt NET_DISBURSED_AMT Interest_Start_Date Authorization_Date  \\\n",
       "0   1,17,10,107.24    1,17,10,107.24            30-Aug-10          29-Aug-10   \n",
       "1   1,92,90,253.32    1,92,90,253.32            15-Sep-10          15-Sep-10   \n",
       "2     39,33,395.00      39,33,395.00            01-Nov-10          02-Nov-10   \n",
       "3   1,00,22,587.71    1,00,22,587.71            06-Oct-10          06-Oct-10   \n",
       "4     77,55,937.31      77,55,937.31            26-Oct-10          26-Oct-10   \n",
       "\n",
       "     City Last_Receipt_Date Product NPA_In_Last_Month NPA_In_Current_Month  \\\n",
       "0  MUMBAI         05-May-14      HL               NaN                  NaN   \n",
       "1  MUMBAI         01-Nov-13      HL               NaN                  NaN   \n",
       "2  MUMBAI         05-Aug-17      HL               NaN                  NaN   \n",
       "3   THANE         02-May-18      HL               NaN                  NaN   \n",
       "4  MUMBAI         05-Apr-18      HL               NaN                  NaN   \n",
       "\n",
       "  Gender Marital_Status Qualification  \n",
       "0      M              M      POSTGRAD  \n",
       "1      M              M      POSTGRAD  \n",
       "2      M              M          GRAD  \n",
       "3      M              M      POSTGRAD  \n",
       "4      M              M            UG  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# encode categorical variables using Label Encoder\n",
    "\n",
    "# select all categorical variables\n",
    "df_categorical = Train.select_dtypes(include=['object'])\n",
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TrainX_merged = pd.read_csv(\"TrainX_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_categorical = df_categorical[['City','Product','Gender','Marital_Status','Qualification']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.Gender = df_categorical.Gender.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_categorical.Marital_Status = df_categorical.Marital_Status.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_categorical.Qualification = df_categorical.Qualification.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Product</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Qualification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   City  Product  Gender  Marital_Status  Qualification\n",
       "0   145        0       1               0              5\n",
       "1   145        0       1               0              5\n",
       "2   145        0       1               0              2\n",
       "3   222        0       1               0              5\n",
       "4   145        0       1               0              7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply Label encoder to df_categorical\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_categorical = df_categorical.apply(le.fit_transform)\n",
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data = pd.concat([TrainX_merged, df_categorical],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data = Train_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Net_Disbursed_Amt</th>\n",
       "      <th>ROI_Change_Ratio</th>\n",
       "      <th>Tenure_Ratio</th>\n",
       "      <th>Tenure_Change_Ratio</th>\n",
       "      <th>Foreclosure</th>\n",
       "      <th>Agreement_ID</th>\n",
       "      <th>Net_LTV</th>\n",
       "      <th>City</th>\n",
       "      <th>Product</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Qualification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117.10</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "      <td>11220001</td>\n",
       "      <td>40.06</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.90</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1</td>\n",
       "      <td>11220002</td>\n",
       "      <td>84.31</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.33</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>1</td>\n",
       "      <td>11220006</td>\n",
       "      <td>50.89</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.23</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1</td>\n",
       "      <td>11220008</td>\n",
       "      <td>84.63</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.56</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>1</td>\n",
       "      <td>11220010</td>\n",
       "      <td>30.94</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.54</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0</td>\n",
       "      <td>11220011</td>\n",
       "      <td>23.24</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66.06</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0</td>\n",
       "      <td>11220012</td>\n",
       "      <td>60.69</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>329.60</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1</td>\n",
       "      <td>11220014</td>\n",
       "      <td>87.75</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>84.07</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>11220016</td>\n",
       "      <td>84.25</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.76</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>0</td>\n",
       "      <td>11220017</td>\n",
       "      <td>23.77</td>\n",
       "      <td>222</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>146.65</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1</td>\n",
       "      <td>11220020</td>\n",
       "      <td>48.63</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>43.44</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1</td>\n",
       "      <td>11220021</td>\n",
       "      <td>77.43</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40.81</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>11220022</td>\n",
       "      <td>85.00</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21.67</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>1</td>\n",
       "      <td>11220025</td>\n",
       "      <td>84.96</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>37.53</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>11220026</td>\n",
       "      <td>63.13</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>90.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1</td>\n",
       "      <td>11220029</td>\n",
       "      <td>54.64</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.13</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>11220030</td>\n",
       "      <td>46.17</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>638.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0</td>\n",
       "      <td>11220032</td>\n",
       "      <td>85.14</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>600.52</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0</td>\n",
       "      <td>11220034</td>\n",
       "      <td>43.31</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>61.57</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>11220036</td>\n",
       "      <td>78.06</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>78.29</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>11220037</td>\n",
       "      <td>65.85</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>69.20</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>1</td>\n",
       "      <td>11220039</td>\n",
       "      <td>40.97</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>502.93</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>11220040</td>\n",
       "      <td>63.09</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>76.57</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>11220041</td>\n",
       "      <td>45.54</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>327.43</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0</td>\n",
       "      <td>11220042</td>\n",
       "      <td>25.47</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>233.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0</td>\n",
       "      <td>11220043</td>\n",
       "      <td>73.06</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>145.25</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0</td>\n",
       "      <td>11220044</td>\n",
       "      <td>60.00</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>78.07</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0</td>\n",
       "      <td>11220046</td>\n",
       "      <td>78.61</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>53.82</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>1</td>\n",
       "      <td>11220048</td>\n",
       "      <td>76.08</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>75.82</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "      <td>11220050</td>\n",
       "      <td>48.25</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>27.62</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>1</td>\n",
       "      <td>11220112</td>\n",
       "      <td>47.79</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>75.06</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>11220113</td>\n",
       "      <td>39.57</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>90.08</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0</td>\n",
       "      <td>11220114</td>\n",
       "      <td>71.61</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>12.84</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>11220115</td>\n",
       "      <td>22.21</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75.82</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0</td>\n",
       "      <td>11220117</td>\n",
       "      <td>56.11</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>441.38</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0</td>\n",
       "      <td>11220118</td>\n",
       "      <td>74.57</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>210.18</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>11220120</td>\n",
       "      <td>56.98</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1005.87</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0</td>\n",
       "      <td>11220122</td>\n",
       "      <td>59.99</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>165.14</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>11220124</td>\n",
       "      <td>69.99</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>48.64</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>11220126</td>\n",
       "      <td>79.98</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>525.45</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1</td>\n",
       "      <td>11220128</td>\n",
       "      <td>58.30</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>45.04</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>11220129</td>\n",
       "      <td>48.57</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>100.84</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0</td>\n",
       "      <td>11220130</td>\n",
       "      <td>55.80</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>25.52</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>11220132</td>\n",
       "      <td>79.14</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>159.33</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0</td>\n",
       "      <td>11220133</td>\n",
       "      <td>70.40</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>52.55</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0</td>\n",
       "      <td>11220134</td>\n",
       "      <td>11.16</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>45.99</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>11220135</td>\n",
       "      <td>66.98</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>120.10</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1</td>\n",
       "      <td>11220136</td>\n",
       "      <td>34.84</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>82.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1</td>\n",
       "      <td>11220137</td>\n",
       "      <td>63.43</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>46.72</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>11220138</td>\n",
       "      <td>54.59</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>74.36</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>11220139</td>\n",
       "      <td>76.37</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>34.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1</td>\n",
       "      <td>11220143</td>\n",
       "      <td>42.58</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>21.93</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>11220144</td>\n",
       "      <td>7.14</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>29.85</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>11220145</td>\n",
       "      <td>9.72</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>36.56</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>11220146</td>\n",
       "      <td>7.53</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>900.78</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0</td>\n",
       "      <td>11220150</td>\n",
       "      <td>61.53</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1</td>\n",
       "      <td>11220152</td>\n",
       "      <td>6.76</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>30.19</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>1</td>\n",
       "      <td>11220153</td>\n",
       "      <td>49.43</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>56.61</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>1</td>\n",
       "      <td>11220154</td>\n",
       "      <td>54.65</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>30.03</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>11220155</td>\n",
       "      <td>63.56</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Net_Disbursed_Amt  ROI_Change_Ratio  Tenure_Ratio  Tenure_Change_Ratio  \\\n",
       "0              117.10             -0.26          1.00                 0.62   \n",
       "1              192.90             -0.29          0.28                 0.24   \n",
       "2               39.33             -0.36          0.26                -0.73   \n",
       "3              100.23             -0.06          1.00                 0.49   \n",
       "4               77.56             -0.38          0.29                -0.69   \n",
       "5                9.54             -0.17          0.42                -0.31   \n",
       "6               66.06             -0.49          0.25                -0.30   \n",
       "7              329.60             -0.26          0.16                -0.09   \n",
       "8               84.07             -0.21          0.33                -0.00   \n",
       "9                9.76             -0.08          0.25                -1.16   \n",
       "10             146.65             -0.43          0.98                 0.72   \n",
       "11              43.44             -0.13          0.97                 0.61   \n",
       "12              40.81             -0.21          1.00                 0.00   \n",
       "13              21.67             -0.29          0.19                -0.28   \n",
       "14              37.53             -0.19          0.42                -0.23   \n",
       "15              90.08             -0.10          0.40                 0.37   \n",
       "16              46.13             -0.38          0.37                 0.03   \n",
       "17             638.05             -0.17          0.39                -0.38   \n",
       "18             600.52             -0.39          0.34                -0.49   \n",
       "19              61.57             -0.39          0.23                 0.35   \n",
       "20              78.29             -0.32          0.16                -0.17   \n",
       "21              69.20             -0.30          0.16                -0.99   \n",
       "22             502.93             -0.20          0.13                -0.75   \n",
       "23              76.57             -0.12          0.33                -0.43   \n",
       "24             327.43             -0.36          0.25                -1.10   \n",
       "25             233.08              0.00          0.64                -0.22   \n",
       "26             145.25             -0.34          0.21                -0.39   \n",
       "27              78.07             -0.13          0.41                -0.36   \n",
       "28              53.82             -0.24          0.17                -0.52   \n",
       "29              75.82             -0.28          0.98                 0.77   \n",
       "..                ...               ...           ...                  ...   \n",
       "70              27.62             -0.20          0.18                -0.49   \n",
       "71              75.06             -0.10          0.72                 0.80   \n",
       "72              90.08             -0.23          0.77                -0.13   \n",
       "73              12.84             -0.22          0.17                -0.54   \n",
       "74              75.82             -0.10          0.56                -0.18   \n",
       "75             441.38             -0.15          0.30                -0.18   \n",
       "76             210.18             -0.02          0.15                -0.05   \n",
       "77            1005.87             -0.15          0.12                -0.35   \n",
       "78             165.14             -0.12          0.23                -0.17   \n",
       "79              48.64             -0.15          0.17                -0.10   \n",
       "80             525.45             -0.03          0.68                 0.29   \n",
       "81              45.04             -0.27          0.64                -0.15   \n",
       "82             100.84             -0.09          0.39                -0.18   \n",
       "83              25.52             -0.12          0.92                 0.75   \n",
       "84             159.33             -0.02          0.55                 0.34   \n",
       "85              52.55              0.05          0.47                -0.04   \n",
       "86              45.99              0.02          0.37                -0.04   \n",
       "87             120.10             -0.04          0.14                -0.11   \n",
       "88              82.27              0.03          0.32                 0.19   \n",
       "89              46.72             -0.12          0.20                -0.18   \n",
       "90              74.36             -0.02          0.13                 0.04   \n",
       "91              34.24              0.06          1.00                 0.64   \n",
       "92              21.93             -0.15          0.20                -0.23   \n",
       "93              29.85             -0.15          0.14                -0.23   \n",
       "94              36.56             -0.15          0.20                -0.23   \n",
       "95             900.78              0.01          0.44                -0.12   \n",
       "96               0.38             -0.02          0.27                -0.02   \n",
       "97              30.19             -0.15          0.13                -0.24   \n",
       "98              56.61             -0.15          0.10                -0.24   \n",
       "99              30.03             -0.26          0.30                -0.64   \n",
       "\n",
       "    Foreclosure  Agreement_ID  Net_LTV  City  Product  Gender  Marital_Status  \\\n",
       "0             1      11220001    40.06   145        0       1               0   \n",
       "1             1      11220002    84.31   145        0       1               0   \n",
       "2             1      11220006    50.89   145        0       1               0   \n",
       "3             1      11220008    84.63   222        0       1               0   \n",
       "4             1      11220010    30.94   145        0       1               0   \n",
       "5             0      11220011    23.24   222        0       1               0   \n",
       "6             0      11220012    60.69   145        0       1               0   \n",
       "7             1      11220014    87.75   145        0       1               0   \n",
       "8             1      11220016    84.25   222        0       1               0   \n",
       "9             0      11220017    23.77   222        1       2               2   \n",
       "10            1      11220020    48.63   145        0       1               0   \n",
       "11            1      11220021    77.43   145        0       0               1   \n",
       "12            1      11220022    85.00   145        0       0               0   \n",
       "13            1      11220025    84.96   222        0       1               1   \n",
       "14            0      11220026    63.13   159        0       1               0   \n",
       "15            1      11220029    54.64   145        0       1               1   \n",
       "16            1      11220030    46.17   145        0       1               0   \n",
       "17            0      11220032    85.14   159        0       1               0   \n",
       "18            0      11220034    43.31   145        1       1               0   \n",
       "19            1      11220036    78.06   145        0       1               0   \n",
       "20            1      11220037    65.85   145        0       1               1   \n",
       "21            1      11220039    40.97   145        0       1               0   \n",
       "22            0      11220040    63.09   145        1       2               2   \n",
       "23            0      11220041    45.54   145        1       1               0   \n",
       "24            0      11220042    25.47   145        0       1               0   \n",
       "25            0      11220043    73.06   145        1       1               0   \n",
       "26            0      11220044    60.00   145        1       2               2   \n",
       "27            0      11220046    78.61   222        0       1               0   \n",
       "28            1      11220048    76.08   145        0       1               0   \n",
       "29            0      11220050    48.25   145        1       0               0   \n",
       "..          ...           ...      ...   ...      ...     ...             ...   \n",
       "70            1      11220112    47.79    20        0       1               0   \n",
       "71            1      11220113    39.57   145        0       2               2   \n",
       "72            0      11220114    71.61   145        1       2               2   \n",
       "73            0      11220115    22.21    20        1       2               2   \n",
       "74            0      11220117    56.11   145        1       2               2   \n",
       "75            0      11220118    74.57   145        1       2               2   \n",
       "76            1      11220120    56.98   145        0       2               2   \n",
       "77            0      11220122    59.99     6        1       1               0   \n",
       "78            1      11220124    69.99   145        0       1               0   \n",
       "79            1      11220126    79.98   145        0       1               0   \n",
       "80            1      11220128    58.30   159        0       2               2   \n",
       "81            0      11220129    48.57   145        1       1               1   \n",
       "82            0      11220130    55.80   145        0       1               0   \n",
       "83            1      11220132    79.14   222        0       2               2   \n",
       "84            0      11220133    70.40   145        1       2               2   \n",
       "85            0      11220134    11.16    53        1       2               2   \n",
       "86            1      11220135    66.98   222        0       1               0   \n",
       "87            1      11220136    34.84   145        0       1               0   \n",
       "88            1      11220137    63.43   145        0       1               1   \n",
       "89            1      11220138    54.59    74        0       1               0   \n",
       "90            1      11220139    76.37   185        0       0               0   \n",
       "91            1      11220143    42.58   145        0       1               0   \n",
       "92            1      11220144     7.14   145        0       1               0   \n",
       "93            1      11220145     9.72   145        0       1               0   \n",
       "94            1      11220146     7.53   145        0       1               0   \n",
       "95            0      11220150    61.53   145        1       2               2   \n",
       "96            1      11220152     6.76   145        0       2               2   \n",
       "97            1      11220153    49.43    74        0       0               1   \n",
       "98            1      11220154    54.65   145        0       1               0   \n",
       "99            0      11220155    63.56    80        1       1               0   \n",
       "\n",
       "    Qualification  \n",
       "0               5  \n",
       "1               5  \n",
       "2               2  \n",
       "3               5  \n",
       "4               7  \n",
       "5               2  \n",
       "6               2  \n",
       "7               5  \n",
       "8               2  \n",
       "9               8  \n",
       "10              2  \n",
       "11              2  \n",
       "12              7  \n",
       "13              2  \n",
       "14              2  \n",
       "15              5  \n",
       "16              7  \n",
       "17              2  \n",
       "18              9  \n",
       "19              2  \n",
       "20              5  \n",
       "21              2  \n",
       "22              8  \n",
       "23              2  \n",
       "24              5  \n",
       "25              2  \n",
       "26              8  \n",
       "27              2  \n",
       "28              5  \n",
       "29              5  \n",
       "..            ...  \n",
       "70              2  \n",
       "71              8  \n",
       "72              8  \n",
       "73              8  \n",
       "74              8  \n",
       "75              8  \n",
       "76              8  \n",
       "77              5  \n",
       "78              2  \n",
       "79              7  \n",
       "80              8  \n",
       "81              2  \n",
       "82              5  \n",
       "83              8  \n",
       "84              8  \n",
       "85              8  \n",
       "86              2  \n",
       "87              2  \n",
       "88              2  \n",
       "89              2  \n",
       "90              2  \n",
       "91              2  \n",
       "92              2  \n",
       "93              2  \n",
       "94              2  \n",
       "95              8  \n",
       "96              8  \n",
       "97              6  \n",
       "98              2  \n",
       "99              6  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing train-test-split \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Putting feature variable to X\n",
    "X = Train_data.drop(['Foreclosure','Agreement_ID'],axis=1)\n",
    "\n",
    "# Putting response variable to y\n",
    "y = Train_data['Foreclosure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Net_Disbursed_Amt</th>\n",
       "      <th>ROI_Change_Ratio</th>\n",
       "      <th>Tenure_Ratio</th>\n",
       "      <th>Tenure_Change_Ratio</th>\n",
       "      <th>Net_LTV</th>\n",
       "      <th>City</th>\n",
       "      <th>Product</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Qualification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16257</th>\n",
       "      <td>13.51</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>37.83</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>30.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>70.92</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>23.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.12</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>27.58</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>73.06</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16897</th>\n",
       "      <td>15.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>85.15</td>\n",
       "      <td>247</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Net_Disbursed_Amt  ROI_Change_Ratio  Tenure_Ratio  Tenure_Change_Ratio  \\\n",
       "16257              13.51             -0.10          0.02                -0.17   \n",
       "7515               30.03              0.00          0.10                 0.00   \n",
       "1993               23.87              0.00          0.05                 0.00   \n",
       "2239               27.58             -0.04          0.22                -0.03   \n",
       "16897              15.51              0.00          0.03                 0.00   \n",
       "\n",
       "       Net_LTV  City  Product  Gender  Marital_Status  Qualification  \n",
       "16257    37.83   261        1       2               2              8  \n",
       "7515     70.92    90        2       1               1              7  \n",
       "1993     54.12    90        2       0               0              5  \n",
       "2239     73.06     2        0       1               0              7  \n",
       "16897    85.15   247        2       2               2              8  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state = 99)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing decision tree classifier from sklearn library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Fitting the decision tree with default hyperparameters, apart from\n",
    "# max_depth which is 5 so that we can plot and read the tree.\n",
    "dt_default = DecisionTreeClassifier(max_depth=5)\n",
    "dt_default.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      5354\n",
      "           1       0.64      0.57      0.61       566\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      5920\n",
      "   macro avg       0.80      0.77      0.78      5920\n",
      "weighted avg       0.93      0.93      0.93      5920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check the evaluation metrics of our default model\n",
    "\n",
    "# Importing classification report and confusion matrix from sklearn metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Making predictions\n",
    "y_pred_default = dt_default.predict(X_test)\n",
    "\n",
    "# Printing classification report\n",
    "print(classification_report(y_test, y_pred_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5172  182]\n",
      " [ 241  325]]\n",
      "0.9285472972972973\n"
     ]
    }
   ],
   "source": [
    "# Printing confusion matrix and accuracy\n",
    "print(confusion_matrix(y_test,y_pred_default))\n",
    "print(accuracy_score(y_test,y_pred_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': range(1, 40)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='accuracy',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV to find optimal max_depth\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'max_depth': range(1, 40)}\n",
    "\n",
    "# instantiate the model\n",
    "dtree = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                               random_state = 100)\n",
    "\n",
    "# fit tree on training data\n",
    "tree = GridSearchCV(dtree, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.911013</td>\n",
       "      <td>0.911013</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>39</td>\n",
       "      <td>0.910966</td>\n",
       "      <td>0.911025</td>\n",
       "      <td>0.910966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910934</td>\n",
       "      <td>0.911033</td>\n",
       "      <td>0.910934</td>\n",
       "      <td>0.911033</td>\n",
       "      <td>0.911264</td>\n",
       "      <td>0.910950</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011631</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.927160</td>\n",
       "      <td>0.927196</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 2}</td>\n",
       "      <td>37</td>\n",
       "      <td>0.923272</td>\n",
       "      <td>0.928313</td>\n",
       "      <td>0.925805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925416</td>\n",
       "      <td>0.927595</td>\n",
       "      <td>0.932657</td>\n",
       "      <td>0.925785</td>\n",
       "      <td>0.928649</td>\n",
       "      <td>0.926787</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015140</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.926653</td>\n",
       "      <td>0.929205</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 3}</td>\n",
       "      <td>38</td>\n",
       "      <td>0.923272</td>\n",
       "      <td>0.928313</td>\n",
       "      <td>0.927615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926140</td>\n",
       "      <td>0.931940</td>\n",
       "      <td>0.926503</td>\n",
       "      <td>0.928953</td>\n",
       "      <td>0.929736</td>\n",
       "      <td>0.928145</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018448</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.932807</td>\n",
       "      <td>0.935124</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 4}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.934130</td>\n",
       "      <td>0.934920</td>\n",
       "      <td>0.927615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930847</td>\n",
       "      <td>0.936917</td>\n",
       "      <td>0.939175</td>\n",
       "      <td>0.933026</td>\n",
       "      <td>0.932271</td>\n",
       "      <td>0.934299</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>0.001423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019746</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.937152</td>\n",
       "      <td>0.942256</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.941006</td>\n",
       "      <td>0.944424</td>\n",
       "      <td>0.936301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931933</td>\n",
       "      <td>0.939361</td>\n",
       "      <td>0.939175</td>\n",
       "      <td>0.943886</td>\n",
       "      <td>0.937342</td>\n",
       "      <td>0.940814</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.001903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.007514         0.001304         0.911013          0.911013   \n",
       "1       0.011631         0.001598         0.927160          0.927196   \n",
       "2       0.015140         0.001504         0.926653          0.929205   \n",
       "3       0.018448         0.001203         0.932807          0.935124   \n",
       "4       0.019746         0.001308         0.937152          0.942256   \n",
       "\n",
       "  param_max_depth            params  rank_test_score  split0_test_score  \\\n",
       "0               1  {'max_depth': 1}               39           0.910966   \n",
       "1               2  {'max_depth': 2}               37           0.923272   \n",
       "2               3  {'max_depth': 3}               38           0.923272   \n",
       "3               4  {'max_depth': 4}               13           0.934130   \n",
       "4               5  {'max_depth': 5}                7           0.941006   \n",
       "\n",
       "   split0_train_score  split1_test_score       ...         split2_test_score  \\\n",
       "0            0.911025           0.910966       ...                  0.910934   \n",
       "1            0.928313           0.925805       ...                  0.925416   \n",
       "2            0.928313           0.927615       ...                  0.926140   \n",
       "3            0.934920           0.927615       ...                  0.930847   \n",
       "4            0.944424           0.936301       ...                  0.931933   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0            0.911033           0.910934            0.911033   \n",
       "1            0.927595           0.932657            0.925785   \n",
       "2            0.931940           0.926503            0.928953   \n",
       "3            0.936917           0.939175            0.933026   \n",
       "4            0.939361           0.939175            0.943886   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.911264            0.910950      0.000544        0.000401   \n",
       "1           0.928649            0.926787      0.000736        0.000493   \n",
       "2           0.929736            0.928145      0.000802        0.000317   \n",
       "3           0.932271            0.934299      0.001161        0.000245   \n",
       "4           0.937342            0.940814      0.000515        0.000250   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.000126         0.000032  \n",
       "1        0.003238         0.000855  \n",
       "2        0.002105         0.001396  \n",
       "3        0.003831         0.001423  \n",
       "4        0.003064         0.001903  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = tree.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAELCAYAAAAoUKpTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XlclVX+wPHPl01EEQXcUdGy3EBF\nXNJKTTO1Ms02l0pnypaxaX6NlTZtY+PUlNO+zFhZaZaatquTe2VZCS64izuLC6AiLihwz++P54I3\nBC4Kl+cC3/frdV/32e+XR7lfzjnPOUeMMSillFIl8bE7AKWUUt5Pk4VSSim3NFkopZRyS5OFUkop\ntzRZKKWUckuThVJKKbc0WSillHJLk4VSSim3NFkopZRyy8/uAMpLeHi4iYyMtDsMpZSqVOLj49ON\nMfXdHVdlkkVkZCRxcXF2h6GUUpWKiOwrzXFaDaWUUsotTRZKKaXc0mShlFLKLU0WSiml3NJkoZRS\nyi2PJQsRmS4ih0VkUzH7RUReF5GdIpIgIjEu++4WkUTn625PxaiUUqp0PFmy+BAYWML+QUBr52sc\n8A6AiIQCzwDdgW7AMyJSz4NxKqWUcsNj/SyMMT+ISGQJh9wEzDDWvK6/iEhdEWkM9AGWGGOOAIjI\nEqyk86mnYlWqKjPGcCbXQU6eg5w8w1nn8rltDnIdBofDkOcwOAw4TP6y8+WAPGMwxpDn4Nx257q7\nzzcABgzW9Y3LMjq1c5mF167BoKjGHv0MOzvlNQWSXNaTnduK234eERmHVSqhefPmnolSKZvl5Dk4\nfjqHE2dyycq2XifO5HLiTE7B+vHsHI6fzuH46VwyT+cUvPK3O/T7uErr1KxulU4WUsQ2U8L28zca\nMw2YBhAbG6u/DqrSynMYUo+dZk/6yYLX3gzrPfnoafLcfNsH+PpQp6Y/dWr6EVLTn7DaAbSqX4s6\ngf6E1PSnZoAvAb4+BPj54F/wLgT4Wut+voKPCL4+1ruPgK+PIM5tviKIc5t1HNY+sdalqN/aQkQo\nOFawPgPncmnOV8Xz8/H8DbQzWSQDzVzWI4BU5/Y+hbavrLColPKw02fz2HLgOJtSMklIzmRzaia7\n005y1qU+JyjAl8iwWnRoEsIN0Y1pEBxI7Rp+BAf6UTvQj+Aa/tQO9CvYVsPPB9FvXOVBdiaLr4Hx\nIjIbqzE70xhzQES+A/7p0qg9AJhkV5BKlYUxhq0Hsliz9wgbUzLZlJJJ4uETBSWF8NoBRDUNoffl\n9WkZVouW4darfnAN/fJXXsVjyUJEPsUqIYSLSDLWE07+AMaY/wALgcHATuAUMNa574iIPAescV5q\ncn5jt1KVxdGTZ/lyfQpz45LZeuA4AGG1AoiKCOHadg2JahpCVEQIjeoEalJQlYKYKvIkQmxsrNFR\nZ5Wd8hyGHxLT+CwuiaVbDnM2z0FU0xBui43gmrYNaRKiiUF5HxGJN8bEujuuygxRrpRd9qaf5LP4\nJObHp3DweDahtQIY3aMFt8ZG0LZxHbvDU6pcaLJQ6iLkOQzLtx1mxuq9/JiYjo9An8sb8OyQdlzT\npiEBfjqSjqpaNFkodQEyTpxh9pokPvl1PynHTtOoTiCPXHsZt3dtRsM6gXaHp5THaLJQyg1jDGv3\nH2Pm6r0s3HiQs3kOel4SxlM3tKV/24b4+WopQlV9miyUKoYxVlXTa8sSSUjOJLiGHyO7N2d0j+Zc\n2iDY7vCUqlCaLJQqxBjD9zvSeGXJDjYkZ9I8NIh/DO3AsM5NqVVDf2VU9aT/85VyMsawamc6ryzZ\nwdr9x2hatyb/Gh7FzTER+GtVk6rmNFkoBfy8y0oSa/YepUlIIP8cFsUtXSL0qSalnDRZqGpt5+ET\nPPP1Jn7amUGjOoE8d1N7buvajBp+vnaHppRX0WShqqXsnDzeXrGTd77fRVCAH0/f0I6R3ZsT6K9J\nQqmiaLJQ1c6qxHSe/HIjezNOMaxzU/52fVvCa9ewOyylvJomC1VtpGWd4R8LtvDV+lQiw4L4+I/d\nubJ1uN1hKVUpaLJQVZ7DYZi9JokXFm3ldE4ef+7Xmgf7XKJVTkpdAE0WqkrLOHGGB2et5dc9R+je\nMpQpw6K4tEFtu8NSqtLRZKGqrJ2HT/CHD9dw6Hg2/xoexW2xzXSIcKUukiYLVSWt3pXBfTPjCPDz\nYfa4HnRuXs/9SUqpYmmyUFXOvPhkJn2eQGRYLaaP6Uqz0CC7Q1Kq0tNkoaoMYwwvL9nBG8t30uvS\nMN4e1YWQmv52h6VUlaDJQlUJ2Tl5PDYvga83pHJ7bDP+MayDjuekVDnSZKEqvSMnzzJuRhxx+47y\n+MA23N+7lTZkK1XONFmoSi3pyCnufP9XDmRm89bIGK6Pbmx3SEpVSZosVKW1K+0Eo9/7lZNncvnk\n3h50aaFPPCnlKZosVKW0OTWTu97/DRFhzn1X0LZxHbtDUqpK0xZAVenE7zvCHdN+oYafD3Pv66GJ\nQqkKoCULVamsSkzn3hlxNAoJ5ON7utO0bk27Q1KqWtBkoSqNxZsPMv6TdbSqX4uZf+xO/WAdVlyp\niqLJQlUKX6xLZsJnCUQ1DeHDsV2pGxRgd0hKVSuaLJTX+/iXfTz11SZ6tAzj3btjqV1D/9sqVdH0\nt055tfd+3M0/FmylX5sGvDUqRuegUMommiyU13pjWSL/XrKD66Ma88rtnQjw04f3lLKLJgvldYwx\nTF28nbdW7OLmzk158ZZo/HScJ6VspclCeRVjDP9YsJX3V+1hRLdmTBkahY+PjvOklN00WSiv4XAY\nnvpqE7N+3c+YnpE8c2M7HRBQKS+hyUJ5hTyH4fH5CcyLT+b+3pfw+MDLNVEo5UU0WSjb5eQ5eGTu\nBr7ZkMr/9b+MP/e7VBOFUl7Go62GIjJQRLaLyE4RmVjE/hYiskxEEkRkpYhEuOx7UUQ2i8hWEXld\n9NujSsrJc/DQJ+v4ZkMqEwe14eH+rTVRKOWFPJYsRMQXeAsYBLQDRohIu0KHTQVmGGOigcnA885z\newK9gGigA9AV6O2pWJU9HA7DY/MS+N/mgzx9Qzvu732J3SEppYrhyZJFN2CnMWa3MeYsMBu4qdAx\n7YBlzuUVLvsNEAgEADUAf+CQB2NVFcwYw+Rvt/DFuhQmDLiMP1zZ0u6QlFIl8GSyaAokuawnO7e5\n2gAMdy4PA4JFJMwYsxoreRxwvr4zxmz1YKyqgr26NJEPf97LvVe15E99L7U7HKWUG55MFkVVPJtC\n6xOA3iKyDquaKQXIFZFLgbZABFaCuUZErj7vA0TGiUiciMSlpaWVb/TKY6av2sNryxK5LTaCJwa3\n1TYKpSoBTyaLZKCZy3oEkOp6gDEm1RhzszGmM/A357ZMrFLGL8aYE8aYE8AioEfhDzDGTDPGxBpj\nYuvXr++pn0OVo3nxyUz+dgsD2zfin8OiNFEoVUl4MlmsAVqLSEsRCQDuAL52PUBEwkUkP4ZJwHTn\n8n6sEoefiPhjlTq0GqqS+27zQR6fn8CVl4bz2ohOOoSHUpWIx35bjTG5wHjgO6wv+rnGmM0iMllE\nhjgP6wNsF5EdQENginP7PGAXsBGrXWODMeYbT8WqPO/nnek89Mk6opqG8N87u1DDT0ePVaoyEWMK\nNyNUTrGxsSYuLs7uMFQR1icdY+S7v9CsXhBz7uuhExcp5UVEJN4YE+vuOK0HUB6VdOQUYz/4jbDa\nAcz4YzdNFEpVUposlMdk5+TxwKx4ch2Gj8Z2o2GdQLtDUkpdJB0bSnmEMYa/fbGJTSnHef/uWFrV\nr213SEqpMtCShfKIj3/Zx/y1yTzcrzX92ja0OxylVBlpslDlLn7fEf7+zRb6tWnAw/1a2x2OUqoc\naLJQ5erw8Wwe+HgtTevV5OXbO+ksd0pVEZosVLk5m+vgwVlrycrOZdqdsYTU9Lc7JKVUOdEGblVu\npizYQty+o7wxojOXNwq2OxylVDnSkoUqF/Pjk/lo9T7uvaolN3ZsYnc4SqlypslCldmmlEye+GIj\nV7QK4/GBbewORynlAZosVJkcO3WW+z+OJ6xWAG+O7KyDAypVRWmbhbpoDofhr3M3cOh4Np/d35Ow\n2jXsDkkp5SH6Z6C6aNN+3M2ybYd58vp2dGpW1+5wlFIepMlCXZRfd2fw0nfbuT66MXdd0cLucJRS\nHqbJQl2wtKwzPPTpOpqHBvHCzTrbnVLVgSYLdUHyHIa/zFlH5ukc3h4VQ3CgdrxTqjrQBm51QV5b\nlshPOzN4cXg0bRvXsTscpVQF0ZKFKrUfdqTxxvJEbukSwW1dm9kdjlKqAmmyUKVyIPM0f5mznssa\nBPPcTR3sDkcpVcE0WSi3cvIcjP9kHWdy8nh7dAw1A3ztDkkpVcG0zUK59dJ324l3DhB4ic54p1S1\npCULVaLVuzKY9sNuRvdorgMEKlWNabJQxTp9No+JnyfQIiyIvw1uZ3c4SikbaTWUKtYrS3ewL+MU\nn9zbXdsplKrmtGShirQh6Rjv/bibEd2a0/OScLvDUUrZTJOFOs/ZXAePzUugQXAgkwbr/BRKKa2G\nUkV4e+VOth/K4v27Y6mjw3kopdCShSpk+8Es3lqxk5s6NaFf24Z2h6OU8hJuk4WIjBeRehURjLJX\nbp6Dx+ZtIDjQn2dubG93OEopL1KakkUjYI2IzBWRgaLjUVdZ03/aw4bkTJ4d0p7QWgF2h6OU8iJu\nk4Ux5kmgNfA+MAZIFJF/isglHo5NVaC96Sf59+Id9G/bkBujG9sdjlLKy5SqzcIYY4CDzlcuUA+Y\nJyIvejA2VUEcDsPj8xMI8PNhyrAOOpmRUuo8bp+GEpE/A3cD6cB7wKPGmBwR8QESgcc8G6LytE9+\n28+ve47wws1RNKwTaHc4SikvVJpHZ8OBm40x+1w3GmMcInKDZ8JSFWVX2gmeX7iVXpeGcbvOUaGU\nKkZpqqEWAkfyV0QkWES6AxhjtnoqMOV5J8/kcv/MeGr4+/LiLR21+kkpVazSJIt3gBMu6yed29xy\nPj21XUR2isjEIva3EJFlIpIgIitFJMJlX3MRWSwiW0Vki4hEluYzVekYY3hsfgK70k7wxojONK1b\n0+6QlFJerDTJQpwN3IBV/UTp2jp8gbeAQUA7YISIFB66dCowwxgTDUwGnnfZNwN4yRjTFugGHC5F\nrKqU3l+1hwUJB3j0ujb0ulTHflJKlaw0yWK3iPxZRPydr4eB3aU4rxuw0xiz2xhzFpgN3FTomHbA\nMufyivz9zqTiZ4xZAmCMOWGMOVWKz1SlsHpXBs8v2sZ17Rtyf+9WdoejlKoESpMs7gd6AilAMtAd\nGFeK85oCSS7ryc5trjYAw53Lw4BgEQkDLgOOicjnIrJORF5yllRUGR3MzOahT9fSIiyIqbdqO4VS\nqnRK0ynvsDHmDmNMA2NMQ2PMSGNMaaqEivoWMoXWJwC9RWQd0BsrIeViVXNd5dzfFWiF1SHw9x8g\nMk5E4kQkLi0trRQhVW9ncx08MCueU2fz+O/oLgTrIIFKqVIqTdtDIPBHoD1Q8BC+MeYPbk5NBlyf\nxYwAUl0PMMakAjc7P6c2MNwYkykiycA6Y8xu574vgR5Yvchdz58GTAOIjY0tnIhUIc99u4V1+4/x\n1sgYWjcMtjscpVQlUppqqJlY40NdB3yP9aWfVYrz1gCtRaSliAQAdwBfux4gIuHOzn0Ak4DpLufW\nE5H6zvVrgC2l+ExVjPnxycz8ZR/3XtWS63U4D6XUBSpNsrjUGPMUcNIY8xFwPRDl7iRjTC4wHvgO\n2ArMNcZsFpHJIjLEeVgfYLuI7AAaAlOc5+ZhVUEtE5GNWFVa717QT6YKbE7N5IkvNtKjVSiPD9TJ\njJRSF640PbhznO/HRKQD1vhQkaW5uDFmIVanPtdtT7sszwPmFXPuEiC6NJ+jipdx4gz3fxxPvaAA\n3hwZg5+vTmGilLpwpUkW05zzWTyJVY1UG3jKo1GpcpGdk8e4mfEcPn6G2eN6EF67ht0hKaUqqRKT\nhbM94bgx5ijwA9ZTSaoScDgMEz7bQPy+o7w9KobOzXX+KqXUxSuxTsLZW3t8BcWiytHLS3bwbcIB\nJg5qw+AobdBWSpVNaSqwl4jIBBFpJiKh+S+PR6Yu2ty4JN5csZMR3Zpx39VaGFRKlV1p2izy+1P8\nyWWbQaukvNLPO9N54vONXNU6nMk36URGSqny4TZZGGNaVkQgqux2Hs7ivo/jaRlei7dGxeCvTz4p\npcpJaXpw31XUdmPMjPIPR12s9BNnGPvhGmr4+TJ9TFfq6FAeSqlyVJpqqK4uy4FAP2At1hDiygtk\n5+Rx74w40rLOMHvcFTQLDbI7JKVUFVOaaqiHXNdFJARrCBDlBYwx/PWzDaxPOsY7o7rQqVldu0NS\nSlVBF1OpfQpoXd6BqIszZ00SCxIO8Nh1bRjYoZHd4SilqqjStFl8w7mhxX2wJiya68mgVOkkHTnF\nc99uoeclYfqIrFLKo0rTZjHVZTkX2GeMSfZQPKqUHA7Do/M2ICK8eEs0Pj76iKxSynNKkyz2AweM\nMdkAIlJTRCKNMXs9Gpkq0Uer9/LL7iO8ODyaiHraoK2U8qzStFl8Bjhc1vOc25RNdqWd4IVF27im\nTQNujY2wOxylVDVQmmThZ4w5m7/iXA7wXEiqJLl5DiZ8toFAf19euDlKe2grpSpEaZJFmstkRYjI\nTUC650JSJZn2427W7T/Gc0M70KBOoPsTlFKqHJSmzeJ+YJaIvOlcTwaK7NWtPGvbweO8smQH10c1\n5kadGlUpVYFK0ylvF9BDRGoDYowpzfzbqpydzXXwyJwNhNT057mhOkCgUqpiua2GEpF/ikhdY8wJ\nY0yWiNQTkX9URHDqnDeXJ7LlwHGevzma0FraZKSUqlilabMYZIw5lr/inDVvsOdCUoVtSDrGWyt3\nMTwmgmvbNbQ7HKVUNVSaZOErIgWTN4tITUAnc64gZ3LzeGTuehoE1+DpG9vZHY5SqpoqTQP3x8Ay\nEfnAuT4W+MhzISlX01ftZVfaST4Y05WQmjrsuFLKHqVp4H5RRBKA/oAA/wNaeDowBYeOZ/PG8kT6\nt21I3zYN7A5HKVWNlXbU2YNYvbiHY81nsdVjEakCzy/cSq7D8NQNbe0ORSlVzRVbshCRy4A7gBFA\nBjAH69HZvhUUW7UWt/cIX65PZXzfS2kRVsvucJRS1VxJ1VDbgB+BG40xOwFE5P8qJKpqLs9hePqr\nzTQOCeTBvpfYHY5SSpVYDTUcq/pphYi8KyL9sNoslIfNXrOfLQeO88TgtgQFlOYZBKWU8qxik4Ux\n5gtjzO1AG2Al8H9AQxF5R0QGVFB81c6xU2eZ+t12urcM5QYd0kMp5SXcNnAbY04aY2YZY24AIoD1\nwESPR1ZNvbxkB5mnc3h2SHsd0kMp5TUuaA5uY8wRY8x/jTHXeCqg6mxL6nE+/mUfo3u0oG3jOnaH\no5RSBS4oWSjPMcbw7DebCanpzyPXXmZ3OEop9TuaLLzENwkH+G3PESZcdzl1g3SgQKWUd9Fk4QVO\nnc3lnwu20r5JHe7o2tzucJRS6jz6XKYXeHvFLg4ez+bNkZ3x9dFGbaWU99GShc1Sj51m2o+7Gdqp\nCbGRoXaHo5RSRdJkYbO3VuzEGMOjA9vYHYpSShXLo8lCRAaKyHYR2Ski5/XNEJEWIrJMRBJEZKWI\nRBTaX0dEUlzm/65Sko+eYm5cErfFNqNp3ZrFH3gmC3LPeC4QY8DhKPmllKrWPNZmISK+wFvAtUAy\nsEZEvjbGbHE5bCowwxjzkYhcAzwP3Omy/znge0/FaLe3VuxEEP7U99LiD9q5FD4dCT5+0PJquLSf\n9QptdfEfnHMakuNg/2rY9xMkrYGckyWfc2l/uOZJaNL54j9XKVVpebKBuxuw0xizG0BEZgM3Aa7J\noh3WMCIAK4Av83eISBegIdb8GbEejNMWSUdO8VlcMiO7N6dJcaWKvT/B7NEQfhk07w6JS2DHImtf\naCvrC/zSayGyFwS4jExrDDjywJEDeTmQmw0HNliJYd9qSIm39iHQsAN0Ggm16hcf7NksWPcxTOsD\nbYdA379BA602U6o68WSyaAokuawnA90LHbMBa8DC14BhQLCIhAFHgX9jlTL6eTBG27yxPBEfH+HB\nPsWUKpLj4JPboG5zuOtLqBVuJYEju63Sxs6lsHYm/DYNfPwhIMhKDHk5zkRQBB8/q2RwxYPQohc0\n6w4165Yu4KsfhdVvw+o3Ydu3EH0H9Hkc6kVe1M+vlKpcPJksinoG1BRanwC8KSJjgB+AFCAXeBBY\naIxJKml8JBEZB4wDaN688vRP2JdxkvlrU7izRwsahQSef8CBBPj4Zuuv/bu+shIFgAiEXWK9ut8H\nOdmw/2fY/b1VteTrbyUEX38rgfjmvwKgfhuIiP19CeRCBIZA30nQbRysehnWvAcbP4Mud1uJJLjR\nxd8QpZTXE2MKf3+X04VFrgCeNcZc51yfBGCMeb6Y42sD24wxESIyC7gKa3a+2kAA8LYxptgBDGNj\nY01cXFw5/xSe8de5G/g2IZUfH+tLgzqFksXhbfDhYPCrCX9YZJUsvNHxVPjhJVg7w0pQvR6GKx8B\n/yKSn1LKa4lIvDHGbVW/J5+GWgO0FpGWIhKANeve164HiEi4iOTHMAmYDmCMGWWMaW6MicQqfcwo\nKVFUJnvST/LFumRG92hxfqLI2AUzbrK+fO/+2nsTBUCdJnDDKzB+DVw+GL7/F7zT0yrlKKWqHI8l\nC2NMLjAe+A5rzu65xpjNIjJZRIY4D+sDbBeRHViN2VM8FY+3eGNZIgF+Ptzfu9AMeMeSrESRd9aq\negqrJDPkhbaCWz+A0Z+DccCMIfD5ODiRZndkSqly5LFqqIpWGaqhdqWd4NqXv+eeq1rxxOC253Zk\nHYQPBsHJDBjzDTTuaF+QZZFzGn78N6x61WobuXYydL4TfLTvp1LeyhuqoVQhry9LpIafL+Oudukj\ncfoYzBgKWYdg9LzKmygA/GtafTEe+Akatodv/my1vxzeandkSqky0mRRQRIPZfH1hlTu7hlJeO0a\n1sa8XJj3B8hIhBGfQrNu9gZZXupfDmMWwE1vQdo2+M+VsPBRq6pNKVUpabKoIK8tSyTIv1CpYslT\nsGsZXP8ytOptX3CeIAKdR8P4eOg0CuKmw+ud4avxVkO+UqpS0WRRAbYfzGLBxgPc3TOS0FrOiY3i\nP4Jf3oYeD1p9FaqqWmEw5HX483qIHWv1zXgzFubfo9VTSlUimiwqwGvLdlArwI97r3KWKvauggWP\nwCX94Nrn7A2uotRtBoNfgocT4IrxsH0RvN0DZo+C1HV2R6eUckOThYfF7zvKwo0HGdsrknq1AuDI\nHphz57lHTn2r2fxTwQ1hwHPwl43Q+3HY+6M15tRnY+DEYbujU0oVQ5OFB+XmOXjyy000qhNo9avI\nPg6fjrD6I4yYbQ2hUV0FhULfJ+Avm6DPJNi2AN7sCutmWWNglZXDYT2SnLQGNn8JadvLfk2lqrFq\n9mdtxZqxeh9bDxznnVEx1PIX+PQeSN8Bd35ReTrdeVpgHegzEdrfbD1q+9WDsHEu3PAqhLZ0f372\nceshgbTt1tNWmfut9+MpVgdHV01jrUb3DjdX70St1EXQTnkecuh4Nv3+/T1dWtTjw7FdkSVPw8+v\nw/X/hq732B2ed3I4IH46LHkWTJ41FHqPB8DH9/fHncyA7Qth6zewe8W5pFC7kdU2EtLM5b051G5o\ntROt+xjStlrjbrUbYiWOFldqp0FVrZW2U54mCw956NN1fLf5IIv/cjWRSV9afzF3vReun2p3aN4v\nMwUW/NWau6NJDAx5w6q22rYAtnxlzcthHFYiaDsE2twATWPAr0bJ1zUGUtdaSWPjPDhzHOq2sB7t\nveQaqH+ZljhUtaPJwkarEtMZ/f6v/KV/a/7S2dd66qf5FTB6vjVkuHLPGNj8OSx8DE4fsZIDQPjl\n0PZGq2TQKNrqz3Exck7D1m9h3UzY4zL4YXBja7Kp+pefe6/fBmo3KPvPpJQX0mRhkzO5eQx69Ufy\njOG7v1xN4NwRsO9neCjeehJIXZhTR6zqu4BaVimi/uXl/xmZKdZMgunbIW3HufezWeeOibzKevS3\nQdvir6NUJVTaZKEN3OXs3R92szv9JB+O7UrgnqWQ+B0M+IcmiosVFAr9n/XsZ4Q0tV4MPrfNGGvO\njvTtVj+Qn9+whi3pfr/VIF8j2LMxKeVltGWvHCUdOcUby3cyOKoRfS4Jgf9NgrDW0O0+u0NTF0rE\nSiCXXANX/fXcsCWr37Qe8d00v3we8VWqktBkUU6MMTzz9WZ8fYSnbmgHv7wDR3bBwBfAL8Du8FRZ\n5Q9bcs8yq/1i3h+suTu0/4aqJjRZlJMlWw6xfNth/q//ZTSWY9aUo5cPhtb97Q5NlaeIWLh3BQye\narVzvNMTljwNp4/aHZlSHqVtFuXg1Nlc/v7NFi5rWJsxvSLhqwesZ/+vq/IT/1VPPr7Q7V5oNxSW\nPgs/vQar34IWPeGyQXD5QGs4F7vlnIbkOEj61Yqn3U3n91lRqpQ0WZSDN5fvJOXYaebedwX+KWsg\nYbZVz+0NXxjKc2rXh6FvQfdxsPkLa3DE7yZZr/DLraRx2SBrnhLXL+m8XKuPR3bmuffcM9YfGHk5\n4Mi13vPOgiPHOr5GsPWQRHBj61Wz3vmPDWdnQtJvVj+UfashJd46P1/9NtD7MWg3TDsiqgumj86W\nweGsbF5ZsoM5a5IY2rkpL98SBe9eYw2IN34N1KhdofEoL3BkD+z4n5U49v1kffHXDLXaObIzreFJ\nck6W/XN8A6we68GNrGtnJsHBjVZ/FB8/aNLZKuk072klq90rYOW/rKe76rexBnFsN7T8koYx55Kc\nI8eZ7FyWc06d+/kLkuRxyD5mLefluP8MVbx6LaHP4xd1qvaz8KDTZ/N498fd/Of7XeTkObizRyR/\nHXAZtTbNssY3Gv4+RN1SIbEoL5adCTuXQuISOHvSGgcrsC7UqGP1FA90vteoY01J6+NnJQFff+ey\nP/j4W+/ZmXDiEGQdsAZIzDpgTcWbv167AbToZSWIiFirX0phjjyrBPT9i86k0db6gml707mkkXPa\nmmfk0GY4tMn5vtn6/GKZc51lDTOHAAAVw0lEQVQmL5R/kPXz60MgZdMoGu6YdVGnarLwgDyH4fO1\nyUxdvJ1Dx88wsH0jJg5qQ2R4LWsu7TdirF6/YxddfM9ipTytIGn8yxrYskE7q7PjwU3WE3z5X/z+\nQda+hu2hVv2Sr+nj60xsfucSnGuy8w/6fXIMrGut64gGttNOeeXsp53pTFmwlS0HjtOxWV3eHBlD\n18jQcwesfMHqbTzoX5oolHfz8bVKvu2HWUlj1SuQshYaRVkj8jbsYCWIei21bUMV0GThRl6eg4Vv\nPcLuw5l0r9mMiQN6cGW3y/GpVe/cQYe3wm/ToMsYaNzRtliVuiD5SUOrTFUpaLJwY/eWNdx45APr\nTuUAP7wGPwA1Qqz5FkJbQXqi1Zh9zVM2R6uUUp6hycKNjK0/0ho4MGoljevVhiO7rVfGLus9dR1k\nJltDj9cKsztcpZTyCE0WbkjKGo5Sh0aXdLTqb8Nbn3+QMdpOoZSq0rT1yo1GxzeyP6g9UlJDnyYK\npVQVp8miBBlpB2lhUshu1MXuUJRSylaaLEqwP8GaQS2kdS+bI1FKKXtpsijB6d2ryTU+REZfaXco\nSillK00WJQhOW8dev5YE1qpjdyhKKWUrfRqqGDk5ObQ6s43NDa63OxSlPConJ4fk5GSys7PtDkV5\nUGBgIBEREfj7X9wQK5osirF3SxytJRvfFt3tDkUpj0pOTiY4OJjIyEhEn+yrkowxZGRkkJycTMuW\nLS/qGloNVYz0bT8C0KTD1TZHopRnZWdnExYWpomiChMRwsLCylR61GRRDN+UODIIoVHzy+0ORSmP\n00RR9ZX139ijyUJEBorIdhHZKSITi9jfQkSWiUiCiKwUkQjn9k4islpENjv33e7JOIvSOCuBpKAO\nJXfGU0qV2bFjx3j77bcv6tzBgwdz7NixEo95+umnWbp06UVdX53jsW9CEfEF3gIGAe2AESLSrtBh\nU4EZxphoYDLwvHP7KeAuY0x7YCDwqojU9VSshaUfSqGZOcCZxtoZTylPKylZ5OXllXjuwoULqVu3\n5K+GyZMn079//4uOzw65ubl2h3AeT/7Z3A3YaYzZbYw5C8wGbip0TDtgmXN5Rf5+Y8wOY0yiczkV\nOAy4mX2l/BR0xrtMO+Mp5WkTJ05k165ddOrUiUcffZSVK1fSt29fRo4cSVRUFABDhw6lS5cutG/f\nnmnTphWcGxkZSXp6Onv37qVt27bce++9tG/fngEDBnD69GkAxowZw7x58wqOf+aZZ4iJiSEqKopt\n27YBkJaWxrXXXktMTAz33XcfLVq0ID09/bxYH3jgAWJjY2nfvj3PPPNMwfY1a9bQs2dPOnbsSLdu\n3cjKyiIvL48JEyYQFRVFdHQ0b7zxxu9iBoiLi6NPnz4APPvss4wbN44BAwZw1113sXfvXq666ipi\nYmKIiYnh559/Lvi8F198kaioKDp27Fhw/2JiYgr2JyYm0qVL+f6x68mnoZoCSS7ryUDhR4s2AMOB\n14BhQLCIhBljMvIPEJFuQACwy4Ox/k72nl/IMb5ERmlnPFW9/P2bzWxJPV6u12zXpA7P3Ni+2P0v\nvPACmzZtYv369QCsXLmS3377jU2bNhU8uTN9+nRCQ0M5ffo0Xbt2Zfjw4YSF/X6U58TERD799FPe\nffddbrvtNubPn8/o0aPP+7zw8HDWrl3L22+/zdSpU3nvvff4+9//zjXXXMOkSZP43//+97uE5GrK\nlCmEhoaSl5dHv379SEhIoE2bNtx+++3MmTOHrl27cvz4cWrWrMm0adPYs2cP69atw8/PjyNHjri9\nV/Hx8axatYqaNWty6tQplixZQmBgIImJiYwYMYK4uDgWLVrEl19+ya+//kpQUBBHjhwhNDSUkJAQ\n1q9fT6dOnfjggw8YM2aM28+7EJ4sWRTVmlJ4DtcJQG8RWQf0BlKAgvKXiDQGZgJjjTl/kl8RGSci\ncSISl5aWVm6B10lfx17/VgQG1S63ayqlSq9bt26/e8Tz9ddfp2PHjvTo0YOkpCQSExPPO6dly5Z0\n6tQJgC5durB3794ir33zzTefd8yqVau44447ABg4cCD16tUr8ty5c+cSExND586d2bx5M1u2bGH7\n9u00btyYrl27AlCnTh38/PxYunQp999/P35+1t/koaGhRV7T1ZAhQ6hZsyZg9X+59957iYqK4tZb\nb2XLli0ALF26lLFjxxIUFPS7695zzz188MEH5OXlMWfOHEaOHOn28y6EJ0sWyUAzl/UIINX1AGcV\n080AIlIbGG6MyXSu1wEWAE8aY34p6gOMMdOAaWDNwV0eQefknKXVme1sbHBjeVxOqUqlpBJARapV\nq1bB8sqVK1m6dCmrV68mKCiIPn36FPkIaI0aNQqWfX19C6qhijvO19e3oG3AGPdfH3v27GHq1Kms\nWbOGevXqMWbMGLKzszHGFPmkUXHb/fz8cDisv30L/xyuP/crr7xCw4YN2bBhAw6Hg8DAwBKvO3z4\n8IISUpcuXc4reZWVJ0sWa4DWItJSRAKAO4CvXQ8QkXARyY9hEjDduT0A+AKr8fszD8Z4nr2bfyVI\nzuDXokdFfqxS1VZwcDBZWVnF7s/MzKRevXoEBQWxbds2fvmlyL8dy+TKK69k7ty5ACxevJijR4+e\nd8zx48epVasWISEhHDp0iEWLFgHQpk0bUlNTWbNmDQBZWVnk5uYyYMAA/vOf/xQkpPxqqMjISOLj\n4wGYP39+sTFlZmbSuHFjfHx8mDlzZkFj/4ABA5g+fTqnTp363XUDAwO57rrreOCBBxg7dmyZ70lh\nHksWxphcYDzwHbAVmGuM2Swik0VkiPOwPsB2EdkBNASmOLffBlwNjBGR9c5XJ0/F6ipj2yoAmkZp\nZzylKkJYWBi9evWiQ4cOPProo+ftHzhwILm5uURHR/PUU0/Ro0f5/yH3zDPPsHjxYmJiYli0aBGN\nGzcmODj4d8d07NiRzp070759e/7whz/Qq5f1AExAQABz5szhoYceomPHjlx77bVkZ2dzzz330Lx5\nc6Kjo+nYsSOffPJJwWc9/PDDXHXVVfj6+hYb04MPPshHH31Ejx492LFjR0GpY+DAgQwZMoTY2Fg6\nderE1KlTC84ZNWoUIsKAAQPK+xYhpSl+VQaxsbEmLi6uzNdZ8+/hRGbFE/70bu1joaqFrVu30rZt\nW7vDsNWZM2fw9fXFz8+P1atX88ADDxQ0uFcmU6dOJTMzk+eee67I/UX9W4tIvDEm1t21dWyoQpqc\n2EhyrQ7U10ShVLWxf/9+brvtNhwOBwEBAbz77rt2h3TBhg0bxq5du1i+fLlHrq/JwkXaoSSamkMk\nNS7fpwiUUt6tdevWrFu3zu4wyuSLL77w6PX1z2cXSRusznh1tTOeUkr9jiYLF2f2rOas8aVldE+7\nQ1FKKa+iycJFSPp69vpfQo3AWu4PVkqpakSThdPZM2doeXYHR0Mr5AldpZSqVDRZOO3d/Cs15Sz+\nkVfYHYpS1UpZhigHePXVVws6qCnP0WThdGS7szNetHbGU6oiVYVk4Y1Dipc3TRZOfqlxHCaUhhGX\n2h2KUtVK4SHKAV566SW6du1KdHR0wVDgJ0+e5Prrr6djx4506NCBOXPm8Prrr5Oamkrfvn3p27fv\nedeePHkyXbt2pUOHDowbN65gDKidO3fSv39/OnbsSExMDLt2WYNaFx76G6BPnz7kd/hNT08nMjIS\ngA8//JBbb72VG2+8kQEDBnDixAn69etXMPz5V199VRDHjBkzCnpy33nnnWRlZdGyZUtycnIAayiR\nyMjIgnVvpP0snJqc2Ehy7Sga2B2IUnZaNBEObizfazaKgkEvFLu78BDlixcvJjExkd9++w1jDEOG\nDOGHH34gLS2NJk2asGDBAsAaOykkJISXX36ZFStWEB4eft61x48fz9NPPw3AnXfeybfffsuNN97I\nqFGjmDhxIsOGDSM7OxuHw1Hk0N/urF69moSEBEJDQ8nNzeWLL76gTp06pKen06NHD4YMGcKWLVuY\nMmUKP/30E+Hh4Rw5coTg4GD69OnDggULGDp0KLNnz2b48OH4+/tfzB2uEFqyANJT99HEHOaszoyn\nlO0WL17M4sWL6dy5MzExMWzbto3ExESioqJYunQpjz/+OD/++CMhISFur7VixQq6d+9OVFQUy5cv\nZ/PmzWRlZZGSksKwYcMAawC+oKCgYof+Lsm1115bcJwxhieeeILo6Gj69+9PSkoKhw4dYvny5dxy\nyy0FyazwkOIAH3zwgUcG/ytPWrIAkjauJByod7l2xlPVXAklgIpijGHSpEncd9995+2Lj49n4cKF\nTJo0iQEDBhSUGoqSnZ3Ngw8+SFxcHM2aNePZZ58tGFK8uM8ty5Dis2bNIi0tjfj4ePz9/YmMjCxx\nCPNevXqxd+9evv/+e/Ly8ujQoUOxP4s30JIFcGbPr5w1fkR20M54SlW0wkOUX3fddUyfPp0TJ04A\nkJKSwuHDh0lNTSUoKIjRo0czYcIE1q5dW+T5+fK/2MPDwzlx4kTB1Kp16tQhIiKCL7/8ErAGETx1\n6lSxQ3+7Dimef42iZGZm0qBBA/z9/VmxYgX79u0DoF+/fsydO5eMjIzfXRfgrrvuYsSIEV5fqgBN\nFgCEZKxjt39ragQG2R2KUtVO4SHKBwwYwMiRI7niiiuIiorilltuISsri40bN9KtWzc6derElClT\nePLJJwEYN24cgwYNOq+Bu27dugUzzQ0dOrRgJjuAmTNn8vrrrxMdHU3Pnj05ePBgsUN/T5gwgXfe\neYeePXsWOS93vlGjRhEXF0dsbCyzZs2iTZs2ALRv356//e1v9O7dm44dO/LII4/87pyjR48yYsSI\ncrufnlLthyg/eyYb888I1jW6hR4P/McDkSnl3XSIcvvMmzePr776ipkzZ1bI5+kQ5WWQmXGQQ4Ht\nCWzd2+5QlFLVyEMPPcSiRYtYuHCh3aGUSrVPFvWbRFJ/0vd2h6GUqmbeeOMNu0O4INpmoZRSyi1N\nFkqpYh8nVVVHWf+NNVkoVc0FBgaSkZGhCaMKM8aQkZFBYGDgRV+j2rdZKFXdRUREkJycTFpamt2h\nKA8KDAwkIiLios/XZKFUNefv70/Lli3tDkN5Oa2GUkop5ZYmC6WUUm5pslBKKeVWlRnuQ0TSgH0l\nHBIOFD+wi/00vrLR+MpG4yubyhxfC2NMfXcXqDLJwh0RiSvN+Cd20fjKRuMrG42vbKpDfFoNpZRS\nyi1NFkoppdyqTslimt0BuKHxlY3GVzYaX9lU+fiqTZuFUkqpi1edShZKKaUuUpVPFiIyUES2i8hO\nEZlodzyFicheEdkoIutF5MKn+vMAEZkuIodFZJPLtlARWSIiic73el4W37MikuK8j+tFZLBNsTUT\nkRUislVENovIw87tXnH/SojPW+5foIj8JiIbnPH93bm9pYj86rx/c0QkwMvi+1BE9rjcv052xOcS\np6+IrBORb53rZb9/xpgq+wJ8gV1AKyAA2AC0szuuQjHuBcLtjqNQTFcDMcAml20vAhOdyxOBf3lZ\nfM8CE7zg3jUGYpzLwcAOoJ233L8S4vOW+ydAbeeyP/Ar0AOYC9zh3P4f4AEvi+9D4Ba7759LnI8A\nnwDfOtfLfP+qesmiG7DTGLPbGHMWmA3cZHNMXs8Y8wNwpNDmm4CPnMsfAUMrNCgXxcTnFYwxB4wx\na53LWcBWoClecv9KiM8rGMsJ56q/82WAa4B5zu123r/i4vMaIhIBXA+851wXyuH+VfVk0RRIcllP\nxot+MZwMsFhE4kVknN3BlKChMeYAWF84QAOb4ynKeBFJcFZT2VZNlk9EIoHOWH99et39KxQfeMn9\nc1ahrAcOA0uwageOGWNynYfY+ntcOD5jTP79m+K8f6+ISA274gNeBR4DHM71MMrh/lX1ZCFFbPOq\nvwKAXsaYGGAQ8CcRudrugCqpd4BLgE7AAeDfdgYjIrWB+cBfjDHH7YylKEXE5zX3zxiTZ4zpBERg\n1Q60Leqwio3K5YMLxSciHYBJQBugKxAKPG5HbCJyA3DYGBPvurmIQy/4/lX1ZJEMNHNZjwBSbYql\nSMaYVOf7YeALrF8Ob3RIRBoDON8P2xzP7xhjDjl/iR3Au9h4H0XEH+uLeJYx5nPnZq+5f0XF5033\nL58x5hiwEqtNoK6I5M+/4xW/xy7xDXRW7xljzBngA+y7f72AISKyF6va/RqskkaZ719VTxZrgNbO\nJwECgDuAr22OqYCI1BKR4PxlYACwqeSzbPM1cLdz+W7gKxtjOU/+F7HTMGy6j8764feBrcaYl112\necX9Ky4+L7p/9UWkrnO5JtAfq11lBXCL8zA7719R8W1z+UNAsNoDbLl/xphJxpgIY0wk1vfdcmPM\nKMrj/tndau/pFzAY64mPXcDf7I6nUGytsJ7Q2gBs9pb4gE+xqiJysEpnf8Sq91wGJDrfQ70svpnA\nRiAB64u5sU2xXYlVxE8A1jtfg73l/pUQn7fcv2hgnTOOTcDTzu2tgN+AncBnQA0vi2+58/5tAj7G\n+cSUnS+gD+eehirz/dMe3Eoppdyq6tVQSimlyoEmC6WUUm5pslBKKeWWJgullFJuabJQSinlliYL\npZRSbmmyUKqCOYelD7/Ic8eISJPyuJZSF0KThVKVyxigibuDlCpvmixUtSUikSKyTUTeE5FNIjJL\nRPqLyE/OSWK6OV8/OyeS+VlELnee+4iITHcuRznPDyrmc8JEZLHzGv/FZWA3ERntnExnvYj8V0R8\nndtPiMi/RWStiCxzDjNxCxALzHIeX9N5mYecx20UkTaevGeq+tJkoaq7S4HXsIZxaAOMxBoSYwLw\nBLANuNoY0xl4Gvin87xXgUtFZBjWwHH3GWNOFfMZzwCrnNf4GmgOICJtgduxRh7uBOQBo5zn1ALW\nGmtE4u+BZ4wx84A4YJQxppMx5rTz2HTnce8441aq3Pm5P0SpKm2PMWYjgIhsBpYZY4yIbAQigRDg\nIxFpjTWmkj+AMcYhImOwxgj6rzHmpxI+42rgZud5C0TkqHN7P6ALsMYaf46anBuN1gHMcS5/DHxO\n8fL3xed/jlLlTZOFqu7OuCw7XNYdWL8fzwErjDHDnJMFrXQ5vjVwgtK1IRQ1CJsAHxljJl3k+fny\nY85Df6eVh2g1lFIlCwFSnMtj8jeKSAhW9dXVQJizPaE4P+CsXhKRQUD+LHTLgFtEpIFzX6iItHDu\n8+HckNIjgVXO5SysubOVqlCaLJQq2YvA8yLyE+Drsv0V4G1jzA6sIdJfyP/SL8LfgatFZC3WnCX7\nAYwxW4AnsabVTcCaQjR/XomTQHsRiceawGayc/uHwH8KNXAr5XE6RLlSXkhEThhjatsdh1L5tGSh\nlFLKLS1ZKFVORGQs8HChzT8ZY/5kRzxKlSdNFkoppdzSaiillFJuabJQSinlliYLpZRSbmmyUEop\n5ZYmC6WUUm79P6qDMyQ3UxscAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting accuracies with max_depth\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'min_samples_leaf': range(5, 200, 20)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV to find optimal max_depth\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_leaf': range(5, 200, 20)}\n",
    "\n",
    "# instantiate the model\n",
    "dtree = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                               random_state = 100)\n",
    "\n",
    "# fit tree on training data\n",
    "tree = GridSearchCV(dtree, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034103</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.937731</td>\n",
       "      <td>0.968576</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 5}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.938473</td>\n",
       "      <td>0.967505</td>\n",
       "      <td>0.937387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937726</td>\n",
       "      <td>0.967689</td>\n",
       "      <td>0.939175</td>\n",
       "      <td>0.969318</td>\n",
       "      <td>0.935893</td>\n",
       "      <td>0.970226</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029880</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.941568</td>\n",
       "      <td>0.950836</td>\n",
       "      <td>25</td>\n",
       "      <td>{'min_samples_leaf': 25}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.940644</td>\n",
       "      <td>0.950127</td>\n",
       "      <td>0.941730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938088</td>\n",
       "      <td>0.952394</td>\n",
       "      <td>0.944243</td>\n",
       "      <td>0.950584</td>\n",
       "      <td>0.943137</td>\n",
       "      <td>0.950498</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026570</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.939758</td>\n",
       "      <td>0.946094</td>\n",
       "      <td>45</td>\n",
       "      <td>{'min_samples_leaf': 45}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.936663</td>\n",
       "      <td>0.946054</td>\n",
       "      <td>0.941006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938088</td>\n",
       "      <td>0.946511</td>\n",
       "      <td>0.938812</td>\n",
       "      <td>0.945877</td>\n",
       "      <td>0.944223</td>\n",
       "      <td>0.946063</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.939686</td>\n",
       "      <td>0.943903</td>\n",
       "      <td>65</td>\n",
       "      <td>{'min_samples_leaf': 65}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.939558</td>\n",
       "      <td>0.944696</td>\n",
       "      <td>0.942816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937002</td>\n",
       "      <td>0.942438</td>\n",
       "      <td>0.938088</td>\n",
       "      <td>0.944158</td>\n",
       "      <td>0.940963</td>\n",
       "      <td>0.944163</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025256</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.940120</td>\n",
       "      <td>0.942654</td>\n",
       "      <td>85</td>\n",
       "      <td>{'min_samples_leaf': 85}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.941368</td>\n",
       "      <td>0.942161</td>\n",
       "      <td>0.943540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937364</td>\n",
       "      <td>0.943977</td>\n",
       "      <td>0.938088</td>\n",
       "      <td>0.942167</td>\n",
       "      <td>0.940239</td>\n",
       "      <td>0.942986</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.000748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.034103         0.001505         0.937731          0.968576   \n",
       "1       0.029880         0.001804         0.941568          0.950836   \n",
       "2       0.026570         0.001203         0.939758          0.946094   \n",
       "3       0.025275         0.001498         0.939686          0.943903   \n",
       "4       0.025256         0.001318         0.940120          0.942654   \n",
       "\n",
       "  param_min_samples_leaf                    params  rank_test_score  \\\n",
       "0                      5   {'min_samples_leaf': 5}                6   \n",
       "1                     25  {'min_samples_leaf': 25}                1   \n",
       "2                     45  {'min_samples_leaf': 45}                3   \n",
       "3                     65  {'min_samples_leaf': 65}                4   \n",
       "4                     85  {'min_samples_leaf': 85}                2   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score       ...         \\\n",
       "0           0.938473            0.967505           0.937387       ...          \n",
       "1           0.940644            0.950127           0.941730       ...          \n",
       "2           0.936663            0.946054           0.941006       ...          \n",
       "3           0.939558            0.944696           0.942816       ...          \n",
       "4           0.941368            0.942161           0.943540       ...          \n",
       "\n",
       "   split2_test_score  split2_train_score  split3_test_score  \\\n",
       "0           0.937726            0.967689           0.939175   \n",
       "1           0.938088            0.952394           0.944243   \n",
       "2           0.938088            0.946511           0.938812   \n",
       "3           0.937002            0.942438           0.938088   \n",
       "4           0.937364            0.943977           0.938088   \n",
       "\n",
       "   split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
       "0            0.969318           0.935893            0.970226      0.002142   \n",
       "1            0.950584           0.943137            0.950498      0.002967   \n",
       "2            0.945877           0.944223            0.946063      0.000884   \n",
       "3            0.944158           0.940963            0.944163      0.001173   \n",
       "4            0.942167           0.940239            0.942986      0.001488   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.000020        0.001107         0.001039  \n",
       "1        0.000399        0.002126         0.000797  \n",
       "2        0.000246        0.002636         0.000219  \n",
       "3        0.000318        0.002059         0.000766  \n",
       "4        0.000258        0.002236         0.000748  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = tree.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting accuracies with min_samples_leaf\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_leaf\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'min_samples_split': range(5, 200, 20)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV to find optimal min_samples_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'min_samples_split': range(5, 200, 20)}\n",
    "\n",
    "# instantiate the model\n",
    "dtree = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                               random_state = 100)\n",
    "\n",
    "# fit tree on training data\n",
    "tree = GridSearchCV(dtree, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\")\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036597</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.929983</td>\n",
       "      <td>0.989809</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_split': 5}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.932682</td>\n",
       "      <td>0.990496</td>\n",
       "      <td>0.931596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930123</td>\n",
       "      <td>0.990587</td>\n",
       "      <td>0.928313</td>\n",
       "      <td>0.988596</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>0.989593</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.000721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034691</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.937658</td>\n",
       "      <td>0.965607</td>\n",
       "      <td>25</td>\n",
       "      <td>{'min_samples_split': 25}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.937025</td>\n",
       "      <td>0.966329</td>\n",
       "      <td>0.935577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939537</td>\n",
       "      <td>0.964522</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.966603</td>\n",
       "      <td>0.935893</td>\n",
       "      <td>0.964887</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.000802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033609</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.939469</td>\n",
       "      <td>0.958910</td>\n",
       "      <td>45</td>\n",
       "      <td>{'min_samples_split': 45}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.937387</td>\n",
       "      <td>0.959993</td>\n",
       "      <td>0.938473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940985</td>\n",
       "      <td>0.959453</td>\n",
       "      <td>0.940985</td>\n",
       "      <td>0.959996</td>\n",
       "      <td>0.939515</td>\n",
       "      <td>0.956109</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.001449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033089</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.939396</td>\n",
       "      <td>0.956086</td>\n",
       "      <td>65</td>\n",
       "      <td>{'min_samples_split': 65}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.936663</td>\n",
       "      <td>0.956372</td>\n",
       "      <td>0.940282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.957462</td>\n",
       "      <td>0.937002</td>\n",
       "      <td>0.956738</td>\n",
       "      <td>0.942774</td>\n",
       "      <td>0.955113</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.001015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031391</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.940048</td>\n",
       "      <td>0.952701</td>\n",
       "      <td>85</td>\n",
       "      <td>{'min_samples_split': 85}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.940644</td>\n",
       "      <td>0.952933</td>\n",
       "      <td>0.939920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938812</td>\n",
       "      <td>0.953661</td>\n",
       "      <td>0.938088</td>\n",
       "      <td>0.951579</td>\n",
       "      <td>0.942774</td>\n",
       "      <td>0.953213</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.036597         0.001704         0.929983          0.989809   \n",
       "1       0.034691         0.001705         0.937658          0.965607   \n",
       "2       0.033609         0.001505         0.939469          0.958910   \n",
       "3       0.033089         0.001303         0.939396          0.956086   \n",
       "4       0.031391         0.001403         0.940048          0.952701   \n",
       "\n",
       "  param_min_samples_split                     params  rank_test_score  \\\n",
       "0                       5   {'min_samples_split': 5}               10   \n",
       "1                      25  {'min_samples_split': 25}                9   \n",
       "2                      45  {'min_samples_split': 45}                6   \n",
       "3                      65  {'min_samples_split': 65}                7   \n",
       "4                      85  {'min_samples_split': 85}                3   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score       ...         \\\n",
       "0           0.932682            0.990496           0.931596       ...          \n",
       "1           0.937025            0.966329           0.935577       ...          \n",
       "2           0.937387            0.959993           0.938473       ...          \n",
       "3           0.936663            0.956372           0.940282       ...          \n",
       "4           0.940644            0.952933           0.939920       ...          \n",
       "\n",
       "   split2_test_score  split2_train_score  split3_test_score  \\\n",
       "0           0.930123            0.990587           0.928313   \n",
       "1           0.939537            0.964522           0.940261   \n",
       "2           0.940985            0.959453           0.940985   \n",
       "3           0.940261            0.957462           0.937002   \n",
       "4           0.938812            0.953661           0.938088   \n",
       "\n",
       "   split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
       "0            0.988596           0.927200            0.989593      0.002308   \n",
       "1            0.966603           0.935893            0.964887      0.001598   \n",
       "2            0.959996           0.939515            0.956109      0.001387   \n",
       "3            0.956738           0.942774            0.955113      0.001749   \n",
       "4            0.951579           0.942774            0.953213      0.001338   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.000511        0.002022         0.000721  \n",
       "1        0.000246        0.001905         0.000802  \n",
       "2        0.000018        0.001409         0.001449  \n",
       "3        0.000246        0.002287         0.001015  \n",
       "4        0.000585        0.001623         0.000753  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = tree.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAELCAYAAAAoUKpTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncdwQ687FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting accuracies with min_samples_leaf\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_min_samples_split\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': range(5, 15, 5), 'min_samples_leaf': range(50, 150, 50), 'min_samples_split': range(50, 150, 50), 'criterion': ['entropy', 'gini']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "    'criterion': [\"entropy\", \"gini\"]\n",
    "}\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, \n",
    "                          cv = n_folds, verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\SharmilaK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.161511</td>\n",
       "      <td>0.006515</td>\n",
       "      <td>0.937658</td>\n",
       "      <td>0.940808</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931209</td>\n",
       "      <td>0.940176</td>\n",
       "      <td>0.937364</td>\n",
       "      <td>0.941895</td>\n",
       "      <td>0.938428</td>\n",
       "      <td>0.939729</td>\n",
       "      <td>0.044493</td>\n",
       "      <td>1.764442e-03</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097158</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.937658</td>\n",
       "      <td>0.940808</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931209</td>\n",
       "      <td>0.940176</td>\n",
       "      <td>0.937364</td>\n",
       "      <td>0.941895</td>\n",
       "      <td>0.938428</td>\n",
       "      <td>0.939729</td>\n",
       "      <td>0.068223</td>\n",
       "      <td>2.407311e-03</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026770</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.937007</td>\n",
       "      <td>0.939541</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931571</td>\n",
       "      <td>0.939995</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.937342</td>\n",
       "      <td>0.938552</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>2.455542e-04</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.001099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021950</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.937007</td>\n",
       "      <td>0.939541</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931571</td>\n",
       "      <td>0.939995</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.937342</td>\n",
       "      <td>0.938552</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>2.459440e-04</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.001099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032179</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.940555</td>\n",
       "      <td>0.945170</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938812</td>\n",
       "      <td>0.946511</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.943977</td>\n",
       "      <td>0.944223</td>\n",
       "      <td>0.945701</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>3.850423e-04</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.029381</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.940555</td>\n",
       "      <td>0.945170</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938812</td>\n",
       "      <td>0.946511</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.943977</td>\n",
       "      <td>0.944223</td>\n",
       "      <td>0.945701</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>2.501460e-04</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.026658</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.939758</td>\n",
       "      <td>0.942003</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936278</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.940963</td>\n",
       "      <td>0.942624</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>5.492910e-04</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.001495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.027668</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.939758</td>\n",
       "      <td>0.942003</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936278</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.940963</td>\n",
       "      <td>0.942624</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>2.582152e-04</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.001495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.019753</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.937369</td>\n",
       "      <td>0.941080</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931209</td>\n",
       "      <td>0.938546</td>\n",
       "      <td>0.938088</td>\n",
       "      <td>0.942167</td>\n",
       "      <td>0.938790</td>\n",
       "      <td>0.939910</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>2.513384e-04</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.019552</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.937369</td>\n",
       "      <td>0.941080</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931209</td>\n",
       "      <td>0.938546</td>\n",
       "      <td>0.938088</td>\n",
       "      <td>0.942167</td>\n",
       "      <td>0.938790</td>\n",
       "      <td>0.939910</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>4.010321e-04</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.018042</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.939469</td>\n",
       "      <td>0.939487</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934468</td>\n",
       "      <td>0.936103</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.940719</td>\n",
       "      <td>0.937704</td>\n",
       "      <td>0.939095</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>3.273867e-04</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.001836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.019054</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.939469</td>\n",
       "      <td>0.939487</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934468</td>\n",
       "      <td>0.936103</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.940719</td>\n",
       "      <td>0.937704</td>\n",
       "      <td>0.939095</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>4.101908e-07</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.001836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.026565</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.940699</td>\n",
       "      <td>0.945189</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940623</td>\n",
       "      <td>0.944429</td>\n",
       "      <td>0.938812</td>\n",
       "      <td>0.945696</td>\n",
       "      <td>0.942774</td>\n",
       "      <td>0.944887</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>3.690599e-04</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.024867</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.940699</td>\n",
       "      <td>0.945189</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940623</td>\n",
       "      <td>0.944429</td>\n",
       "      <td>0.938812</td>\n",
       "      <td>0.945696</td>\n",
       "      <td>0.942774</td>\n",
       "      <td>0.944887</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>2.484872e-04</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.022283</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.941641</td>\n",
       "      <td>0.941297</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938812</td>\n",
       "      <td>0.939542</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.940719</td>\n",
       "      <td>0.940601</td>\n",
       "      <td>0.942896</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>2.394671e-04</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.001146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.022352</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.941641</td>\n",
       "      <td>0.941297</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938812</td>\n",
       "      <td>0.939542</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.940719</td>\n",
       "      <td>0.940601</td>\n",
       "      <td>0.942896</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>2.439733e-04</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.001146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        0.161511         0.006515         0.937658          0.940808   \n",
       "1        0.097158         0.004713         0.937658          0.940808   \n",
       "2        0.026770         0.001203         0.937007          0.939541   \n",
       "3        0.021950         0.001304         0.937007          0.939541   \n",
       "4        0.032179         0.001596         0.940555          0.945170   \n",
       "5        0.029381         0.001309         0.940555          0.945170   \n",
       "6        0.026658         0.001506         0.939758          0.942003   \n",
       "7        0.027668         0.001209         0.939758          0.942003   \n",
       "8        0.019753         0.001208         0.937369          0.941080   \n",
       "9        0.019552         0.001303         0.937369          0.941080   \n",
       "10       0.018042         0.001511         0.939469          0.939487   \n",
       "11       0.019054         0.001003         0.939469          0.939487   \n",
       "12       0.026565         0.001408         0.940699          0.945189   \n",
       "13       0.024867         0.001307         0.940699          0.945189   \n",
       "14       0.022283         0.001297         0.941641          0.941297   \n",
       "15       0.022352         0.001314         0.941641          0.941297   \n",
       "\n",
       "   param_criterion param_max_depth param_min_samples_leaf  \\\n",
       "0          entropy               5                     50   \n",
       "1          entropy               5                     50   \n",
       "2          entropy               5                    100   \n",
       "3          entropy               5                    100   \n",
       "4          entropy              10                     50   \n",
       "5          entropy              10                     50   \n",
       "6          entropy              10                    100   \n",
       "7          entropy              10                    100   \n",
       "8             gini               5                     50   \n",
       "9             gini               5                     50   \n",
       "10            gini               5                    100   \n",
       "11            gini               5                    100   \n",
       "12            gini              10                     50   \n",
       "13            gini              10                     50   \n",
       "14            gini              10                    100   \n",
       "15            gini              10                    100   \n",
       "\n",
       "   param_min_samples_split                                             params  \\\n",
       "0                       50  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "1                      100  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "2                       50  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "3                      100  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "4                       50  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "5                      100  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "6                       50  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "7                      100  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "8                       50  {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "9                      100  {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "10                      50  {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "11                     100  {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "12                      50  {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "13                     100  {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "14                      50  {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "15                     100  {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "\n",
       "    rank_test_score       ...         split2_test_score  split2_train_score  \\\n",
       "0                11       ...                  0.931209            0.940176   \n",
       "1                11       ...                  0.931209            0.940176   \n",
       "2                15       ...                  0.931571            0.939995   \n",
       "3                15       ...                  0.931571            0.939995   \n",
       "4                 5       ...                  0.938812            0.946511   \n",
       "5                 5       ...                  0.938812            0.946511   \n",
       "6                 7       ...                  0.936278            0.943796   \n",
       "7                 7       ...                  0.936278            0.943796   \n",
       "8                13       ...                  0.931209            0.938546   \n",
       "9                13       ...                  0.931209            0.938546   \n",
       "10                9       ...                  0.934468            0.936103   \n",
       "11                9       ...                  0.934468            0.936103   \n",
       "12                3       ...                  0.940623            0.944429   \n",
       "13                3       ...                  0.940623            0.944429   \n",
       "14                1       ...                  0.938812            0.939542   \n",
       "15                1       ...                  0.938812            0.939542   \n",
       "\n",
       "    split3_test_score  split3_train_score  split4_test_score  \\\n",
       "0            0.937364            0.941895           0.938428   \n",
       "1            0.937364            0.941895           0.938428   \n",
       "2            0.935554            0.939271           0.937342   \n",
       "3            0.935554            0.939271           0.937342   \n",
       "4            0.935554            0.943977           0.944223   \n",
       "5            0.935554            0.943977           0.944223   \n",
       "6            0.935554            0.939271           0.940963   \n",
       "7            0.935554            0.939271           0.940963   \n",
       "8            0.938088            0.942167           0.938790   \n",
       "9            0.938088            0.942167           0.938790   \n",
       "10           0.940261            0.940719           0.937704   \n",
       "11           0.940261            0.940719           0.937704   \n",
       "12           0.938812            0.945696           0.942774   \n",
       "13           0.938812            0.945696           0.942774   \n",
       "14           0.940261            0.940719           0.940601   \n",
       "15           0.940261            0.940719           0.940601   \n",
       "\n",
       "    split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0             0.939729      0.044493    1.764442e-03        0.004569   \n",
       "1             0.939729      0.068223    2.407311e-03        0.004569   \n",
       "2             0.938552      0.003733    2.455542e-04        0.004416   \n",
       "3             0.938552      0.000669    2.459440e-04        0.004416   \n",
       "4             0.945701      0.003065    3.850423e-04        0.003040   \n",
       "5             0.945701      0.000748    2.501460e-04        0.003040   \n",
       "6             0.942624      0.001499    5.492910e-04        0.003234   \n",
       "7             0.942624      0.001016    2.582152e-04        0.003234   \n",
       "8             0.939910      0.001129    2.513384e-04        0.003361   \n",
       "9             0.939910      0.000448    4.010321e-04        0.003361   \n",
       "10            0.939095      0.000318    3.273867e-04        0.003347   \n",
       "11            0.939095      0.000626    4.101908e-07        0.003347   \n",
       "12            0.944887      0.002339    3.690599e-04        0.001276   \n",
       "13            0.944887      0.000534    2.484872e-04        0.001276   \n",
       "14            0.942896      0.001176    2.394671e-04        0.002236   \n",
       "15            0.942896      0.000681    2.439733e-04        0.002236   \n",
       "\n",
       "    std_train_score  \n",
       "0          0.001402  \n",
       "1          0.001402  \n",
       "2          0.001099  \n",
       "3          0.001099  \n",
       "4          0.000902  \n",
       "5          0.000902  \n",
       "6          0.001495  \n",
       "7          0.001495  \n",
       "8          0.001729  \n",
       "9          0.001729  \n",
       "10         0.001836  \n",
       "11         0.001836  \n",
       "12         0.000465  \n",
       "13         0.000465  \n",
       "14         0.001146  \n",
       "15         0.001146  \n",
       "\n",
       "[16 rows x 24 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy 0.9416407211642893\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=100, min_samples_split=50,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print(\"best accuracy\", grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=50, min_samples_split=50,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model with optimal hyperparameters\n",
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                  random_state = 100,\n",
    "                                  max_depth=10, \n",
    "                                  min_samples_leaf=50,\n",
    "                                  min_samples_split=50)\n",
    "clf_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9366554054054054"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score\n",
    "clf_gini.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
